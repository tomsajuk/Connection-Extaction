{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models for predicting labels\n",
    "\n",
    "## Train and Test Data\n",
    "Before going forward, the first and foremost step is to divide the data into training and test data in 70:30 ration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fileName = \"WhiteText(1).csv\"\n",
    "trainSet = pd.read_csv('data/'+fileName,delimiter=\"|\")\n",
    "\n",
    "fileEval = \"WhiteTextUnseenEval(1).csv\"\n",
    "testSet = pd.read_csv('data/'+fileEval,delimiter=\"|\")\n",
    "\n",
    "\n",
    "trainSet.to_csv('Train(1)',sep=\"|\")\n",
    "testSet.to_csv('Test(1)',sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have divided the data into Train and Test data we are now going to build models\n",
    "\n",
    "-----------\n",
    "\n",
    "### Sentence preprocessing\n",
    "At the start the Brain region mentions that were being looked into were tagged into BR1 and BR2 using replace function. Some more preprocessing were also done, which will be added on as we go on.\n",
    "\n",
    "So first reading the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22572\n",
      "11219\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.read_csv('Train(1)',delimiter='|')\n",
    "trainSen = trainData['sentence']\n",
    "trainEn1 = trainData['entity1']\n",
    "trainEn2 = trainData['entity2']\n",
    "trainLab = trainData['connection']\n",
    "trainLen = len(trainSen)\n",
    "print trainLen\n",
    "\n",
    "testData = pd.read_csv('Test(1)',delimiter='|')\n",
    "testSen = testData['sentence']\n",
    "testEn1 = testData['entity1']\n",
    "testEn2 = testData['entity2']\n",
    "testLab = testData['connection']\n",
    "testLen = len(testSen)\n",
    "print testLen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be replacing every instance of the occurence of Entity1 and Entity2 in a sentence. After that removing of words inside brackets, and removing all characters other than alphabets and lowering the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def replaceBR(s, en1, en2):\n",
    "    s = s.replace(en1,\" BR1 \")\n",
    "    s = s.replace(en2,\" BR2 \")\n",
    "    return s\n",
    "\n",
    "def formatSen(x):\n",
    "    x = re.sub(\"\\((.*?)\\)\",\" \",x.lower())\n",
    "    x = re.sub(\"^[ ]*([a-z])\",r\"\\1\",x)\n",
    "    return re.sub(\"[^a-z0-9]\",\" \",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFSen = []\n",
    "for i in range(0,trainLen):\n",
    "    trainFSen.append(replaceBR(trainSen[i], trainEn1[i], trainEn2[i]))\n",
    "    trainFSen[i] = formatSen(trainFSen[i])\n",
    "    \n",
    "testFSen = []\n",
    "for i in range(0,testLen):\n",
    "    testFSen.append(replaceBR(testSen[i], testEn1[i], testEn2[i]))\n",
    "    testFSen[i] = formatSen(testFSen[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After formatting all the sentences we come to a way to represent sentences as numbers as classfication models accept numbers only.\n",
    "\n",
    "## Word Embedding\n",
    "The input for all models that have been created till date take numbers as input. They donot accept words or letters. So we need to convert these words into corresponding numbers that would identity that word. This process is called word embedding.\n",
    "\n",
    "There are different word embedding techniques.\n",
    "1. Count Vectorization\n",
    "2. Tf-idf Vectorization\n",
    "3. Word2Vec Embedding\n",
    "4. FastText\n",
    "\n",
    "CountVectorization take each count of a particular word in a sentence. The whole array formed later is used as the representation basis. This is not being used as this would create a pattern based classifier and also perform bad.\n",
    "\n",
    "#### Tf-idf Vec\n",
    "Tf-idf Vectorization takes term frequency and inverse document frequency into consideration for denoting a value for a word. Common words that appear repeatedly in a sentence are given lesser value than rare words. We will be implementing this.\n",
    "\n",
    "\n",
    "#### Word2Vec\n",
    "Word2Vec Embedding is a model that is trained on set of sentences. The hidden layer weight after training is used a the representation of a word. Model training is already done on both train and test data together.\n",
    "\n",
    "To represent a sentence using word2vec here we are going to be taking the mean of all the word vectors present in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def convertTfidf(train, test):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,3))               #Calling tfidf Vectorizer\n",
    "    train_vect = vectorizer.fit_transform(train) #Fitting the training data for getting tfidf values\n",
    "    test_vect = vectorizer.transform(test)       #Transforming test sentences to their respective tfidf vector\n",
    "    return train_vect, test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier Models\n",
    "The different classifier algorithms that are going to be used are - \n",
    "1. Bernoulli Navies Bayes\n",
    "2. Bagging Classifier\n",
    "3. Decision Tree Classifier\n",
    "4. Random Forest Classifier\n",
    "5. Extra Trees Classifier\n",
    "6. Calibrated Classifier\n",
    "7. SGD Classifier\n",
    "8. K-Neighbours Classifier\n",
    "9. MLP Classifier\n",
    "Each have their own speciality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "from sklearn.dummy import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.calibration import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.multiclass import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.neural_network import *\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# from prettytable import PrettyTable\n",
    "\n",
    "\n",
    "classifiers = [BernoulliNB(), \n",
    "               RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "               BaggingClassifier(n_estimators=100, n_jobs=-1), \n",
    "               ExtraTreesClassifier(n_jobs=-1),\n",
    "               DecisionTreeClassifier(criterion='gini',splitter='random'), \n",
    "               CalibratedClassifierCV(),\n",
    "               SGDClassifier(n_jobs=-1), \n",
    "               #KNeighborsClassifier(n_neighbors=1,weights='distance',n_jobs=2,algorithm='kd_tree'),\n",
    "               MLPClassifier(hidden_layer_sizes=(100,100,),verbose=True)]\n",
    "\n",
    "def classify(train_vect,trainLab,test_vect,testLab):\n",
    "#     tableRep = PrettyTable(['Name','Precision','Recall','F1 Score','Accuracy'])\n",
    "    tableSent = []\n",
    "    \n",
    "    for classifier in classifiers:\n",
    "\n",
    "        print \"Training \",classifier.__class__.__name__\n",
    "        classifier.fit(train_vect, trainLab)\n",
    "\n",
    "        score = classifier.predict(test_vect)\n",
    "        \n",
    "        mat = confusion_matrix(testLab, score)\n",
    "\n",
    "        print mat\n",
    "        tp = mat[1][1]\n",
    "        fp = mat[0][1]\n",
    "        fn = mat[1][0]\n",
    "        tn = mat[0][0]\n",
    "        if tp == 0 :\n",
    "            recall = 0.0\n",
    "            precision = 0.0\n",
    "            f1 = 0.0\n",
    "        else :\n",
    "            recall = float(tp)/float((tp+fn))\n",
    "            precision = float(tp)/float((tp+fp))\n",
    "            f1 = 2 * ((precision*recall)/(precision+recall))\n",
    "        accuracy = float(tp+tn)/float(len(testLab))\n",
    "        print precision,recall,f1\n",
    "#         tableRep.add_row([classifier.__class__.__name__,precision,recall,f1,accuracy])\n",
    "        tableSent.append([classifier.__class__.__name__,precision,recall,f1,accuracy])\n",
    "    for i in tableSent:\n",
    "        print i\n",
    "    return 'OK'\n",
    "#     return tableSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vect, test_vect = convertTfidf(trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf has a argument called n_gram where the words are taken together according to the argument value. So here we are going to be taking n_gram=(1,3) as this had better classifier report than other n_grams. From here on Tf-IDF Vectorizer will be taking n_grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  BernoulliNB\n",
      "[[7541 1671]\n",
      " [1129  878]]\n",
      "0.344448803452 0.437468858994 0.385425812116\n",
      "Training  RandomForestClassifier\n",
      "[[9126   86]\n",
      " [1827  180]]\n",
      "0.676691729323 0.0896860986547 0.158380994281\n",
      "Training  BaggingClassifier\n",
      "[[8772  440]\n",
      " [1534  473]]\n",
      "0.518072289157 0.23567513702 0.32397260274\n",
      "Training  ExtraTreesClassifier\n",
      "[[9048  164]\n",
      " [1817  190]]\n",
      "0.536723163842 0.0946686596911 0.160948750529\n",
      "Training  DecisionTreeClassifier\n",
      "[[7860 1352]\n",
      " [1300  707]]\n",
      "0.343370568237 0.352267065272 0.347761928185\n",
      "Training  CalibratedClassifierCV\n",
      "[[9115   97]\n",
      " [1767  240]]\n",
      "0.712166172107 0.119581464873 0.204778156997\n",
      "Training  SGDClassifier\n",
      "[[9192   20]\n",
      " [1921   86]]\n",
      "0.811320754717 0.0428500249128 0.0814008518694\n",
      "Training  MLPClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trainee/tensorflow/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.36876203\n",
      "Iteration 2, loss = 0.23150532\n",
      "Iteration 3, loss = 0.16845859\n",
      "Iteration 4, loss = 0.13692128\n",
      "Iteration 5, loss = 0.11581572\n",
      "Iteration 6, loss = 0.10286865\n",
      "Iteration 7, loss = 0.09371703\n",
      "Iteration 8, loss = 0.08640060\n",
      "Iteration 9, loss = 0.08104282\n",
      "Iteration 10, loss = 0.07766270\n",
      "Iteration 11, loss = 0.07349220\n",
      "Iteration 12, loss = 0.07100190\n",
      "Iteration 13, loss = 0.06882192\n",
      "Iteration 14, loss = 0.06695088\n",
      "Iteration 15, loss = 0.06325177\n",
      "Iteration 16, loss = 0.06256271\n",
      "Iteration 17, loss = 0.06135273\n",
      "Iteration 18, loss = 0.05938897\n",
      "Iteration 19, loss = 0.05805327\n",
      "Iteration 20, loss = 0.05720237\n",
      "Iteration 21, loss = 0.05595367\n",
      "Iteration 22, loss = 0.05525690\n",
      "Iteration 23, loss = 0.05525735\n",
      "Iteration 24, loss = 0.05488095\n",
      "Iteration 25, loss = 0.05446911\n",
      "Iteration 26, loss = 0.05301586\n",
      "Iteration 27, loss = 0.05258843\n",
      "Iteration 28, loss = 0.05157864\n",
      "Iteration 29, loss = 0.05084661\n",
      "Iteration 30, loss = 0.05041137\n",
      "Iteration 31, loss = 0.04959424\n",
      "Iteration 32, loss = 0.05036091\n",
      "Iteration 33, loss = 0.05092231\n",
      "Iteration 34, loss = 0.05036545\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8018 1194]\n",
      " [1200  807]]\n",
      "0.403298350825 0.402092675635 0.402694610778\n",
      "['BernoulliNB', 0.34444880345233425, 0.43746885899352267, 0.3854258121158911, 0.7504233888938409]\n",
      "['RandomForestClassifier', 0.6766917293233082, 0.08968609865470852, 0.1583809942806863, 0.8294856939121134]\n",
      "['BaggingClassifier', 0.5180722891566265, 0.2356751370204285, 0.323972602739726, 0.8240484891701577]\n",
      "['ExtraTreesClassifier', 0.536723163841808, 0.09466865969108122, 0.16094875052943672, 0.8234245476423924]\n",
      "['DecisionTreeClassifier', 0.34337056823700823, 0.35226706527154955, 0.34776192818494833, 0.7636152954808807]\n",
      "['CalibratedClassifierCV', 0.712166172106825, 0.11958146487294469, 0.20477815699658702, 0.8338532846064711]\n",
      "['SGDClassifier', 0.8113207547169812, 0.04285002491280518, 0.08140085186938001, 0.8269899278010517]\n",
      "['MLPClassifier', 0.4032983508245877, 0.4020926756352765, 0.4026946107784431, 0.7866119975042339]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_vect, test_vect = convertTfidf(trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using word2vec embedding we need to first load the model here for use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "def loadW2V(modelName):\n",
    "    model = word2vec.Word2Vec.load(modelName)\n",
    "    return dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "\n",
    "def convertW2V(data,w2v):\n",
    "    wholeM = []\n",
    "    count = 0\n",
    "    le = 0\n",
    "    print 'Embedding...',len(data)\n",
    "    for sentence in data:\n",
    "        le += 1\n",
    "        arr = []    \n",
    "        for word in sentence.split():\n",
    "            if word in w2v:\n",
    "                arr.append(np.array(w2v[word],copy=True))  \n",
    "                                #Each word is checked if it is there in the word2vec vocabulary. If there then\n",
    "                                #the vector space for the word is taken and then the mean is calculated.\n",
    "\n",
    "        mean = np.zeros(100)\n",
    "        for mat in arr:\n",
    "            for j in range(len(mat)):\n",
    "                mean[j] += mat[j]\n",
    "        if len(arr) != 0:\n",
    "            mean = np.array(mean/len(arr))\n",
    "        else:\n",
    "            count +=1\n",
    "        wholeM.append(mean)\n",
    "    print count,le\n",
    "    return wholeM\n",
    "\n",
    "def buildW2V(modelName, train, test):\n",
    "    w2v = loadW2V(\"w2v_models/\"+modelName)\n",
    "    train_vect = convertW2V(train,w2v)\n",
    "    test_vect = convertW2V(test,w2v)\n",
    "    return train_vect, test_vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "0 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7748 1464]\n",
      " [1166  841]]\n",
      "0.364859002169 0.419033383159 0.390074211503\n",
      "Training  RandomForestClassifier\n",
      "[[9144   68]\n",
      " [1903  104]]\n",
      "0.604651162791 0.0518186347783 0.0954566314823\n",
      "Training  BaggingClassifier\n",
      "[[9108  104]\n",
      " [1874  133]]\n",
      "0.561181434599 0.0662680617838 0.118538324421\n",
      "Training  ExtraTreesClassifier\n",
      "[[9027  185]\n",
      " [1883  124]]\n",
      "0.401294498382 0.061783756851 0.107081174439\n",
      "Training  DecisionTreeClassifier\n",
      "[[7713 1499]\n",
      " [1515  492]]\n",
      "0.247112004018 0.24514200299 0.246123061531\n",
      "Training  CalibratedClassifierCV\n",
      "[[9169   43]\n",
      " [1924   83]]\n",
      "0.65873015873 0.0413552566019 0.0778246601031\n",
      "Training  SGDClassifier\n",
      "[[8987  225]\n",
      " [1812  195]]\n",
      "0.464285714286 0.0971599402093 0.160692212608\n",
      "Training  KNeighborsClassifier\n",
      "[[7991 1221]\n",
      " [1566  441]]\n",
      "0.265342960289 0.219730941704 0.240392477514\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.37752531\n",
      "Iteration 2, loss = 0.33732279\n",
      "Iteration 3, loss = 0.32549862\n",
      "Iteration 4, loss = 0.31631332\n",
      "Iteration 5, loss = 0.30781668\n",
      "Iteration 6, loss = 0.30001684\n",
      "Iteration 7, loss = 0.29320507\n",
      "Iteration 8, loss = 0.28858145\n",
      "Iteration 9, loss = 0.28162721\n",
      "Iteration 10, loss = 0.27604524\n",
      "Iteration 11, loss = 0.27056748\n",
      "Iteration 12, loss = 0.26606333\n",
      "Iteration 13, loss = 0.26271348\n",
      "Iteration 14, loss = 0.25706855\n",
      "Iteration 15, loss = 0.25122657\n",
      "Iteration 16, loss = 0.24803237\n",
      "Iteration 17, loss = 0.24460237\n",
      "Iteration 18, loss = 0.24084740\n",
      "Iteration 19, loss = 0.23510337\n",
      "Iteration 20, loss = 0.23223693\n",
      "Iteration 21, loss = 0.22730485\n",
      "Iteration 22, loss = 0.22452594\n",
      "Iteration 23, loss = 0.22105594\n",
      "Iteration 24, loss = 0.21804832\n",
      "Iteration 25, loss = 0.21467340\n",
      "Iteration 26, loss = 0.21112288\n",
      "Iteration 27, loss = 0.20768460\n",
      "Iteration 28, loss = 0.20482205\n",
      "Iteration 29, loss = 0.20141013\n",
      "Iteration 30, loss = 0.20038300\n",
      "Iteration 31, loss = 0.19473041\n",
      "Iteration 32, loss = 0.19388922\n",
      "Iteration 33, loss = 0.19342842\n",
      "Iteration 34, loss = 0.19030697\n",
      "Iteration 35, loss = 0.18772312\n",
      "Iteration 36, loss = 0.18453051\n",
      "Iteration 37, loss = 0.18646343\n",
      "Iteration 38, loss = 0.17946806\n",
      "Iteration 39, loss = 0.17955918\n",
      "Iteration 40, loss = 0.17568624\n",
      "Iteration 41, loss = 0.17775073\n",
      "Iteration 42, loss = 0.17348410\n",
      "Iteration 43, loss = 0.17387340\n",
      "Iteration 44, loss = 0.17083718\n",
      "Iteration 45, loss = 0.16676782\n",
      "Iteration 46, loss = 0.16587842\n",
      "Iteration 47, loss = 0.16471028\n",
      "Iteration 48, loss = 0.16405890\n",
      "Iteration 49, loss = 0.16101482\n",
      "Iteration 50, loss = 0.16013305\n",
      "Iteration 51, loss = 0.15887779\n",
      "Iteration 52, loss = 0.16077078\n",
      "Iteration 53, loss = 0.15796393\n",
      "Iteration 54, loss = 0.15680347\n",
      "Iteration 55, loss = 0.15386077\n",
      "Iteration 56, loss = 0.15324584\n",
      "Iteration 57, loss = 0.15455059\n",
      "Iteration 58, loss = 0.15416767\n",
      "Iteration 59, loss = 0.14991313\n",
      "Iteration 60, loss = 0.14851354\n",
      "Iteration 61, loss = 0.14786436\n",
      "Iteration 62, loss = 0.14789859\n",
      "Iteration 63, loss = 0.14574288\n",
      "Iteration 64, loss = 0.14461381\n",
      "Iteration 65, loss = 0.14372609\n",
      "Iteration 66, loss = 0.14426726\n",
      "Iteration 67, loss = 0.14324775\n",
      "Iteration 68, loss = 0.13951537\n",
      "Iteration 69, loss = 0.13999060\n",
      "Iteration 70, loss = 0.13773627\n",
      "Iteration 71, loss = 0.13708753\n",
      "Iteration 72, loss = 0.13864986\n",
      "Iteration 73, loss = 0.13570984\n",
      "Iteration 74, loss = 0.13686787\n",
      "Iteration 75, loss = 0.13296527\n",
      "Iteration 76, loss = 0.13419157\n",
      "Iteration 77, loss = 0.13465199\n",
      "Iteration 78, loss = 0.13645703\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8062 1150]\n",
      " [1418  589]]\n",
      "0.33870040253 0.293472845042 0.314468766684\n",
      "['BernoulliNB', 0.3648590021691974, 0.4190333831589437, 0.390074211502783, 0.7655762545681434]\n",
      "['RandomForestClassifier', 0.6046511627906976, 0.051818634778276036, 0.09545663148233134, 0.8243158926820572]\n",
      "['BaggingClassifier', 0.5611814345991561, 0.06626806178375685, 0.11853832442067735, 0.8236919511542918]\n",
      "['ExtraTreesClassifier', 0.40129449838187703, 0.06178375685102142, 0.1070811744386874, 0.8156698457973082]\n",
      "['DecisionTreeClassifier', 0.24711200401808137, 0.24514200298953662, 0.24612306153076538, 0.731348605045013]\n",
      "['CalibratedClassifierCV', 0.6587301587301587, 0.041355256601893375, 0.07782466010314111, 0.8246724306979232]\n",
      "['SGDClassifier', 0.4642857142857143, 0.09715994020926756, 0.16069221260815822, 0.8184330154202691]\n",
      "['KNeighborsClassifier', 0.26534296028880866, 0.21973094170403587, 0.2403924775143091, 0.7515821374454051]\n",
      "['MLPClassifier', 0.33870040253018974, 0.29347284504235177, 0.31446876668446344, 0.7711025938140654]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "5 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7786 1426]\n",
      " [1277  730]]\n",
      "0.338589981447 0.363726955655 0.350708623589\n",
      "Training  RandomForestClassifier\n",
      "[[9142   70]\n",
      " [1879  128]]\n",
      "0.646464646465 0.0637767812656 0.116099773243\n",
      "Training  BaggingClassifier\n",
      "[[9118   94]\n",
      " [1849  158]]\n",
      "0.626984126984 0.0787244643747 0.139884904825\n",
      "Training  ExtraTreesClassifier\n",
      "[[9045  167]\n",
      " [1869  138]]\n",
      "0.452459016393 0.0687593423019 0.11937716263\n",
      "Training  DecisionTreeClassifier\n",
      "[[7764 1448]\n",
      " [1446  561]]\n",
      "0.279243404679 0.279521674141 0.27938247012\n",
      "Training  CalibratedClassifierCV\n",
      "[[9180   32]\n",
      " [1938   69]]\n",
      "0.683168316832 0.034379671151 0.0654648956357\n",
      "Training  SGDClassifier\n",
      "[[9011  201]\n",
      " [1820  187]]\n",
      "0.481958762887 0.0931738913802 0.156158663883\n",
      "Training  KNeighborsClassifier\n",
      "[[7922 1290]\n",
      " [1525  482]]\n",
      "0.272009029345 0.240159441953 0.255093940196\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.38729613\n",
      "Iteration 2, loss = 0.34390020\n",
      "Iteration 3, loss = 0.33299805\n",
      "Iteration 4, loss = 0.32476261\n",
      "Iteration 5, loss = 0.31945831\n",
      "Iteration 6, loss = 0.31292685\n",
      "Iteration 7, loss = 0.30780940\n",
      "Iteration 8, loss = 0.30084242\n",
      "Iteration 9, loss = 0.29731568\n",
      "Iteration 10, loss = 0.29270924\n",
      "Iteration 11, loss = 0.28734489\n",
      "Iteration 12, loss = 0.28435676\n",
      "Iteration 13, loss = 0.27959925\n",
      "Iteration 14, loss = 0.27688364\n",
      "Iteration 15, loss = 0.27372358\n",
      "Iteration 16, loss = 0.26980085\n",
      "Iteration 17, loss = 0.26636407\n",
      "Iteration 18, loss = 0.26439727\n",
      "Iteration 19, loss = 0.26149881\n",
      "Iteration 20, loss = 0.25965856\n",
      "Iteration 21, loss = 0.25705105\n",
      "Iteration 22, loss = 0.25232176\n",
      "Iteration 23, loss = 0.25049893\n",
      "Iteration 24, loss = 0.24708686\n",
      "Iteration 25, loss = 0.24561762\n",
      "Iteration 26, loss = 0.24359539\n",
      "Iteration 27, loss = 0.24117106\n",
      "Iteration 28, loss = 0.23984993\n",
      "Iteration 29, loss = 0.23590025\n",
      "Iteration 30, loss = 0.23337418\n",
      "Iteration 31, loss = 0.23411460\n",
      "Iteration 32, loss = 0.23239536\n",
      "Iteration 33, loss = 0.23160863\n",
      "Iteration 34, loss = 0.22845863\n",
      "Iteration 35, loss = 0.22514136\n",
      "Iteration 36, loss = 0.22483924\n",
      "Iteration 37, loss = 0.22161686\n",
      "Iteration 38, loss = 0.22308313\n",
      "Iteration 39, loss = 0.22170268\n",
      "Iteration 40, loss = 0.21791479\n",
      "Iteration 41, loss = 0.21933030\n",
      "Iteration 42, loss = 0.21798693\n",
      "Iteration 43, loss = 0.21504912\n",
      "Iteration 44, loss = 0.21624999\n",
      "Iteration 45, loss = 0.21104299\n",
      "Iteration 46, loss = 0.21065278\n",
      "Iteration 47, loss = 0.21127605\n",
      "Iteration 48, loss = 0.20889198\n",
      "Iteration 49, loss = 0.20816642\n",
      "Iteration 50, loss = 0.20595302\n",
      "Iteration 51, loss = 0.20374050\n",
      "Iteration 52, loss = 0.20524296\n",
      "Iteration 53, loss = 0.20391771\n",
      "Iteration 54, loss = 0.20189804\n",
      "Iteration 55, loss = 0.20139968\n",
      "Iteration 56, loss = 0.20052843\n",
      "Iteration 57, loss = 0.20239475\n",
      "Iteration 58, loss = 0.19944890\n",
      "Iteration 59, loss = 0.19939378\n",
      "Iteration 60, loss = 0.19639568\n",
      "Iteration 61, loss = 0.19619494\n",
      "Iteration 62, loss = 0.19477270\n",
      "Iteration 63, loss = 0.19447485\n",
      "Iteration 64, loss = 0.19688877\n",
      "Iteration 65, loss = 0.19275725\n",
      "Iteration 66, loss = 0.19147179\n",
      "Iteration 67, loss = 0.19262636\n",
      "Iteration 68, loss = 0.19088004\n",
      "Iteration 69, loss = 0.18907739\n",
      "Iteration 70, loss = 0.18815043\n",
      "Iteration 71, loss = 0.18837879\n",
      "Iteration 72, loss = 0.18690600\n",
      "Iteration 73, loss = 0.18619373\n",
      "Iteration 74, loss = 0.18545818\n",
      "Iteration 75, loss = 0.18359862\n",
      "Iteration 76, loss = 0.18539329\n",
      "Iteration 77, loss = 0.18302415\n",
      "Iteration 78, loss = 0.18482250\n",
      "Iteration 79, loss = 0.18134305\n",
      "Iteration 80, loss = 0.18428634\n",
      "Iteration 81, loss = 0.18174118\n",
      "Iteration 82, loss = 0.18012640\n",
      "Iteration 83, loss = 0.18078410\n",
      "Iteration 84, loss = 0.18117642\n",
      "Iteration 85, loss = 0.18019579\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8160 1052]\n",
      " [1512  495]]\n",
      "0.319974143504 0.2466367713 0.278559369724\n",
      "['BernoulliNB', 0.3385899814471243, 0.3637269556552068, 0.35070862358875815, 0.7590694357785899]\n",
      "['RandomForestClassifier', 0.6464646464646465, 0.0637767812655705, 0.11609977324263039, 0.8262768517693199]\n",
      "['BaggingClassifier', 0.626984126984127, 0.0787244643746886, 0.13988490482514387, 0.8268116587931188]\n",
      "['ExtraTreesClassifier', 0.4524590163934426, 0.0687593423019432, 0.11937716262975778, 0.8185221499242357]\n",
      "['DecisionTreeClassifier', 0.27924340467894476, 0.2795216741405082, 0.27938247011952194, 0.7420447455209912]\n",
      "['CalibratedClassifierCV', 0.6831683168316832, 0.0343796711509716, 0.06546489563567362, 0.8244050271860237]\n",
      "['SGDClassifier', 0.48195876288659795, 0.0931738913801694, 0.15615866388308974, 0.819859167483733]\n",
      "['KNeighborsClassifier', 0.27200902934537247, 0.24015944195316394, 0.25509394019581905, 0.7490863713343435]\n",
      "['MLPClassifier', 0.31997414350355524, 0.24663677130044842, 0.2785593697242543, 0.7714591318299314]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "0 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7461 1751]\n",
      " [1192  815]]\n",
      "0.317614964926 0.406078724464 0.356439973759\n",
      "Training  RandomForestClassifier\n",
      "[[9170   42]\n",
      " [1931   76]]\n",
      "0.64406779661 0.0378674638764 0.0715294117647\n",
      "Training  BaggingClassifier\n",
      "[[9124   88]\n",
      " [1879  128]]\n",
      "0.592592592593 0.0637767812656 0.115159694107\n",
      "Training  ExtraTreesClassifier\n",
      "[[9117   95]\n",
      " [1936   71]]\n",
      "0.427710843373 0.0353761833582 0.0653474459273\n",
      "Training  DecisionTreeClassifier\n",
      "[[7992 1220]\n",
      " [1588  419]]\n",
      "0.255643685174 0.208769307424 0.229840921558\n",
      "Training  CalibratedClassifierCV\n",
      "[[9189   23]\n",
      " [1946   61]]\n",
      "0.72619047619 0.0303936223219 0.0583452893352\n",
      "Training  SGDClassifier\n",
      "[[8921  291]\n",
      " [1756  251]]\n",
      "0.463099630996 0.125062282013 0.196939976461\n",
      "Training  KNeighborsClassifier\n",
      "[[8048 1164]\n",
      " [1528  479]]\n",
      "0.291539866099 0.238664673642 0.262465753425\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.38545695\n",
      "Iteration 2, loss = 0.34045677\n",
      "Iteration 3, loss = 0.32568083\n",
      "Iteration 4, loss = 0.31661979\n",
      "Iteration 5, loss = 0.30854139\n",
      "Iteration 6, loss = 0.29904471\n",
      "Iteration 7, loss = 0.29164450\n",
      "Iteration 8, loss = 0.28512878\n",
      "Iteration 9, loss = 0.27839174\n",
      "Iteration 10, loss = 0.27080153\n",
      "Iteration 11, loss = 0.26638388\n",
      "Iteration 12, loss = 0.25845360\n",
      "Iteration 13, loss = 0.25312913\n",
      "Iteration 14, loss = 0.24902547\n",
      "Iteration 15, loss = 0.24238075\n",
      "Iteration 16, loss = 0.23829955\n",
      "Iteration 17, loss = 0.23248873\n",
      "Iteration 18, loss = 0.22757660\n",
      "Iteration 19, loss = 0.22587728\n",
      "Iteration 20, loss = 0.21872635\n",
      "Iteration 21, loss = 0.21591303\n",
      "Iteration 22, loss = 0.21097136\n",
      "Iteration 23, loss = 0.20966492\n",
      "Iteration 24, loss = 0.20566347\n",
      "Iteration 25, loss = 0.20177807\n",
      "Iteration 26, loss = 0.19709057\n",
      "Iteration 27, loss = 0.19425159\n",
      "Iteration 28, loss = 0.19213843\n",
      "Iteration 29, loss = 0.18783136\n",
      "Iteration 30, loss = 0.18761321\n",
      "Iteration 31, loss = 0.18275926\n",
      "Iteration 32, loss = 0.18107627\n",
      "Iteration 33, loss = 0.17695951\n",
      "Iteration 34, loss = 0.17581660\n",
      "Iteration 35, loss = 0.17231483\n",
      "Iteration 36, loss = 0.17117397\n",
      "Iteration 37, loss = 0.16691403\n",
      "Iteration 38, loss = 0.16660258\n",
      "Iteration 39, loss = 0.16915901\n",
      "Iteration 40, loss = 0.16382573\n",
      "Iteration 41, loss = 0.15976810\n",
      "Iteration 42, loss = 0.16140748\n",
      "Iteration 43, loss = 0.15883423\n",
      "Iteration 44, loss = 0.15776033\n",
      "Iteration 45, loss = 0.15672742\n",
      "Iteration 46, loss = 0.15295881\n",
      "Iteration 47, loss = 0.15321244\n",
      "Iteration 48, loss = 0.14976109\n",
      "Iteration 49, loss = 0.15066544\n",
      "Iteration 50, loss = 0.14736447\n",
      "Iteration 51, loss = 0.14697325\n",
      "Iteration 52, loss = 0.14503038\n",
      "Iteration 53, loss = 0.14658352\n",
      "Iteration 54, loss = 0.14165709\n",
      "Iteration 55, loss = 0.14142489\n",
      "Iteration 56, loss = 0.14142237\n",
      "Iteration 57, loss = 0.14070967\n",
      "Iteration 58, loss = 0.14041661\n",
      "Iteration 59, loss = 0.13662653\n",
      "Iteration 60, loss = 0.13786424\n",
      "Iteration 61, loss = 0.13620065\n",
      "Iteration 62, loss = 0.13542506\n",
      "Iteration 63, loss = 0.13417860\n",
      "Iteration 64, loss = 0.13553916\n",
      "Iteration 65, loss = 0.13597395\n",
      "Iteration 66, loss = 0.13152419\n",
      "Iteration 67, loss = 0.13058034\n",
      "Iteration 68, loss = 0.13109797\n",
      "Iteration 69, loss = 0.13537121\n",
      "Iteration 70, loss = 0.12877923\n",
      "Iteration 71, loss = 0.13049174\n",
      "Iteration 72, loss = 0.12612671\n",
      "Iteration 73, loss = 0.12747711\n",
      "Iteration 74, loss = 0.12875822\n",
      "Iteration 75, loss = 0.12941599\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8769  443]\n",
      " [1686  321]]\n",
      "0.420157068063 0.159940209268 0.231685312162\n",
      "['BernoulliNB', 0.3176149649259548, 0.4060787244643747, 0.35643997375902037, 0.7376771548266334]\n",
      "['RandomForestClassifier', 0.6440677966101694, 0.037867463876432486, 0.07152941176470588, 0.8241376236741242]\n",
      "['BaggingClassifier', 0.5925925925925926, 0.0637767812655705, 0.11515969410706253, 0.8246724306979232]\n",
      "['ExtraTreesClassifier', 0.42771084337349397, 0.03537618335824614, 0.06534744592728946, 0.8189678224440681]\n",
      "['DecisionTreeClassifier', 0.2556436851738865, 0.20876930742401595, 0.22984092155787167, 0.7497103128621089]\n",
      "['CalibratedClassifierCV', 0.7261904761904762, 0.030393622321873443, 0.058345289335246284, 0.8244941616899902]\n",
      "['SGDClassifier', 0.46309963099630996, 0.12506228201295466, 0.1969399764613574, 0.8175416703806043]\n",
      "['KNeighborsClassifier', 0.2915398660986001, 0.2386646736422521, 0.2624657534246575, 0.7600499153222212]\n",
      "['MLPClassifier', 0.4201570680628272, 0.15994020926756353, 0.23168531216167446, 0.8102326410553525]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'csvReplaceBR12'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. next word2vec model taken where it was trained with phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trainee/tensorflow/lib/python2.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "5 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7271 1941]\n",
      " [1154  853]]\n",
      "0.30529706514 0.425012456403 0.355342636951\n",
      "Training  RandomForestClassifier\n",
      "[[9146   66]\n",
      " [1904  103]]\n",
      "0.609467455621 0.0513203786746 0.0946691176471\n",
      "Training  BaggingClassifier\n",
      "[[9099  113]\n",
      " [1872  135]]\n",
      "0.54435483871 0.067264573991 0.119733924612\n",
      "Training  ExtraTreesClassifier\n",
      "[[9047  165]\n",
      " [1880  127]]\n",
      "0.434931506849 0.0632785251619 0.110482818617\n",
      "Training  DecisionTreeClassifier\n",
      "[[7919 1293]\n",
      " [1526  481]]\n",
      "0.271138669673 0.23966118585 0.254430044962\n",
      "Training  CalibratedClassifierCV\n",
      "[[9176   36]\n",
      " [1933   74]]\n",
      "0.672727272727 0.0368709516692 0.0699102503543\n",
      "Training  SGDClassifier\n",
      "[[9177   35]\n",
      " [1967   40]]\n",
      "0.533333333333 0.0199302441455 0.0384245917387\n",
      "Training  KNeighborsClassifier\n",
      "[[7821 1391]\n",
      " [1441  566]]\n",
      "0.289218191109 0.282012954659 0.285570131181\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.38366683\n",
      "Iteration 2, loss = 0.34687879\n",
      "Iteration 3, loss = 0.33910307\n",
      "Iteration 4, loss = 0.33202739\n",
      "Iteration 5, loss = 0.32372011\n",
      "Iteration 6, loss = 0.31931855\n",
      "Iteration 7, loss = 0.31212704\n",
      "Iteration 8, loss = 0.30818695\n",
      "Iteration 9, loss = 0.30341878\n",
      "Iteration 10, loss = 0.29937647\n",
      "Iteration 11, loss = 0.29486079\n",
      "Iteration 12, loss = 0.29249202\n",
      "Iteration 13, loss = 0.28901656\n",
      "Iteration 14, loss = 0.28678345\n",
      "Iteration 15, loss = 0.28307738\n",
      "Iteration 16, loss = 0.28149681\n",
      "Iteration 17, loss = 0.27703106\n",
      "Iteration 18, loss = 0.27456733\n",
      "Iteration 19, loss = 0.26992886\n",
      "Iteration 20, loss = 0.26922536\n",
      "Iteration 21, loss = 0.26671292\n",
      "Iteration 22, loss = 0.26495155\n",
      "Iteration 23, loss = 0.26349719\n",
      "Iteration 24, loss = 0.25841497\n",
      "Iteration 25, loss = 0.25737859\n",
      "Iteration 26, loss = 0.25641943\n",
      "Iteration 27, loss = 0.25577585\n",
      "Iteration 28, loss = 0.25272428\n",
      "Iteration 29, loss = 0.24990381\n",
      "Iteration 30, loss = 0.24954255\n",
      "Iteration 31, loss = 0.24688725\n",
      "Iteration 32, loss = 0.24543811\n",
      "Iteration 33, loss = 0.24323269\n",
      "Iteration 34, loss = 0.24250487\n",
      "Iteration 35, loss = 0.24046094\n",
      "Iteration 36, loss = 0.23944479\n",
      "Iteration 37, loss = 0.23988791\n",
      "Iteration 38, loss = 0.23494796\n",
      "Iteration 39, loss = 0.23401670\n",
      "Iteration 40, loss = 0.23562487\n",
      "Iteration 41, loss = 0.23101737\n",
      "Iteration 42, loss = 0.23048083\n",
      "Iteration 43, loss = 0.22973806\n",
      "Iteration 44, loss = 0.22843587\n",
      "Iteration 45, loss = 0.22725711\n",
      "Iteration 46, loss = 0.22728502\n",
      "Iteration 47, loss = 0.22768535\n",
      "Iteration 48, loss = 0.22410508\n",
      "Iteration 49, loss = 0.22244206\n",
      "Iteration 50, loss = 0.22135774\n",
      "Iteration 51, loss = 0.22082406\n",
      "Iteration 52, loss = 0.21819993\n",
      "Iteration 53, loss = 0.21944037\n",
      "Iteration 54, loss = 0.21594324\n",
      "Iteration 55, loss = 0.21581203\n",
      "Iteration 56, loss = 0.21505825\n",
      "Iteration 57, loss = 0.21459484\n",
      "Iteration 58, loss = 0.21306709\n",
      "Iteration 59, loss = 0.21234599\n",
      "Iteration 60, loss = 0.21342884\n",
      "Iteration 61, loss = 0.20936852\n",
      "Iteration 62, loss = 0.21020020\n",
      "Iteration 63, loss = 0.20874892\n",
      "Iteration 64, loss = 0.20676872\n",
      "Iteration 65, loss = 0.20953381\n",
      "Iteration 66, loss = 0.20565505\n",
      "Iteration 67, loss = 0.20831957\n",
      "Iteration 68, loss = 0.20548033\n",
      "Iteration 69, loss = 0.20390807\n",
      "Iteration 70, loss = 0.20244461\n",
      "Iteration 71, loss = 0.20395087\n",
      "Iteration 72, loss = 0.20076924\n",
      "Iteration 73, loss = 0.20114585\n",
      "Iteration 74, loss = 0.20006531\n",
      "Iteration 75, loss = 0.19923899\n",
      "Iteration 76, loss = 0.19830228\n",
      "Iteration 77, loss = 0.19763994\n",
      "Iteration 78, loss = 0.19746007\n",
      "Iteration 79, loss = 0.19771641\n",
      "Iteration 80, loss = 0.19650220\n",
      "Iteration 81, loss = 0.19686815\n",
      "Iteration 82, loss = 0.19576350\n",
      "Iteration 83, loss = 0.19373690\n",
      "Iteration 84, loss = 0.19428377\n",
      "Iteration 85, loss = 0.19344321\n",
      "Iteration 86, loss = 0.19138587\n",
      "Iteration 87, loss = 0.19188236\n",
      "Iteration 88, loss = 0.19091567\n",
      "Iteration 89, loss = 0.19135825\n",
      "Iteration 90, loss = 0.19414390\n",
      "Iteration 91, loss = 0.18834745\n",
      "Iteration 92, loss = 0.18972347\n",
      "Iteration 93, loss = 0.18916209\n",
      "Iteration 94, loss = 0.18546199\n",
      "Iteration 95, loss = 0.18712034\n",
      "Iteration 96, loss = 0.18575806\n",
      "Iteration 97, loss = 0.18482185\n",
      "Iteration 98, loss = 0.18630586\n",
      "Iteration 99, loss = 0.18392946\n",
      "Iteration 100, loss = 0.18513785\n",
      "Iteration 101, loss = 0.18329100\n",
      "Iteration 102, loss = 0.18434405\n",
      "Iteration 103, loss = 0.18474150\n",
      "Iteration 104, loss = 0.18559611\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8541  671]\n",
      " [1570  437]]\n",
      "0.39440433213 0.217737917289 0.280577849117\n",
      "['BernoulliNB', 0.3052970651395848, 0.42501245640259094, 0.3553426369506353, 0.7241287102237276]\n",
      "['RandomForestClassifier', 0.6094674556213018, 0.05132037867463876, 0.09466911764705882, 0.8244050271860237]\n",
      "['BaggingClassifier', 0.5443548387096774, 0.06726457399103139, 0.11973392461197341, 0.8230680096265264]\n",
      "['ExtraTreesClassifier', 0.4349315068493151, 0.06327852516193323, 0.11048281861678991, 0.8177199393885373]\n",
      "['DecisionTreeClassifier', 0.2711386696730552, 0.23966118584952667, 0.25443004496165034, 0.7487298333184775]\n",
      "['CalibratedClassifierCV', 0.6727272727272727, 0.03687095166915795, 0.06991025035427492, 0.8244941616899902]\n",
      "['SGDClassifier', 0.5333333333333333, 0.019930244145490782, 0.03842459173871277, 0.8215527230590962]\n",
      "['KNeighborsClassifier', 0.28921819110884006, 0.2820129546586946, 0.28557013118062563, 0.7475710847669133]\n",
      "['MLPClassifier', 0.3944043321299639, 0.2177379172894868, 0.2805778491171749, 0.8002495766111062]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_trans = Phrases(trainFSen)\n",
    "\n",
    "modelName = 'csv_3_ReplaceBR_bigram'\n",
    "train_vect, test_vect = buildW2V(modelName,bigram_trans[trainFSen], bigram_trans[testFSen])\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Middle Sentences\n",
    "Until now we were taking into consideration the whole sentence. But on closer inspection most the connection related words appear near of BR1 and BR2, mostly in-between them and just before and just after. So this time we will be only considering words in between the entities taken into consideration and some words before and after the occurence of it say a window of size 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def breakSen(sentences):\n",
    "    middleSen = []\n",
    "    for sentence in sentences:\n",
    "        s = sentence.split()\n",
    "\n",
    "        ind1 = -1\n",
    "        ind2 = -1\n",
    "        for j in range(0,len(s)):\n",
    "            if s[j].__contains__('br'):\n",
    "                if ind1 == -1:\n",
    "                    ind1 = j\n",
    "                else:\n",
    "                    ind2 = j\n",
    "        for j in range(0,len(s)):\n",
    "            if s[j].__contains__('br'):\n",
    "                if j > ind2:\n",
    "                    ind2 = j\n",
    "\n",
    "        if ind1-3 < 0:\n",
    "            ind1 = 0\n",
    "        else: ind1 -= 3\n",
    "\n",
    "        if ind2+3 > len(s):\n",
    "            ind2 = len(s)\n",
    "        else: ind2 += 3\n",
    "\n",
    "        middleSen.append(' '.join(s[ind1:ind2]))\n",
    "    return middleSen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for this', 'for this', 'for this', 'for this', 'for this']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMid = breakSen(trainFSen)\n",
    "testMid = breakSen(testFSen)\n",
    "trainMid[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  BernoulliNB\n",
      "[[8284  928]\n",
      " [1428  579]]\n",
      "0.384207033842 0.288490284006 0.32953898691\n",
      "Training  RandomForestClassifier\n",
      "[[9121   91]\n",
      " [1796  211]]\n",
      "0.698675496689 0.105132037867 0.182763100909\n",
      "Training  BaggingClassifier\n",
      "[[8695  517]\n",
      " [1353  654]]\n",
      "0.558497011102 0.325859491779 0.411579609817\n",
      "Training  ExtraTreesClassifier\n",
      "[[9090  122]\n",
      " [1833  174]]\n",
      "0.587837837838 0.0866965620329 0.151107251411\n",
      "Training  DecisionTreeClassifier\n",
      "[[8120 1092]\n",
      " [1265  742]]\n",
      "0.404580152672 0.369706028899 0.386357719344\n",
      "Training  CalibratedClassifierCV\n",
      "[[9037  175]\n",
      " [1761  246]]\n",
      "0.58432304038 0.122571001495 0.202635914333\n",
      "Training  SGDClassifier\n",
      "[[9129   83]\n",
      " [1853  154]]\n",
      "0.649789029536 0.0767314399601 0.137254901961\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.39351100\n",
      "Iteration 2, loss = 0.20047421\n",
      "Iteration 3, loss = 0.13153227\n",
      "Iteration 4, loss = 0.10019751\n",
      "Iteration 5, loss = 0.08356953\n",
      "Iteration 6, loss = 0.07469413\n",
      "Iteration 7, loss = 0.06617498\n",
      "Iteration 8, loss = 0.06290669\n",
      "Iteration 9, loss = 0.05741581\n",
      "Iteration 10, loss = 0.05498478\n",
      "Iteration 11, loss = 0.05208868\n",
      "Iteration 12, loss = 0.05045136\n",
      "Iteration 13, loss = 0.04825104\n",
      "Iteration 14, loss = 0.04750534\n",
      "Iteration 15, loss = 0.04550677\n",
      "Iteration 16, loss = 0.04455421\n",
      "Iteration 17, loss = 0.04383512\n",
      "Iteration 18, loss = 0.04296961\n",
      "Iteration 19, loss = 0.04237256\n",
      "Iteration 20, loss = 0.04168362\n",
      "Iteration 21, loss = 0.04115859\n",
      "Iteration 22, loss = 0.04099542\n",
      "Iteration 23, loss = 0.04064284\n",
      "Iteration 24, loss = 0.04007016\n",
      "Iteration 25, loss = 0.04015717\n",
      "Iteration 26, loss = 0.03955595\n",
      "Iteration 27, loss = 0.03959450\n",
      "Iteration 28, loss = 0.03892291\n",
      "Iteration 29, loss = 0.03914000\n",
      "Iteration 30, loss = 0.03841333\n",
      "Iteration 31, loss = 0.03808188\n",
      "Iteration 32, loss = 0.03873955\n",
      "Iteration 33, loss = 0.03846582\n",
      "Iteration 34, loss = 0.03892555\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8597  615]\n",
      " [1513  494]]\n",
      "0.445446348061 0.246138515197 0.317073170732\n",
      "['BernoulliNB', 0.38420703384207033, 0.2884902840059791, 0.32953898690950484, 0.7899991086549604]\n",
      "['RandomForestClassifier', 0.6986754966887417, 0.10513203786746388, 0.18276310090948464, 0.831803191015242]\n",
      "['BaggingClassifier', 0.5584970111016225, 0.32585949177877427, 0.4115796098174953, 0.8333184775826723]\n",
      "['ExtraTreesClassifier', 0.5878378378378378, 0.08669656203288491, 0.1511072514112028, 0.825742044745521]\n",
      "['DecisionTreeClassifier', 0.40458015267175573, 0.369706028898854, 0.3863577193439209, 0.7899099741509938]\n",
      "['CalibratedClassifierCV', 0.5843230403800475, 0.12257100149476831, 0.20263591433278416, 0.8274356003208843]\n",
      "['SGDClassifier', 0.6497890295358649, 0.07673143996013951, 0.1372549019607843, 0.8274356003208843]\n",
      "['MLPClassifier', 0.4454463480613165, 0.24613851519681115, 0.3170731707317073, 0.8103217755593191]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_vect, test_vect = convertTfidf(trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "11 22572\n",
      "Embedding... 11219\n",
      "1 11219\n",
      "Training  BernoulliNB\n",
      "[[7594 1618]\n",
      " [1133  874]]\n",
      "0.350722311396 0.435475834579 0.388530784619\n",
      "Training  RandomForestClassifier\n",
      "[[9161   51]\n",
      " [1893  114]]\n",
      "0.690909090909 0.0568011958146 0.104972375691\n",
      "Training  BaggingClassifier\n",
      "[[9119   93]\n",
      " [1864  143]]\n",
      "0.60593220339 0.0712506228201 0.127507802051\n",
      "Training  ExtraTreesClassifier\n",
      "[[9077  135]\n",
      " [1857  150]]\n",
      "0.526315789474 0.0747384155456 0.130890052356\n",
      "Training  DecisionTreeClassifier\n",
      "[[7902 1310]\n",
      " [1527  480]]\n",
      "0.268156424581 0.239162929746 0.252831182513\n",
      "Training  CalibratedClassifierCV\n",
      "[[9154   58]\n",
      " [1958   49]]\n",
      "0.457943925234 0.0244145490782 0.046357615894\n",
      "Training  SGDClassifier\n",
      "[[9144   68]\n",
      " [1936   71]]\n",
      "0.510791366906 0.0353761833582 0.0661696178938\n",
      "Training  KNeighborsClassifier\n",
      "[[7817 1395]\n",
      " [1354  653]]\n",
      "0.31884765625 0.325361235675 0.322071516646\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35755027\n",
      "Iteration 2, loss = 0.31784005\n",
      "Iteration 3, loss = 0.30402039\n",
      "Iteration 4, loss = 0.29327596\n",
      "Iteration 5, loss = 0.28273416\n",
      "Iteration 6, loss = 0.27200653\n",
      "Iteration 7, loss = 0.26268458\n",
      "Iteration 8, loss = 0.25580072\n",
      "Iteration 9, loss = 0.24660260\n",
      "Iteration 10, loss = 0.24006258\n",
      "Iteration 11, loss = 0.23350991\n",
      "Iteration 12, loss = 0.22724335\n",
      "Iteration 13, loss = 0.21993870\n",
      "Iteration 14, loss = 0.21295265\n",
      "Iteration 15, loss = 0.20902784\n",
      "Iteration 16, loss = 0.20266449\n",
      "Iteration 17, loss = 0.19851579\n",
      "Iteration 18, loss = 0.19522123\n",
      "Iteration 19, loss = 0.19088493\n",
      "Iteration 20, loss = 0.18520525\n",
      "Iteration 21, loss = 0.18115695\n",
      "Iteration 22, loss = 0.17598824\n",
      "Iteration 23, loss = 0.17295123\n",
      "Iteration 24, loss = 0.16842490\n",
      "Iteration 25, loss = 0.16548671\n",
      "Iteration 26, loss = 0.16793071\n",
      "Iteration 27, loss = 0.15852889\n",
      "Iteration 28, loss = 0.15611211\n",
      "Iteration 29, loss = 0.15279713\n",
      "Iteration 30, loss = 0.15057948\n",
      "Iteration 31, loss = 0.14622681\n",
      "Iteration 32, loss = 0.14263165\n",
      "Iteration 33, loss = 0.14409617\n",
      "Iteration 34, loss = 0.14178623\n",
      "Iteration 35, loss = 0.13948169\n",
      "Iteration 36, loss = 0.13557194\n",
      "Iteration 37, loss = 0.13837092\n",
      "Iteration 38, loss = 0.12987149\n",
      "Iteration 39, loss = 0.12973791\n",
      "Iteration 40, loss = 0.12741498\n",
      "Iteration 41, loss = 0.12539379\n",
      "Iteration 42, loss = 0.12627181\n",
      "Iteration 43, loss = 0.12225653\n",
      "Iteration 44, loss = 0.12083808\n",
      "Iteration 45, loss = 0.12082513\n",
      "Iteration 46, loss = 0.11915547\n",
      "Iteration 47, loss = 0.11843764\n",
      "Iteration 48, loss = 0.11611198\n",
      "Iteration 49, loss = 0.11404018\n",
      "Iteration 50, loss = 0.11197682\n",
      "Iteration 51, loss = 0.11299699\n",
      "Iteration 52, loss = 0.11537605\n",
      "Iteration 53, loss = 0.11021915\n",
      "Iteration 54, loss = 0.10774942\n",
      "Iteration 55, loss = 0.10901662\n",
      "Iteration 56, loss = 0.10707192\n",
      "Iteration 57, loss = 0.10752201\n",
      "Iteration 58, loss = 0.10597866\n",
      "Iteration 59, loss = 0.10517716\n",
      "Iteration 60, loss = 0.10202578\n",
      "Iteration 61, loss = 0.10256741\n",
      "Iteration 62, loss = 0.10382154\n",
      "Iteration 63, loss = 0.10043922\n",
      "Iteration 64, loss = 0.09942723\n",
      "Iteration 65, loss = 0.09932621\n",
      "Iteration 66, loss = 0.09843397\n",
      "Iteration 67, loss = 0.09884542\n",
      "Iteration 68, loss = 0.09673577\n",
      "Iteration 69, loss = 0.09747830\n",
      "Iteration 70, loss = 0.09534280\n",
      "Iteration 71, loss = 0.09330714\n",
      "Iteration 72, loss = 0.09500277\n",
      "Iteration 73, loss = 0.09214478\n",
      "Iteration 74, loss = 0.09264903\n",
      "Iteration 75, loss = 0.09617237\n",
      "Iteration 76, loss = 0.09265237\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[7959 1253]\n",
      " [1363  644]]\n",
      "0.339483394834 0.320876930742 0.329918032787\n",
      "['BernoulliNB', 0.3507223113964687, 0.4354758345789736, 0.38853078461880414, 0.7547909795881986]\n",
      "['RandomForestClassifier', 0.6909090909090909, 0.05680119581464873, 0.10497237569060773, 0.8267225242891524]\n",
      "['BaggingClassifier', 0.6059322033898306, 0.07125062282012955, 0.1275078020508248, 0.825563775737588]\n",
      "['ExtraTreesClassifier', 0.5263157894736842, 0.07473841554559044, 0.13089005235602094, 0.822444068098761]\n",
      "['DecisionTreeClassifier', 0.2681564245810056, 0.23916292974588937, 0.25283118251250986, 0.7471254122470808]\n",
      "['CalibratedClassifierCV', 0.45794392523364486, 0.024414549078226207, 0.04635761589403973, 0.8203048400035654]\n",
      "['SGDClassifier', 0.5107913669064749, 0.03537618335824614, 0.06616961789375583, 0.8213744540511632]\n",
      "['KNeighborsClassifier', 0.31884765625, 0.32536123567513703, 0.32207151664611594, 0.7549692485961316]\n",
      "['MLPClassifier', 0.33948339483394835, 0.3208769307424016, 0.32991803278688525, 0.7668241376236741]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "29 22572\n",
      "Embedding... 11219\n",
      "7 11219\n",
      "Training  BernoulliNB\n",
      "[[7784 1428]\n",
      " [1224  783]]\n",
      "0.354138398915 0.390134529148 0.371266002845\n",
      "Training  RandomForestClassifier\n",
      "[[9138   74]\n",
      " [1867  140]]\n",
      "0.654205607477 0.0697558545092 0.126069338136\n",
      "Training  BaggingClassifier\n",
      "[[9054  158]\n",
      " [1794  213]]\n",
      "0.574123989218 0.106128550075 0.179142136249\n",
      "Training  ExtraTreesClassifier\n",
      "[[9033  179]\n",
      " [1838  169]]\n",
      "0.485632183908 0.0842052815147 0.143524416136\n",
      "Training  DecisionTreeClassifier\n",
      "[[7872 1340]\n",
      " [1488  519]]\n",
      "0.279182356105 0.258594917788 0.268494568029\n",
      "Training  CalibratedClassifierCV\n",
      "[[9171   41]\n",
      " [1974   33]]\n",
      "0.445945945946 0.01644245142 0.031715521384\n",
      "Training  SGDClassifier\n",
      "[[9057  155]\n",
      " [1889  118]]\n",
      "0.432234432234 0.0587942202292 0.10350877193\n",
      "Training  KNeighborsClassifier\n",
      "[[7722 1490]\n",
      " [1361  646]]\n",
      "0.302434456929 0.32187344295 0.311851315472\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.36606436\n",
      "Iteration 2, loss = 0.32712804\n",
      "Iteration 3, loss = 0.31491537\n",
      "Iteration 4, loss = 0.30334866\n",
      "Iteration 5, loss = 0.29210532\n",
      "Iteration 6, loss = 0.28278856\n",
      "Iteration 7, loss = 0.27308979\n",
      "Iteration 8, loss = 0.26591978\n",
      "Iteration 9, loss = 0.25925458\n",
      "Iteration 10, loss = 0.25197922\n",
      "Iteration 11, loss = 0.24379099\n",
      "Iteration 12, loss = 0.23908494\n",
      "Iteration 13, loss = 0.23701290\n",
      "Iteration 14, loss = 0.22811234\n",
      "Iteration 15, loss = 0.22411707\n",
      "Iteration 16, loss = 0.21705743\n",
      "Iteration 17, loss = 0.21479822\n",
      "Iteration 18, loss = 0.21491860\n",
      "Iteration 19, loss = 0.20835583\n",
      "Iteration 20, loss = 0.20206173\n",
      "Iteration 21, loss = 0.19873052\n",
      "Iteration 22, loss = 0.19632266\n",
      "Iteration 23, loss = 0.19264321\n",
      "Iteration 24, loss = 0.18594933\n",
      "Iteration 25, loss = 0.18643186\n",
      "Iteration 26, loss = 0.18154498\n",
      "Iteration 27, loss = 0.17873172\n",
      "Iteration 28, loss = 0.17653804\n",
      "Iteration 29, loss = 0.17254120\n",
      "Iteration 30, loss = 0.16909006\n",
      "Iteration 31, loss = 0.16807414\n",
      "Iteration 32, loss = 0.16663623\n",
      "Iteration 33, loss = 0.16235167\n",
      "Iteration 34, loss = 0.16302121\n",
      "Iteration 35, loss = 0.15749173\n",
      "Iteration 36, loss = 0.15476290\n",
      "Iteration 37, loss = 0.15432154\n",
      "Iteration 38, loss = 0.15222398\n",
      "Iteration 39, loss = 0.14784033\n",
      "Iteration 40, loss = 0.14867955\n",
      "Iteration 41, loss = 0.14723159\n",
      "Iteration 42, loss = 0.14553813\n",
      "Iteration 43, loss = 0.14137333\n",
      "Iteration 44, loss = 0.14089376\n",
      "Iteration 45, loss = 0.13948930\n",
      "Iteration 46, loss = 0.14007759\n",
      "Iteration 47, loss = 0.13437372\n",
      "Iteration 48, loss = 0.13291853\n",
      "Iteration 49, loss = 0.13282529\n",
      "Iteration 50, loss = 0.13217453\n",
      "Iteration 51, loss = 0.12985245\n",
      "Iteration 52, loss = 0.12860057\n",
      "Iteration 53, loss = 0.13194492\n",
      "Iteration 54, loss = 0.12509176\n",
      "Iteration 55, loss = 0.12758180\n",
      "Iteration 56, loss = 0.12734771\n",
      "Iteration 57, loss = 0.12221021\n",
      "Iteration 58, loss = 0.12332576\n",
      "Iteration 59, loss = 0.12133972\n",
      "Iteration 60, loss = 0.11903494\n",
      "Iteration 61, loss = 0.11647182\n",
      "Iteration 62, loss = 0.11921784\n",
      "Iteration 63, loss = 0.11843737\n",
      "Iteration 64, loss = 0.11581753\n",
      "Iteration 65, loss = 0.11642430\n",
      "Iteration 66, loss = 0.11496902\n",
      "Iteration 67, loss = 0.11515253\n",
      "Iteration 68, loss = 0.11495163\n",
      "Iteration 69, loss = 0.11196046\n",
      "Iteration 70, loss = 0.11058360\n",
      "Iteration 71, loss = 0.11038912\n",
      "Iteration 72, loss = 0.11016300\n",
      "Iteration 73, loss = 0.10890632\n",
      "Iteration 74, loss = 0.10995811\n",
      "Iteration 75, loss = 0.11017292\n",
      "Iteration 76, loss = 0.10906089\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8053 1159]\n",
      " [1354  653]]\n",
      "0.360375275938 0.325361235675 0.341974338832\n",
      "['BernoulliNB', 0.3541383989145183, 0.3901345291479821, 0.3712660028449502, 0.7636152954808807]\n",
      "['RandomForestClassifier', 0.6542056074766355, 0.06975585450921774, 0.12606933813597482, 0.8269899278010517]\n",
      "['BaggingClassifier', 0.5741239892183289, 0.10612855007473841, 0.1791421362489487, 0.8260094482574204]\n",
      "['ExtraTreesClassifier', 0.48563218390804597, 0.08420528151469855, 0.1435244161358811, 0.8202157054995989]\n",
      "['DecisionTreeClassifier', 0.279182356105433, 0.2585949177877429, 0.26849456802897054, 0.7479276227827792]\n",
      "['CalibratedClassifierCV', 0.44594594594594594, 0.016442451420029897, 0.03171552138395003, 0.8203939745075318]\n",
      "['SGDClassifier', 0.43223443223443225, 0.05879422022919781, 0.10350877192982456, 0.8178090738925038]\n",
      "['KNeighborsClassifier', 0.30243445692883897, 0.3218734429496761, 0.3118513154718803, 0.74587752919155]\n",
      "['MLPClassifier', 0.36037527593818985, 0.32536123567513703, 0.341974338832155, 0.7760049915322221]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "1 22572\n",
      "Embedding... 11219\n",
      "4 11219\n",
      "Training  BernoulliNB\n",
      "[[7102 2110]\n",
      " [ 991 1016]]\n",
      "0.325015994882 0.506228201295 0.395869861679\n",
      "Training  RandomForestClassifier\n",
      "[[9180   32]\n",
      " [1915   92]]\n",
      "0.741935483871 0.0458395615346 0.0863444392304\n",
      "Training  BaggingClassifier\n",
      "[[9145   67]\n",
      " [1880  127]]\n",
      "0.654639175258 0.0632785251619 0.115402089959\n",
      "Training  ExtraTreesClassifier\n",
      "[[9100  112]\n",
      " [1852  155]]\n",
      "0.580524344569 0.0772296960638 0.136323658751\n",
      "Training  DecisionTreeClassifier\n",
      "[[8026 1186]\n",
      " [1490  517]]\n",
      "0.303581914269 0.25759840558 0.278706199461\n",
      "Training  CalibratedClassifierCV\n",
      "[[9169   43]\n",
      " [1972   35]]\n",
      "0.448717948718 0.0174389636273 0.0335731414868\n",
      "Training  SGDClassifier\n",
      "[[8691  521]\n",
      " [1636  371]]\n",
      "0.415919282511 0.184853014449 0.255950327699\n",
      "Training  KNeighborsClassifier\n",
      "[[7878 1334]\n",
      " [1364  643]]\n",
      "0.325240263025 0.320378674639 0.322791164659\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35462162\n",
      "Iteration 2, loss = 0.31490376\n",
      "Iteration 3, loss = 0.29902402\n",
      "Iteration 4, loss = 0.28558311\n",
      "Iteration 5, loss = 0.27380555\n",
      "Iteration 6, loss = 0.26184207\n",
      "Iteration 7, loss = 0.25283126\n",
      "Iteration 8, loss = 0.24363984\n",
      "Iteration 9, loss = 0.23664507\n",
      "Iteration 10, loss = 0.22646139\n",
      "Iteration 11, loss = 0.21691344\n",
      "Iteration 12, loss = 0.21140350\n",
      "Iteration 13, loss = 0.20465309\n",
      "Iteration 14, loss = 0.19828663\n",
      "Iteration 15, loss = 0.19397563\n",
      "Iteration 16, loss = 0.18636906\n",
      "Iteration 17, loss = 0.17893170\n",
      "Iteration 18, loss = 0.17344070\n",
      "Iteration 19, loss = 0.16979636\n",
      "Iteration 20, loss = 0.16555653\n",
      "Iteration 21, loss = 0.16015747\n",
      "Iteration 22, loss = 0.15880996\n",
      "Iteration 23, loss = 0.15359649\n",
      "Iteration 24, loss = 0.14701431\n",
      "Iteration 25, loss = 0.14423338\n",
      "Iteration 26, loss = 0.14116302\n",
      "Iteration 27, loss = 0.13829184\n",
      "Iteration 28, loss = 0.13590940\n",
      "Iteration 29, loss = 0.13107718\n",
      "Iteration 30, loss = 0.13182796\n",
      "Iteration 31, loss = 0.12589800\n",
      "Iteration 32, loss = 0.12514039\n",
      "Iteration 33, loss = 0.12172497\n",
      "Iteration 34, loss = 0.11931107\n",
      "Iteration 35, loss = 0.11727490\n",
      "Iteration 36, loss = 0.11706877\n",
      "Iteration 37, loss = 0.11393940\n",
      "Iteration 38, loss = 0.11080517\n",
      "Iteration 39, loss = 0.11163394\n",
      "Iteration 40, loss = 0.10810241\n",
      "Iteration 41, loss = 0.10768460\n",
      "Iteration 42, loss = 0.10877536\n",
      "Iteration 43, loss = 0.10483117\n",
      "Iteration 44, loss = 0.10470952\n",
      "Iteration 45, loss = 0.09939697\n",
      "Iteration 46, loss = 0.10131505\n",
      "Iteration 47, loss = 0.09854802\n",
      "Iteration 48, loss = 0.10236401\n",
      "Iteration 49, loss = 0.09844841\n",
      "Iteration 50, loss = 0.09590313\n",
      "Iteration 51, loss = 0.09520602\n",
      "Iteration 52, loss = 0.09484322\n",
      "Iteration 53, loss = 0.09521788\n",
      "Iteration 54, loss = 0.09252962\n",
      "Iteration 55, loss = 0.09401160\n",
      "Iteration 56, loss = 0.09291940\n",
      "Iteration 57, loss = 0.09493319\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[7919 1293]\n",
      " [1347  660]]\n",
      "0.337941628264 0.328849028401 0.333333333333\n",
      "['BernoulliNB', 0.32501599488163785, 0.5062282012954659, 0.3958698616793298, 0.7235939031999287]\n",
      "['RandomForestClassifier', 0.7419354838709677, 0.0458395615346288, 0.08634443923040827, 0.8264551207772529]\n",
      "['BaggingClassifier', 0.654639175257732, 0.06327852516193323, 0.1154020899591095, 0.8264551207772529]\n",
      "['ExtraTreesClassifier', 0.5805243445692884, 0.07722969606377678, 0.1363236587510994, 0.8249398342098226]\n",
      "['DecisionTreeClassifier', 0.30358191426893716, 0.25759840558046837, 0.27870619946091646, 0.761476067385685]\n",
      "['CalibratedClassifierCV', 0.44871794871794873, 0.017438963627304436, 0.03357314148681055, 0.8203939745075318]\n",
      "['SGDClassifier', 0.41591928251121074, 0.184853014449427, 0.2559503276992066, 0.8077368749442909]\n",
      "['KNeighborsClassifier', 0.32524026302478504, 0.32037867463876435, 0.32279116465863456, 0.7595151082984223]\n",
      "['MLPClassifier', 0.3379416282642089, 0.32884902840059793, 0.33333333333333337, 0.7646849095284785]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'csvReplaceBR12'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. next word2vec model taken where it was trained with phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "29 22572\n",
      "Embedding... 11219\n",
      "7 11219\n",
      "Training  BernoulliNB\n",
      "[[7260 1952]\n",
      " [1025  982]]\n",
      "0.33469665985 0.489287493772 0.397490386561\n",
      "Training  RandomForestClassifier\n",
      "[[9127   85]\n",
      " [1859  148]]\n",
      "0.635193133047 0.0737419033383 0.132142857143\n",
      "Training  BaggingClassifier\n",
      "[[9047  165]\n",
      " [1796  211]]\n",
      "0.561170212766 0.105132037867 0.177087704574\n",
      "Training  ExtraTreesClassifier\n",
      "[[9022  190]\n",
      " [1795  212]]\n",
      "0.52736318408 0.105630293971 0.17600664176\n",
      "Training  DecisionTreeClassifier\n",
      "[[7799 1413]\n",
      " [1465  542]]\n",
      "0.277237851662 0.270054808171 0.273599192327\n",
      "Training  CalibratedClassifierCV\n",
      "[[9176   36]\n",
      " [1982   25]]\n",
      "0.409836065574 0.0124564025909 0.0241779497099\n",
      "Training  SGDClassifier\n",
      "[[9078  134]\n",
      " [1943   64]]\n",
      "0.323232323232 0.0318883906328 0.0580498866213\n",
      "Training  KNeighborsClassifier\n",
      "[[7900 1312]\n",
      " [1353  654]]\n",
      "0.332655137335 0.325859491779 0.329222250189\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.36678657\n",
      "Iteration 2, loss = 0.32997380\n",
      "Iteration 3, loss = 0.31787366\n",
      "Iteration 4, loss = 0.30751700\n",
      "Iteration 5, loss = 0.29821983\n",
      "Iteration 6, loss = 0.28975119\n",
      "Iteration 7, loss = 0.28371464\n",
      "Iteration 8, loss = 0.27465521\n",
      "Iteration 9, loss = 0.26917036\n",
      "Iteration 10, loss = 0.26193316\n",
      "Iteration 11, loss = 0.25665469\n",
      "Iteration 12, loss = 0.25111694\n",
      "Iteration 13, loss = 0.24660618\n",
      "Iteration 14, loss = 0.24108213\n",
      "Iteration 15, loss = 0.23635706\n",
      "Iteration 16, loss = 0.23133642\n",
      "Iteration 17, loss = 0.23101928\n",
      "Iteration 18, loss = 0.22402636\n",
      "Iteration 19, loss = 0.21935717\n",
      "Iteration 20, loss = 0.21563356\n",
      "Iteration 21, loss = 0.21275810\n",
      "Iteration 22, loss = 0.20890012\n",
      "Iteration 23, loss = 0.20746422\n",
      "Iteration 24, loss = 0.20324586\n",
      "Iteration 25, loss = 0.20071288\n",
      "Iteration 26, loss = 0.19701893\n",
      "Iteration 27, loss = 0.19336374\n",
      "Iteration 28, loss = 0.19141358\n",
      "Iteration 29, loss = 0.18742989\n",
      "Iteration 30, loss = 0.18602157\n",
      "Iteration 31, loss = 0.18331423\n",
      "Iteration 32, loss = 0.17848926\n",
      "Iteration 33, loss = 0.17678528\n",
      "Iteration 34, loss = 0.17370649\n",
      "Iteration 35, loss = 0.17261875\n",
      "Iteration 36, loss = 0.17120362\n",
      "Iteration 37, loss = 0.16940079\n",
      "Iteration 38, loss = 0.16692553\n",
      "Iteration 39, loss = 0.16393722\n",
      "Iteration 40, loss = 0.16155759\n",
      "Iteration 41, loss = 0.15977432\n",
      "Iteration 42, loss = 0.15676995\n",
      "Iteration 43, loss = 0.15709599\n",
      "Iteration 44, loss = 0.15540196\n",
      "Iteration 45, loss = 0.15328822\n",
      "Iteration 46, loss = 0.15178969\n",
      "Iteration 47, loss = 0.15015256\n",
      "Iteration 48, loss = 0.14809456\n",
      "Iteration 49, loss = 0.14723953\n",
      "Iteration 50, loss = 0.14507236\n",
      "Iteration 51, loss = 0.14464555\n",
      "Iteration 52, loss = 0.14107604\n",
      "Iteration 53, loss = 0.13995207\n",
      "Iteration 54, loss = 0.14236843\n",
      "Iteration 55, loss = 0.13840981\n",
      "Iteration 56, loss = 0.13478579\n",
      "Iteration 57, loss = 0.13544007\n",
      "Iteration 58, loss = 0.13439254\n",
      "Iteration 59, loss = 0.13400846\n",
      "Iteration 60, loss = 0.13314537\n",
      "Iteration 61, loss = 0.13193683\n",
      "Iteration 62, loss = 0.12938824\n",
      "Iteration 63, loss = 0.12958022\n",
      "Iteration 64, loss = 0.12811991\n",
      "Iteration 65, loss = 0.12564043\n",
      "Iteration 66, loss = 0.12819035\n",
      "Iteration 67, loss = 0.12595512\n",
      "Iteration 68, loss = 0.12555514\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8301  911]\n",
      " [1426  581]]\n",
      "0.389410187668 0.289486796213 0.332094884253\n",
      "['BernoulliNB', 0.3346966598500341, 0.4892874937717987, 0.3974903865614248, 0.7346465816917729]\n",
      "['RandomForestClassifier', 0.6351931330472103, 0.0737419033383159, 0.13214285714285717, 0.8267225242891524]\n",
      "['BaggingClassifier', 0.5611702127659575, 0.10513203786746388, 0.17708770457406633, 0.8252072377217221]\n",
      "['ExtraTreesClassifier', 0.527363184079602, 0.10563029397110114, 0.17600664176006642, 0.8230680096265264]\n",
      "['DecisionTreeClassifier', 0.2772378516624041, 0.2700548081714001, 0.27359919232710755, 0.743470897584455]\n",
      "['CalibratedClassifierCV', 0.4098360655737705, 0.01245640259093174, 0.024177949709864605, 0.8201265709956325]\n",
      "['SGDClassifier', 0.32323232323232326, 0.03188839063278525, 0.058049886621315196, 0.8148676352616098]\n",
      "['KNeighborsClassifier', 0.3326551373346897, 0.32585949177877427, 0.32922225018877416, 0.7624565469293163]\n",
      "['MLPClassifier', 0.3894101876675603, 0.2894867962132536, 0.3320948842526436, 0.7916926642303236]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_trans = Phrases(trainFSen)\n",
    "\n",
    "modelName = 'csv_3_ReplaceBR_bigram'\n",
    "train_vect, test_vect = buildW2V(modelName,bigram_trans[trainMid], bigram_trans[testMid])\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## BR1 BR2 tags re-tagging\n",
    "There appeared to be many replacement of BR1 or BR2 in a single sentence itself but the other BR tag didn't signify any relation. So using CharOffset BR1 and BR2 was tagged. (2)csv file used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fileName = \"WhiteText(2).csv\"\n",
    "trainSet = pd.read_csv('data/'+fileName,delimiter=\"|\")\n",
    "\n",
    "fileEval = \"WhiteTextUnseenEval(2).csv\"\n",
    "testSet = pd.read_csv('data/'+fileEval,delimiter=\"|\")\n",
    "\n",
    "\n",
    "trainSet.to_csv('Train(2)',sep=\"|\")\n",
    "testSet.to_csv('Test(2)',sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22572\n",
      "11219\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.read_csv('Train(2)',delimiter='|')\n",
    "trainSen = trainData['sentence']\n",
    "trainLab = trainData['connection']\n",
    "trainLen = len(trainSen)\n",
    "print trainLen\n",
    "\n",
    "testData = pd.read_csv('Test(2)',delimiter='|')\n",
    "testSen = testData['sentence']\n",
    "testLab = testData['connection']\n",
    "testLen = len(testSen)\n",
    "print testLen\n",
    "\n",
    "\n",
    "def formatSen(x):\n",
    "    x = re.sub(\"\\((.*?)\\)\",\" \",x.lower())\n",
    "    x = re.sub(\"^[ ]*([a-z])\",r\"\\1\",x)\n",
    "    return re.sub(\"[^a-z0-9]\",\" \",x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFSen = []\n",
    "for i in range(0,trainLen):\n",
    "    trainFSen.append(formatSen(trainSen[i]))\n",
    "    \n",
    "testFSen = []\n",
    "for i in range(0,testLen):\n",
    "    testFSen.append(formatSen(testSen[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  BernoulliNB\n",
      "[[8628  584]\n",
      " [1654  353]]\n",
      "0.376734258271 0.175884404584 0.239809782609\n",
      "Training  RandomForestClassifier\n",
      "[[9104  108]\n",
      " [1710  297]]\n",
      "0.733333333333 0.14798206278 0.246268656716\n",
      "Training  BaggingClassifier\n",
      "[[8468  744]\n",
      " [1262  745]]\n",
      "0.500335795836 0.37120079721 0.426201372998\n",
      "Training  ExtraTreesClassifier\n",
      "[[9057  155]\n",
      " [1760  247]]\n",
      "0.614427860697 0.123069257598 0.205064342051\n",
      "Training  DecisionTreeClassifier\n",
      "[[8068 1144]\n",
      " [1289  718]]\n",
      "0.385606874329 0.357747882412 0.371155337296\n",
      "Training  CalibratedClassifierCV\n",
      "[[8956  256]\n",
      " [1396  611]]\n",
      "0.704728950404 0.304434479322 0.425191370912\n",
      "Training  SGDClassifier\n",
      "[[9103  109]\n",
      " [1648  359]]\n",
      "0.767094017094 0.178873941206 0.290101010101\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.39001203\n",
      "Iteration 2, loss = 0.20489178\n",
      "Iteration 3, loss = 0.14073977\n",
      "Iteration 4, loss = 0.11332832\n",
      "Iteration 5, loss = 0.09523400\n",
      "Iteration 6, loss = 0.08469402\n",
      "Iteration 7, loss = 0.07419547\n",
      "Iteration 8, loss = 0.06793087\n",
      "Iteration 9, loss = 0.06015482\n",
      "Iteration 10, loss = 0.05728793\n",
      "Iteration 11, loss = 0.05274751\n",
      "Iteration 12, loss = 0.04853205\n",
      "Iteration 13, loss = 0.04612775\n",
      "Iteration 14, loss = 0.04658510\n",
      "Iteration 15, loss = 0.04137928\n",
      "Iteration 16, loss = 0.04037010\n",
      "Iteration 17, loss = 0.03802668\n",
      "Iteration 18, loss = 0.03588097\n",
      "Iteration 19, loss = 0.03580105\n",
      "Iteration 20, loss = 0.03406826\n",
      "Iteration 21, loss = 0.03399837\n",
      "Iteration 22, loss = 0.03279140\n",
      "Iteration 23, loss = 0.03315881\n",
      "Iteration 24, loss = 0.02968445\n",
      "Iteration 25, loss = 0.03006435\n",
      "Iteration 26, loss = 0.02906133\n",
      "Iteration 27, loss = 0.02682011\n",
      "Iteration 28, loss = 0.02868919\n",
      "Iteration 29, loss = 0.02709982\n",
      "Iteration 30, loss = 0.02639605\n",
      "Iteration 31, loss = 0.02643644\n",
      "Iteration 32, loss = 0.02414563\n",
      "Iteration 33, loss = 0.02479450\n",
      "Iteration 34, loss = 0.02593536\n",
      "Iteration 35, loss = 0.02730213\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8253  959]\n",
      " [1026  981]]\n",
      "0.505670103093 0.488789237668 0.49708639473\n",
      "['BernoulliNB', 0.3767342582710779, 0.17588440458395616, 0.23980978260869568, 0.8005169801230057]\n",
      "['RandomForestClassifier', 0.7333333333333333, 0.14798206278026907, 0.24626865671641793, 0.8379534717889295]\n",
      "['BaggingClassifier', 0.5003357958361316, 0.37120079720976584, 0.4262013729977117, 0.8211961850432302]\n",
      "['ExtraTreesClassifier', 0.6144278606965174, 0.12306925759840558, 0.20506434205064342, 0.8293074249041804]\n",
      "['DecisionTreeClassifier', 0.38560687432867885, 0.3577478824115595, 0.37115533729645905, 0.783135751849541]\n",
      "['CalibratedClassifierCV', 0.7047289504036909, 0.3044344793223717, 0.42519137091162146, 0.8527497994473661]\n",
      "['SGDClassifier', 0.7670940170940171, 0.17887394120577976, 0.29010101010101014, 0.8433906765308851]\n",
      "['MLPClassifier', 0.5056701030927835, 0.48878923766816146, 0.4970863947301748, 0.8230680096265264]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_vect, test_vect = convertTfidf(trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "15 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[6759 2453]\n",
      " [1064  943]]\n",
      "0.277679623086 0.46985550573 0.349065334074\n",
      "Training  RandomForestClassifier\n",
      "[[9017  195]\n",
      " [1865  142]]\n",
      "0.421364985163 0.0707523667165 0.121160409556\n",
      "Training  BaggingClassifier\n",
      "[[9049  163]\n",
      " [1838  169]]\n",
      "0.509036144578 0.0842052815147 0.14450619923\n",
      "Training  ExtraTreesClassifier\n",
      "[[8648  564]\n",
      " [1749  258]]\n",
      "0.313868613139 0.128550074738 0.182396606575\n",
      "Training  DecisionTreeClassifier\n",
      "[[7670 1542]\n",
      " [1509  498]]\n",
      "0.244117647059 0.248131539611 0.246108228317\n",
      "Training  CalibratedClassifierCV\n",
      "[[9150   62]\n",
      " [1982   25]]\n",
      "0.287356321839 0.0124564025909 0.0238777459408\n",
      "Training  SGDClassifier\n",
      "[[9126   86]\n",
      " [1968   39]]\n",
      "0.312 0.0194319880419 0.0365853658537\n",
      "Training  KNeighborsClassifier\n",
      "[[7710 1502]\n",
      " [1385  622]]\n",
      "0.292843691149 0.309915296462 0.301137739046\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.38515211\n",
      "Iteration 2, loss = 0.34243146\n",
      "Iteration 3, loss = 0.32866795\n",
      "Iteration 4, loss = 0.31958555\n",
      "Iteration 5, loss = 0.31169813\n",
      "Iteration 6, loss = 0.30682029\n",
      "Iteration 7, loss = 0.30163998\n",
      "Iteration 8, loss = 0.29737912\n",
      "Iteration 9, loss = 0.29295084\n",
      "Iteration 10, loss = 0.28978605\n",
      "Iteration 11, loss = 0.28627080\n",
      "Iteration 12, loss = 0.28359838\n",
      "Iteration 13, loss = 0.28114728\n",
      "Iteration 14, loss = 0.27863491\n",
      "Iteration 15, loss = 0.27654910\n",
      "Iteration 16, loss = 0.27378484\n",
      "Iteration 17, loss = 0.27228375\n",
      "Iteration 18, loss = 0.26973910\n",
      "Iteration 19, loss = 0.26794430\n",
      "Iteration 20, loss = 0.26689815\n",
      "Iteration 21, loss = 0.26533840\n",
      "Iteration 22, loss = 0.26359653\n",
      "Iteration 23, loss = 0.26249081\n",
      "Iteration 24, loss = 0.26136346\n",
      "Iteration 25, loss = 0.25895419\n",
      "Iteration 26, loss = 0.25843435\n",
      "Iteration 27, loss = 0.25749572\n",
      "Iteration 28, loss = 0.25654047\n",
      "Iteration 29, loss = 0.25557924\n",
      "Iteration 30, loss = 0.25421353\n",
      "Iteration 31, loss = 0.25348834\n",
      "Iteration 32, loss = 0.25285544\n",
      "Iteration 33, loss = 0.25243549\n",
      "Iteration 34, loss = 0.25086969\n",
      "Iteration 35, loss = 0.25051309\n",
      "Iteration 36, loss = 0.25051479\n",
      "Iteration 37, loss = 0.24873585\n",
      "Iteration 38, loss = 0.24848093\n",
      "Iteration 39, loss = 0.24849227\n",
      "Iteration 40, loss = 0.24826868\n",
      "Iteration 41, loss = 0.24653278\n",
      "Iteration 42, loss = 0.24559447\n",
      "Iteration 43, loss = 0.24584581\n",
      "Iteration 44, loss = 0.24534040\n",
      "Iteration 45, loss = 0.24625573\n",
      "Iteration 46, loss = 0.24441799\n",
      "Iteration 47, loss = 0.24553057\n",
      "Iteration 48, loss = 0.24390803\n",
      "Iteration 49, loss = 0.24455934\n",
      "Iteration 50, loss = 0.24428587\n",
      "Iteration 51, loss = 0.24265536\n",
      "Iteration 52, loss = 0.24272405\n",
      "Iteration 53, loss = 0.24288871\n",
      "Iteration 54, loss = 0.24216717\n",
      "Iteration 55, loss = 0.24283856\n",
      "Iteration 56, loss = 0.24145212\n",
      "Iteration 57, loss = 0.24128031\n",
      "Iteration 58, loss = 0.24217973\n",
      "Iteration 59, loss = 0.24081370\n",
      "Iteration 60, loss = 0.24049607\n",
      "Iteration 61, loss = 0.23981119\n",
      "Iteration 62, loss = 0.24039972\n",
      "Iteration 63, loss = 0.24030744\n",
      "Iteration 64, loss = 0.23951451\n",
      "Iteration 65, loss = 0.23962745\n",
      "Iteration 66, loss = 0.23990344\n",
      "Iteration 67, loss = 0.23941851\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8505  707]\n",
      " [1549  458]]\n",
      "0.39313304721 0.228201295466 0.288776796974\n",
      "['BernoulliNB', 0.2776796230859835, 0.46985550572994517, 0.34906533407366275, 0.6865139495498708]\n",
      "['RandomForestClassifier', 0.42136498516320475, 0.07075236671649228, 0.12116040955631399, 0.81638292182904]\n",
      "['BaggingClassifier', 0.5090361445783133, 0.08420528151469855, 0.14450619923044036, 0.8216418575630626]\n",
      "['ExtraTreesClassifier', 0.31386861313868614, 0.12855007473841554, 0.1823966065747614, 0.7938318923255192]\n",
      "['DecisionTreeClassifier', 0.24411764705882352, 0.24813153961136025, 0.24610822831727205, 0.728050628398253]\n",
      "['CalibratedClassifierCV', 0.28735632183908044, 0.01245640259093174, 0.023877745940783193, 0.8178090738925038]\n",
      "['SGDClassifier', 0.312, 0.01943198804185351, 0.03658536585365853, 0.816917728852839]\n",
      "['KNeighborsClassifier', 0.2928436911487759, 0.30991529646238164, 0.30113773904623575, 0.7426686870487565]\n",
      "['MLPClassifier', 0.3931330472103004, 0.22820129546586945, 0.28877679697351827, 0.7989125590516089]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "15 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[6405 2807]\n",
      " [1062  945]]\n",
      "0.251865671642 0.470852017937 0.328181976038\n",
      "Training  RandomForestClassifier\n",
      "[[9050  162]\n",
      " [1869  138]]\n",
      "0.46 0.0687593423019 0.119635890767\n",
      "Training  BaggingClassifier\n",
      "[[9033  179]\n",
      " [1847  160]]\n",
      "0.47197640118 0.079720976582 0.136402387042\n",
      "Training  ExtraTreesClassifier\n",
      "[[8892  320]\n",
      " [1845  162]]\n",
      "0.336099585062 0.0807174887892 0.130172760145\n",
      "Training  DecisionTreeClassifier\n",
      "[[7536 1676]\n",
      " [1468  539]]\n",
      "0.243340857788 0.26856003986 0.255329227854\n",
      "Training  CalibratedClassifierCV\n",
      "[[9204    8]\n",
      " [1989   18]]\n",
      "0.692307692308 0.00896860986547 0.0177078209543\n",
      "Training  SGDClassifier\n",
      "[[8898  314]\n",
      " [1839  168]]\n",
      "0.348547717842 0.0837070254111 0.134993973483\n",
      "Training  KNeighborsClassifier\n",
      "[[7531 1681]\n",
      " [1360  647]]\n",
      "0.277920962199 0.322371699053 0.298500576701\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.37988354\n",
      "Iteration 2, loss = 0.34390767\n",
      "Iteration 3, loss = 0.33076545\n",
      "Iteration 4, loss = 0.32276835\n",
      "Iteration 5, loss = 0.31540265\n",
      "Iteration 6, loss = 0.30915295\n",
      "Iteration 7, loss = 0.30475329\n",
      "Iteration 8, loss = 0.29927966\n",
      "Iteration 9, loss = 0.29688636\n",
      "Iteration 10, loss = 0.29324336\n",
      "Iteration 11, loss = 0.29087488\n",
      "Iteration 12, loss = 0.28624148\n",
      "Iteration 13, loss = 0.28422283\n",
      "Iteration 14, loss = 0.28213138\n",
      "Iteration 15, loss = 0.27961273\n",
      "Iteration 16, loss = 0.27821995\n",
      "Iteration 17, loss = 0.27728473\n",
      "Iteration 18, loss = 0.27276235\n",
      "Iteration 19, loss = 0.27095634\n",
      "Iteration 20, loss = 0.26907584\n",
      "Iteration 21, loss = 0.26774018\n",
      "Iteration 22, loss = 0.26723627\n",
      "Iteration 23, loss = 0.26663889\n",
      "Iteration 24, loss = 0.26442450\n",
      "Iteration 25, loss = 0.26435584\n",
      "Iteration 26, loss = 0.26077137\n",
      "Iteration 27, loss = 0.26112174\n",
      "Iteration 28, loss = 0.25941653\n",
      "Iteration 29, loss = 0.25949809\n",
      "Iteration 30, loss = 0.25706656\n",
      "Iteration 31, loss = 0.25702313\n",
      "Iteration 32, loss = 0.25475829\n",
      "Iteration 33, loss = 0.25586443\n",
      "Iteration 34, loss = 0.25570474\n",
      "Iteration 35, loss = 0.25408337\n",
      "Iteration 36, loss = 0.25295000\n",
      "Iteration 37, loss = 0.25182050\n",
      "Iteration 38, loss = 0.25226315\n",
      "Iteration 39, loss = 0.25296194\n",
      "Iteration 40, loss = 0.25051026\n",
      "Iteration 41, loss = 0.25117496\n",
      "Iteration 42, loss = 0.25019346\n",
      "Iteration 43, loss = 0.24919423\n",
      "Iteration 44, loss = 0.24810709\n",
      "Iteration 45, loss = 0.24939810\n",
      "Iteration 46, loss = 0.24740378\n",
      "Iteration 47, loss = 0.24731186\n",
      "Iteration 48, loss = 0.24780778\n",
      "Iteration 49, loss = 0.24625449\n",
      "Iteration 50, loss = 0.24736561\n",
      "Iteration 51, loss = 0.24651032\n",
      "Iteration 52, loss = 0.24554784\n",
      "Iteration 53, loss = 0.24536304\n",
      "Iteration 54, loss = 0.24548106\n",
      "Iteration 55, loss = 0.24406643\n",
      "Iteration 56, loss = 0.24469963\n",
      "Iteration 57, loss = 0.24474820\n",
      "Iteration 58, loss = 0.24384879\n",
      "Iteration 59, loss = 0.24277482\n",
      "Iteration 60, loss = 0.24397575\n",
      "Iteration 61, loss = 0.24363395\n",
      "Iteration 62, loss = 0.24225737\n",
      "Iteration 63, loss = 0.24242077\n",
      "Iteration 64, loss = 0.24260770\n",
      "Iteration 65, loss = 0.24341117\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8744  468]\n",
      " [1632  375]]\n",
      "0.444839857651 0.186846038864 0.263157894737\n",
      "['BernoulliNB', 0.251865671641791, 0.47085201793721976, 0.3281819760375065, 0.6551386041536679]\n",
      "['RandomForestClassifier', 0.46, 0.0687593423019432, 0.11963589076723015, 0.8189678224440681]\n",
      "['BaggingClassifier', 0.471976401179941, 0.07972097658196313, 0.13640238704177324, 0.8194134949639005]\n",
      "['ExtraTreesClassifier', 0.3360995850622407, 0.08071748878923767, 0.13017276014463638, 0.807023798912559]\n",
      "['DecisionTreeClassifier', 0.2433408577878104, 0.26856003986048826, 0.25532922785409756, 0.7197611195293698]\n",
      "['CalibratedClassifierCV', 0.6923076923076923, 0.008968609865470852, 0.017707820954254796, 0.8219983955789286]\n",
      "['SGDClassifier', 0.34854771784232363, 0.08370702541106129, 0.13499397348332662, 0.8080934129601569]\n",
      "['KNeighborsClassifier', 0.2779209621993127, 0.3223716990533134, 0.2985005767012687, 0.7289419734379178]\n",
      "['MLPClassifier', 0.44483985765124556, 0.18684603886397608, 0.2631578947368421, 0.8128175416703806]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "15 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7213 1999]\n",
      " [1229  778]]\n",
      "0.280158444364 0.38764324863 0.32525083612\n",
      "Training  RandomForestClassifier\n",
      "[[9143   69]\n",
      " [1925   82]]\n",
      "0.543046357616 0.0408570004983 0.0759962928638\n",
      "Training  BaggingClassifier\n",
      "[[9086  126]\n",
      " [1894  113]]\n",
      "0.47280334728 0.056302939711 0.100623330365\n",
      "Training  ExtraTreesClassifier\n",
      "[[9007  205]\n",
      " [1872  135]]\n",
      "0.397058823529 0.067264573991 0.115040477205\n",
      "Training  DecisionTreeClassifier\n",
      "[[7556 1656]\n",
      " [1556  451]]\n",
      "0.214048410062 0.22471350274 0.219251336898\n",
      "Training  CalibratedClassifierCV\n",
      "[[9200   12]\n",
      " [1980   27]]\n",
      "0.692307692308 0.0134529147982 0.0263929618768\n",
      "Training  SGDClassifier\n",
      "[[6627 2585]\n",
      " [1041  966]]\n",
      "0.272036046184 0.481315396114 0.347607052897\n",
      "Training  KNeighborsClassifier\n",
      "[[7448 1764]\n",
      " [1450  557]]\n",
      "0.239982766049 0.277528649726 0.257393715342\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.37111459\n",
      "Iteration 2, loss = 0.33596986\n",
      "Iteration 3, loss = 0.32389924\n",
      "Iteration 4, loss = 0.31588880\n",
      "Iteration 5, loss = 0.30911830\n",
      "Iteration 6, loss = 0.30381404\n",
      "Iteration 7, loss = 0.29910975\n",
      "Iteration 8, loss = 0.29382451\n",
      "Iteration 9, loss = 0.29299400\n",
      "Iteration 10, loss = 0.28687670\n",
      "Iteration 11, loss = 0.28368594\n",
      "Iteration 12, loss = 0.28136450\n",
      "Iteration 13, loss = 0.27836266\n",
      "Iteration 14, loss = 0.27618440\n",
      "Iteration 15, loss = 0.27285902\n",
      "Iteration 16, loss = 0.27013137\n",
      "Iteration 17, loss = 0.26932559\n",
      "Iteration 18, loss = 0.26754184\n",
      "Iteration 19, loss = 0.26513418\n",
      "Iteration 20, loss = 0.26443069\n",
      "Iteration 21, loss = 0.26255574\n",
      "Iteration 22, loss = 0.26110457\n",
      "Iteration 23, loss = 0.25952744\n",
      "Iteration 24, loss = 0.25824024\n",
      "Iteration 25, loss = 0.25859263\n",
      "Iteration 26, loss = 0.25784174\n",
      "Iteration 27, loss = 0.25485482\n",
      "Iteration 28, loss = 0.25397005\n",
      "Iteration 29, loss = 0.25562390\n",
      "Iteration 30, loss = 0.25293314\n",
      "Iteration 31, loss = 0.25242095\n",
      "Iteration 32, loss = 0.25242128\n",
      "Iteration 33, loss = 0.25201685\n",
      "Iteration 34, loss = 0.24971317\n",
      "Iteration 35, loss = 0.24835548\n",
      "Iteration 36, loss = 0.24765368\n",
      "Iteration 37, loss = 0.24736149\n",
      "Iteration 38, loss = 0.24894499\n",
      "Iteration 39, loss = 0.24633391\n",
      "Iteration 40, loss = 0.24597821\n",
      "Iteration 41, loss = 0.24587514\n",
      "Iteration 42, loss = 0.24521804\n",
      "Iteration 43, loss = 0.24464856\n",
      "Iteration 44, loss = 0.24420709\n",
      "Iteration 45, loss = 0.24388109\n",
      "Iteration 46, loss = 0.24406819\n",
      "Iteration 47, loss = 0.24410481\n",
      "Iteration 48, loss = 0.24512912\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8743  469]\n",
      " [1675  332]]\n",
      "0.414481897628 0.165421026408 0.236467236467\n",
      "['BernoulliNB', 0.28015844436442205, 0.3876432486297957, 0.32525083612040134, 0.7122738211961851]\n",
      "['RandomForestClassifier', 0.543046357615894, 0.0408570004982561, 0.07599629286376273, 0.8222657990908281]\n",
      "['BaggingClassifier', 0.47280334728033474, 0.05630293971101146, 0.10062333036509351, 0.8199483019876994]\n",
      "['ExtraTreesClassifier', 0.39705882352941174, 0.06726457399103139, 0.11504047720494247, 0.8148676352616098]\n",
      "['DecisionTreeClassifier', 0.21404841006169908, 0.22471350274040858, 0.21925133689839574, 0.7136999732596488]\n",
      "['CalibratedClassifierCV', 0.6923076923076923, 0.013452914798206279, 0.026392961876832842, 0.822444068098761]\n",
      "['SGDClassifier', 0.2720360461841735, 0.4813153961136024, 0.3476070528967255, 0.6767982886175239]\n",
      "['KNeighborsClassifier', 0.23998276604911675, 0.27752864972595914, 0.2573937153419594, 0.7135217042517158]\n",
      "['MLPClassifier', 0.41448189762796506, 0.1654210264075735, 0.23646723646723647, 0.8088956234958552]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'csv_2_ReplaceBR12'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "15 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7774 1438]\n",
      " [1317  690]]\n",
      "0.324248120301 0.34379671151 0.333736396614\n",
      "Training  RandomForestClassifier\n",
      "[[9160   52]\n",
      " [1924   83]]\n",
      "0.614814814815 0.0413552566019 0.077497665733\n",
      "Training  BaggingClassifier\n",
      "[[9072  140]\n",
      " [1871  136]]\n",
      "0.492753623188 0.0677628300947 0.119141480508\n",
      "Training  ExtraTreesClassifier\n",
      "[[8952  260]\n",
      " [1871  136]]\n",
      "0.343434343434 0.0677628300947 0.113191843529\n",
      "Training  DecisionTreeClassifier\n",
      "[[7873 1339]\n",
      " [1496  511]]\n",
      "0.276216216216 0.254608868959 0.26497277677\n",
      "Training  CalibratedClassifierCV\n",
      "[[9201   11]\n",
      " [1981   26]]\n",
      "0.702702702703 0.0129546586946 0.0254403131115\n",
      "Training  SGDClassifier\n",
      "[[9099  113]\n",
      " [1905  102]]\n",
      "0.474418604651 0.050822122571 0.0918091809181\n",
      "Training  KNeighborsClassifier\n",
      "[[7676 1536]\n",
      " [1454  553]]\n",
      "0.264719961704 0.275535625311 0.27001953125\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.37543477\n",
      "Iteration 2, loss = 0.34189493\n",
      "Iteration 3, loss = 0.32883007\n",
      "Iteration 4, loss = 0.32182602\n",
      "Iteration 5, loss = 0.31309096\n",
      "Iteration 6, loss = 0.30770600\n",
      "Iteration 7, loss = 0.30360245\n",
      "Iteration 8, loss = 0.30054458\n",
      "Iteration 9, loss = 0.29518753\n",
      "Iteration 10, loss = 0.29359498\n",
      "Iteration 11, loss = 0.28828198\n",
      "Iteration 12, loss = 0.28646040\n",
      "Iteration 13, loss = 0.28500102\n",
      "Iteration 14, loss = 0.28088327\n",
      "Iteration 15, loss = 0.27879041\n",
      "Iteration 16, loss = 0.27949474\n",
      "Iteration 17, loss = 0.27509914\n",
      "Iteration 18, loss = 0.27307495\n",
      "Iteration 19, loss = 0.27208957\n",
      "Iteration 20, loss = 0.27052371\n",
      "Iteration 21, loss = 0.26764647\n",
      "Iteration 22, loss = 0.26707266\n",
      "Iteration 23, loss = 0.26829155\n",
      "Iteration 24, loss = 0.26369537\n",
      "Iteration 25, loss = 0.26297982\n",
      "Iteration 26, loss = 0.26275823\n",
      "Iteration 27, loss = 0.26078875\n",
      "Iteration 28, loss = 0.26046478\n",
      "Iteration 29, loss = 0.25869673\n",
      "Iteration 30, loss = 0.26021072\n",
      "Iteration 31, loss = 0.25777583\n",
      "Iteration 32, loss = 0.25549568\n",
      "Iteration 33, loss = 0.25513514\n",
      "Iteration 34, loss = 0.25285022\n",
      "Iteration 35, loss = 0.25351389\n",
      "Iteration 36, loss = 0.25151828\n",
      "Iteration 37, loss = 0.25188219\n",
      "Iteration 38, loss = 0.24991484\n",
      "Iteration 39, loss = 0.25127690\n",
      "Iteration 40, loss = 0.24918643\n",
      "Iteration 41, loss = 0.24892758\n",
      "Iteration 42, loss = 0.24775199\n",
      "Iteration 43, loss = 0.24705515\n",
      "Iteration 44, loss = 0.24738248\n",
      "Iteration 45, loss = 0.24653406\n",
      "Iteration 46, loss = 0.24511017\n",
      "Iteration 47, loss = 0.24564547\n",
      "Iteration 48, loss = 0.24615563\n",
      "Iteration 49, loss = 0.24559589\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8800  412]\n",
      " [1683  324]]\n",
      "0.440217391304 0.161434977578 0.236237695953\n",
      "['BernoulliNB', 0.3242481203007519, 0.34379671150971597, 0.3337363966142684, 0.7544344415723326]\n",
      "['RandomForestClassifier', 0.6148148148148148, 0.041355256601893375, 0.07749766573295985, 0.8238702201622248]\n",
      "['BaggingClassifier', 0.4927536231884058, 0.06776283009466866, 0.11914148050810339, 0.8207505125233978]\n",
      "['ExtraTreesClassifier', 0.3434343434343434, 0.06776283009466866, 0.11319184352892218, 0.8100543720474196]\n",
      "['DecisionTreeClassifier', 0.2762162162162162, 0.25460886895864476, 0.2649727767695099, 0.7473036812550138]\n",
      "['CalibratedClassifierCV', 0.7027027027027027, 0.012954658694569009, 0.02544031311154599, 0.822444068098761]\n",
      "['SGDClassifier', 0.4744186046511628, 0.05082212257100149, 0.09180918091809182, 0.8201265709956325]\n",
      "['KNeighborsClassifier', 0.2647199617041647, 0.27553562531141007, 0.27001953125, 0.7334878331402086]\n",
      "['MLPClassifier', 0.44021739130434784, 0.16143497757847533, 0.23623769595333577, 0.8132632141902131]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_trans = Phrases(trainFSen)\n",
    "\n",
    "modelName = 'csv_2_ReplaceBR12_bigram'\n",
    "train_vect, test_vect = buildW2V(modelName,bigram_trans[trainFSen], bigram_trans[testFSen])\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle Sentences taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def breakSen(sentences):\n",
    "    middleSen = []\n",
    "    for sentence in sentences:\n",
    "        s = word_tokenize(sentence)\n",
    "\n",
    "        ind1 = -1\n",
    "        ind2 = -1\n",
    "        for j in range(0,len(s)):\n",
    "            if s[j].__contains__('br1'):\n",
    "                if ind1 == -1:\n",
    "                    ind1 = j\n",
    "                else:\n",
    "                    ind2 = j\n",
    "        for j in range(0,len(s)):\n",
    "            if s[j].__contains__('br2'):\n",
    "                if j > ind2:\n",
    "                    ind2 = j\n",
    "\n",
    "        if ind1-3 < 0:\n",
    "            ind1 = 0\n",
    "        else: ind1 -= 3\n",
    "\n",
    "        if ind2+3 > len(s):\n",
    "            ind2 = len(s)\n",
    "        else: ind2 += 3\n",
    "\n",
    "        middleSen.append(' '.join(s[ind1:ind2]))\n",
    "    return middleSen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for this', 'for this', 'for this', 'for this', 'for this']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMid = breakSen(trainFSen)\n",
    "testMid = breakSen(testFSen)\n",
    "trainMid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22572"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainFSen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  BernoulliNB\n",
      "[[8739  473]\n",
      " [1719  288]]\n",
      "0.378449408673 0.143497757848 0.208092485549\n",
      "Training  RandomForestClassifier\n",
      "[[9062  150]\n",
      " [1576  431]]\n",
      "0.74182444062 0.214748380668 0.333075734158\n",
      "Training  BaggingClassifier\n",
      "[[8760  452]\n",
      " [1269  738]]\n",
      "0.620168067227 0.367713004484 0.461682827651\n",
      "Training  ExtraTreesClassifier\n",
      "[[9029  183]\n",
      " [1600  407]]\n",
      "0.689830508475 0.20279023418 0.31343858298\n",
      "Training  DecisionTreeClassifier\n",
      "[[8280  932]\n",
      " [1218  789]]\n",
      "0.458454386984 0.39312406577 0.423283261803\n",
      "Training  CalibratedClassifierCV\n",
      "[[8850  362]\n",
      " [1363  644]]\n",
      "0.640159045726 0.320876930742 0.427480916031\n",
      "Training  SGDClassifier\n",
      "[[8961  251]\n",
      " [1479  528]]\n",
      "0.677792041078 0.26307922272 0.37903804738\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.39875737\n",
      "Iteration 2, loss = 0.15347717\n",
      "Iteration 3, loss = 0.07824594\n",
      "Iteration 4, loss = 0.05566423\n",
      "Iteration 5, loss = 0.04385750\n",
      "Iteration 6, loss = 0.03726237\n",
      "Iteration 7, loss = 0.03400020\n",
      "Iteration 8, loss = 0.03073874\n",
      "Iteration 9, loss = 0.02844327\n",
      "Iteration 10, loss = 0.02763475\n",
      "Iteration 11, loss = 0.02569557\n",
      "Iteration 12, loss = 0.02465384\n",
      "Iteration 13, loss = 0.02405085\n",
      "Iteration 14, loss = 0.02320330\n",
      "Iteration 15, loss = 0.02347710\n",
      "Iteration 16, loss = 0.02305785\n",
      "Iteration 17, loss = 0.02290930\n",
      "Iteration 18, loss = 0.02207602\n",
      "Iteration 19, loss = 0.02181083\n",
      "Iteration 20, loss = 0.02141484\n",
      "Iteration 21, loss = 0.02117392\n",
      "Iteration 22, loss = 0.02123073\n",
      "Iteration 23, loss = 0.02079479\n",
      "Iteration 24, loss = 0.02096222\n",
      "Iteration 25, loss = 0.02062127\n",
      "Iteration 26, loss = 0.02063337\n",
      "Iteration 27, loss = 0.02046199\n",
      "Iteration 28, loss = 0.02040915\n",
      "Iteration 29, loss = 0.02016635\n",
      "Iteration 30, loss = 0.02034249\n",
      "Iteration 31, loss = 0.01982352\n",
      "Iteration 32, loss = 0.01975262\n",
      "Iteration 33, loss = 0.01968886\n",
      "Iteration 34, loss = 0.02030343\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8439  773]\n",
      " [1187  820]]\n",
      "0.514752040176 0.408570004983 0.455555555556\n",
      "['BernoulliNB', 0.37844940867279897, 0.14349775784753363, 0.20809248554913296, 0.804617167305464]\n",
      "['RandomForestClassifier', 0.7418244406196214, 0.2147483806676632, 0.3330757341576507, 0.8461538461538461]\n",
      "['BaggingClassifier', 0.6201680672268908, 0.36771300448430494, 0.4616828276509228, 0.8465995186736786]\n",
      "['ExtraTreesClassifier', 0.6898305084745763, 0.2027902341803687, 0.313438582980362, 0.8410731794277565]\n",
      "['DecisionTreeClassifier', 0.45845438698431146, 0.3931240657698057, 0.4232832618025751, 0.8083608164720564]\n",
      "['CalibratedClassifierCV', 0.6401590457256461, 0.3208769307424016, 0.42748091603053434, 0.8462429806578127]\n",
      "['SGDClassifier', 0.6777920410783055, 0.26307922272047835, 0.37903804737975594, 0.8457973081379803]\n",
      "['MLPClassifier', 0.514752040175769, 0.408570004982561, 0.45555555555555555, 0.8252963722256885]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_vect, test_vect = convertTfidf(trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "1193 22572\n",
      "Embedding... 11219\n",
      "489 11219\n",
      "Training  BernoulliNB\n",
      "[[7109 2103]\n",
      " [ 914 1093]]\n",
      "0.341989987484 0.544593921276 0.420142225639\n",
      "Training  RandomForestClassifier\n",
      "[[9110  102]\n",
      " [1827  180]]\n",
      "0.63829787234 0.0896860986547 0.157273918742\n",
      "Training  BaggingClassifier\n",
      "[[9003  209]\n",
      " [1776  231]]\n",
      "0.525 0.11509715994 0.188802615447\n",
      "Training  ExtraTreesClassifier\n",
      "[[9014  198]\n",
      " [1779  228]]\n",
      "0.535211267606 0.113602391629 0.187422934649\n",
      "Training  DecisionTreeClassifier\n",
      "[[7925 1287]\n",
      " [1376  631]]\n",
      "0.328988529718 0.314399601395 0.32152866242\n",
      "Training  CalibratedClassifierCV\n",
      "[[9126   86]\n",
      " [1947   60]]\n",
      "0.41095890411 0.0298953662182 0.0557361820715\n",
      "Training  SGDClassifier\n",
      "[[8371  841]\n",
      " [1575  432]]\n",
      "0.339355852317 0.215246636771 0.263414634146\n",
      "Training  KNeighborsClassifier\n",
      "[[8089 1123]\n",
      " [1363  644]]\n",
      "0.364459535937 0.320876930742 0.34128245893\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35862211\n",
      "Iteration 2, loss = 0.30612505\n",
      "Iteration 3, loss = 0.28904674\n",
      "Iteration 4, loss = 0.27643966\n",
      "Iteration 5, loss = 0.26501946\n",
      "Iteration 6, loss = 0.25560764\n",
      "Iteration 7, loss = 0.24548935\n",
      "Iteration 8, loss = 0.23962620\n",
      "Iteration 9, loss = 0.23143084\n",
      "Iteration 10, loss = 0.22358343\n",
      "Iteration 11, loss = 0.21413472\n",
      "Iteration 12, loss = 0.20899203\n",
      "Iteration 13, loss = 0.20191777\n",
      "Iteration 14, loss = 0.19605983\n",
      "Iteration 15, loss = 0.19126291\n",
      "Iteration 16, loss = 0.18714233\n",
      "Iteration 17, loss = 0.18061784\n",
      "Iteration 18, loss = 0.17484051\n",
      "Iteration 19, loss = 0.17098793\n",
      "Iteration 20, loss = 0.16658007\n",
      "Iteration 21, loss = 0.16052138\n",
      "Iteration 22, loss = 0.15757028\n",
      "Iteration 23, loss = 0.15634132\n",
      "Iteration 24, loss = 0.15010163\n",
      "Iteration 25, loss = 0.14602473\n",
      "Iteration 26, loss = 0.13928109\n",
      "Iteration 27, loss = 0.13841756\n",
      "Iteration 28, loss = 0.13691959\n",
      "Iteration 29, loss = 0.13178409\n",
      "Iteration 30, loss = 0.12908777\n",
      "Iteration 31, loss = 0.12796127\n",
      "Iteration 32, loss = 0.12294476\n",
      "Iteration 33, loss = 0.12229792\n",
      "Iteration 34, loss = 0.12038065\n",
      "Iteration 35, loss = 0.11728250\n",
      "Iteration 36, loss = 0.11330589\n",
      "Iteration 37, loss = 0.11470553\n",
      "Iteration 38, loss = 0.11001013\n",
      "Iteration 39, loss = 0.11151024\n",
      "Iteration 40, loss = 0.10474770\n",
      "Iteration 41, loss = 0.10478102\n",
      "Iteration 42, loss = 0.10535827\n",
      "Iteration 43, loss = 0.10225734\n",
      "Iteration 44, loss = 0.10114916\n",
      "Iteration 45, loss = 0.09954532\n",
      "Iteration 46, loss = 0.09702335\n",
      "Iteration 47, loss = 0.09336375\n",
      "Iteration 48, loss = 0.09459230\n",
      "Iteration 49, loss = 0.09497756\n",
      "Iteration 50, loss = 0.09481337\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8397  815]\n",
      " [1357  650]]\n",
      "0.443686006826 0.323866467364 0.374423963134\n",
      "['BernoulliNB', 0.34198998748435544, 0.5445939212755356, 0.4201422256390544, 0.7310812015331135]\n",
      "['RandomForestClassifier', 0.6382978723404256, 0.08968609865470852, 0.15727391874180863, 0.8280595418486496]\n",
      "['BaggingClassifier', 0.525, 0.11509715994020926, 0.1888026154474867, 0.8230680096265264]\n",
      "['ExtraTreesClassifier', 0.5352112676056338, 0.11360239162929746, 0.187422934648582, 0.8237810856582584]\n",
      "['DecisionTreeClassifier', 0.3289885297184567, 0.3143996013951171, 0.3215286624203821, 0.7626348159372494]\n",
      "['CalibratedClassifierCV', 0.410958904109589, 0.029895366218236172, 0.05573618207152809, 0.8187895534361351]\n",
      "['SGDClassifier', 0.33935585231736054, 0.21524663677130046, 0.2634146341463415, 0.7846510384169713]\n",
      "['KNeighborsClassifier', 0.36445953593661573, 0.3208769307424016, 0.34128245892951775, 0.7784116231393172]\n",
      "['MLPClassifier', 0.44368600682593856, 0.3238664673642252, 0.3744239631336405, 0.8063998573847937]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "1214 22572\n",
      "Embedding... 11219\n",
      "506 11219\n",
      "Training  BernoulliNB\n",
      "[[7329 1883]\n",
      " [ 979 1028]]\n",
      "0.353143249742 0.512207274539 0.418056120374\n",
      "Training  RandomForestClassifier\n",
      "[[9104  108]\n",
      " [1795  212]]\n",
      "0.6625 0.105630293971 0.1822088526\n",
      "Training  BaggingClassifier\n",
      "[[8973  239]\n",
      " [1744  263]]\n",
      "0.52390438247 0.131041355257 0.209645277003\n",
      "Training  ExtraTreesClassifier\n",
      "[[8994  218]\n",
      " [1759  248]]\n",
      "0.532188841202 0.123567513702 0.200566114032\n",
      "Training  DecisionTreeClassifier\n",
      "[[7988 1224]\n",
      " [1397  610]]\n",
      "0.332606324973 0.303936223219 0.317625618329\n",
      "Training  CalibratedClassifierCV\n",
      "[[9137   75]\n",
      " [1945   62]]\n",
      "0.452554744526 0.0308918784255 0.0578358208955\n",
      "Training  SGDClassifier\n",
      "[[9144   68]\n",
      " [1998    9]]\n",
      "0.116883116883 0.00448430493274 0.00863723608445\n",
      "Training  KNeighborsClassifier\n",
      "[[8011 1201]\n",
      " [1303  704]]\n",
      "0.369553805774 0.350772296961 0.359918200409\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.37564167\n",
      "Iteration 2, loss = 0.31416076\n",
      "Iteration 3, loss = 0.29497308\n",
      "Iteration 4, loss = 0.28152934\n",
      "Iteration 5, loss = 0.26993142\n",
      "Iteration 6, loss = 0.25846644\n",
      "Iteration 7, loss = 0.24949452\n",
      "Iteration 8, loss = 0.24006595\n",
      "Iteration 9, loss = 0.23320368\n",
      "Iteration 10, loss = 0.22630282\n",
      "Iteration 11, loss = 0.21998913\n",
      "Iteration 12, loss = 0.21038088\n",
      "Iteration 13, loss = 0.20420948\n",
      "Iteration 14, loss = 0.20028499\n",
      "Iteration 15, loss = 0.19256309\n",
      "Iteration 16, loss = 0.18760889\n",
      "Iteration 17, loss = 0.18355358\n",
      "Iteration 18, loss = 0.17906873\n",
      "Iteration 19, loss = 0.17356275\n",
      "Iteration 20, loss = 0.16906666\n",
      "Iteration 21, loss = 0.16428881\n",
      "Iteration 22, loss = 0.16048540\n",
      "Iteration 23, loss = 0.15772804\n",
      "Iteration 24, loss = 0.15572628\n",
      "Iteration 25, loss = 0.14999736\n",
      "Iteration 26, loss = 0.14575592\n",
      "Iteration 27, loss = 0.14423337\n",
      "Iteration 28, loss = 0.14069861\n",
      "Iteration 29, loss = 0.13559419\n",
      "Iteration 30, loss = 0.13462804\n",
      "Iteration 31, loss = 0.13217566\n",
      "Iteration 32, loss = 0.12677300\n",
      "Iteration 33, loss = 0.12566942\n",
      "Iteration 34, loss = 0.12395628\n",
      "Iteration 35, loss = 0.12082169\n",
      "Iteration 36, loss = 0.12251584\n",
      "Iteration 37, loss = 0.11708369\n",
      "Iteration 38, loss = 0.11765774\n",
      "Iteration 39, loss = 0.11516002\n",
      "Iteration 40, loss = 0.11165066\n",
      "Iteration 41, loss = 0.11070527\n",
      "Iteration 42, loss = 0.10923027\n",
      "Iteration 43, loss = 0.10742596\n",
      "Iteration 44, loss = 0.10354320\n",
      "Iteration 45, loss = 0.10289227\n",
      "Iteration 46, loss = 0.10523443\n",
      "Iteration 47, loss = 0.10014209\n",
      "Iteration 48, loss = 0.10012840\n",
      "Iteration 49, loss = 0.09828150\n",
      "Iteration 50, loss = 0.09645169\n",
      "Iteration 51, loss = 0.09571036\n",
      "Iteration 52, loss = 0.09726355\n",
      "Iteration 53, loss = 0.09320746\n",
      "Iteration 54, loss = 0.09164336\n",
      "Iteration 55, loss = 0.09145246\n",
      "Iteration 56, loss = 0.09222783\n",
      "Iteration 57, loss = 0.09287443\n",
      "Iteration 58, loss = 0.09371974\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8224  988]\n",
      " [1352  655]]\n",
      "0.398660986001 0.326357747882 0.358904109589\n",
      "['BernoulliNB', 0.35314324974235656, 0.5122072745391131, 0.4180561203741358, 0.7448970496479187]\n",
      "['RandomForestClassifier', 0.6625, 0.10563029397110114, 0.18220885259991407, 0.8303770389517783]\n",
      "['BaggingClassifier', 0.5239043824701195, 0.1310413552566019, 0.20964527700278995, 0.8232462786344594]\n",
      "['ExtraTreesClassifier', 0.5321888412017167, 0.12356751370204284, 0.20056611403154062, 0.8237810856582584]\n",
      "['DecisionTreeClassifier', 0.3326063249727372, 0.3039362232187344, 0.3176256183285603, 0.7663784651038417]\n",
      "['CalibratedClassifierCV', 0.45255474452554745, 0.03089187842551071, 0.05783582089552239, 0.8199483019876994]\n",
      "['SGDClassifier', 0.11688311688311688, 0.004484304932735426, 0.008637236084452975, 0.8158481148052411]\n",
      "['KNeighborsClassifier', 0.3695538057742782, 0.3507722969606378, 0.35991820040899797, 0.7768072020679205]\n",
      "['MLPClassifier', 0.3986609860012173, 0.32635774788241156, 0.35890410958904106, 0.7914252607184241]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "1214 22572\n",
      "Embedding... 11219\n",
      "506 11219\n",
      "Training  BernoulliNB\n",
      "[[7085 2127]\n",
      " [ 924 1083]]\n",
      "0.33738317757 0.539611360239 0.415181138585\n",
      "Training  RandomForestClassifier\n",
      "[[9118   94]\n",
      " [1817  190]]\n",
      "0.669014084507 0.0946686596911 0.165866433872\n",
      "Training  BaggingClassifier\n",
      "[[9039  173]\n",
      " [1740  267]]\n",
      "0.606818181818 0.133034379671 0.218226399673\n",
      "Training  ExtraTreesClassifier\n",
      "[[9034  178]\n",
      " [1828  179]]\n",
      "0.501400560224 0.0891878425511 0.151438240271\n",
      "Training  DecisionTreeClassifier\n",
      "[[7959 1253]\n",
      " [1477  530]]\n",
      "0.297251822771 0.264075734928 0.279683377309\n",
      "Training  CalibratedClassifierCV\n",
      "[[9135   77]\n",
      " [1929   78]]\n",
      "0.503225806452 0.0388639760837 0.0721554116559\n",
      "Training  SGDClassifier\n",
      "[[8929  283]\n",
      " [1921   86]]\n",
      "0.233062330623 0.0428500249128 0.0723905723906\n",
      "Training  KNeighborsClassifier\n",
      "[[8010 1202]\n",
      " [1298  709]]\n",
      "0.371009942439 0.353263577479 0.361919346605\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35711549\n",
      "Iteration 2, loss = 0.30939865\n",
      "Iteration 3, loss = 0.29294240\n",
      "Iteration 4, loss = 0.28111907\n",
      "Iteration 5, loss = 0.26902764\n",
      "Iteration 6, loss = 0.25811613\n",
      "Iteration 7, loss = 0.24965978\n",
      "Iteration 8, loss = 0.24073518\n",
      "Iteration 9, loss = 0.23293769\n",
      "Iteration 10, loss = 0.22526390\n",
      "Iteration 11, loss = 0.21650483\n",
      "Iteration 12, loss = 0.20978711\n",
      "Iteration 13, loss = 0.20322847\n",
      "Iteration 14, loss = 0.20012365\n",
      "Iteration 15, loss = 0.18998450\n",
      "Iteration 16, loss = 0.18517804\n",
      "Iteration 17, loss = 0.17962761\n",
      "Iteration 18, loss = 0.17348147\n",
      "Iteration 19, loss = 0.17248542\n",
      "Iteration 20, loss = 0.16665010\n",
      "Iteration 21, loss = 0.16123592\n",
      "Iteration 22, loss = 0.15692151\n",
      "Iteration 23, loss = 0.15195025\n",
      "Iteration 24, loss = 0.15050596\n",
      "Iteration 25, loss = 0.14539370\n",
      "Iteration 26, loss = 0.14181422\n",
      "Iteration 27, loss = 0.13962624\n",
      "Iteration 28, loss = 0.13575867\n",
      "Iteration 29, loss = 0.13364497\n",
      "Iteration 30, loss = 0.13164660\n",
      "Iteration 31, loss = 0.12640912\n",
      "Iteration 32, loss = 0.12462872\n",
      "Iteration 33, loss = 0.12054776\n",
      "Iteration 34, loss = 0.12024491\n",
      "Iteration 35, loss = 0.11712954\n",
      "Iteration 36, loss = 0.11616489\n",
      "Iteration 37, loss = 0.11401072\n",
      "Iteration 38, loss = 0.10973575\n",
      "Iteration 39, loss = 0.11003981\n",
      "Iteration 40, loss = 0.10749641\n",
      "Iteration 41, loss = 0.11146703\n",
      "Iteration 42, loss = 0.10478589\n",
      "Iteration 43, loss = 0.10275857\n",
      "Iteration 44, loss = 0.10406103\n",
      "Iteration 45, loss = 0.10422468\n",
      "Iteration 46, loss = 0.10025645\n",
      "Iteration 47, loss = 0.09802453\n",
      "Iteration 48, loss = 0.09590151\n",
      "Iteration 49, loss = 0.09390101\n",
      "Iteration 50, loss = 0.09992686\n",
      "Iteration 51, loss = 0.09200904\n",
      "Iteration 52, loss = 0.09122807\n",
      "Iteration 53, loss = 0.09134046\n",
      "Iteration 54, loss = 0.08781642\n",
      "Iteration 55, loss = 0.09086899\n",
      "Iteration 56, loss = 0.09057121\n",
      "Iteration 57, loss = 0.08728457\n",
      "Iteration 58, loss = 0.08467729\n",
      "Iteration 59, loss = 0.08876508\n",
      "Iteration 60, loss = 0.08546347\n",
      "Iteration 61, loss = 0.08622137\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8017 1195]\n",
      " [1198  809]]\n",
      "0.40369261477 0.403089187843 0.403390675642\n",
      "['BernoulliNB', 0.33738317757009345, 0.5396113602391629, 0.41518113858539396, 0.728050628398253]\n",
      "['RandomForestClassifier', 0.6690140845070423, 0.09466865969108122, 0.16586643387167177, 0.8296639629200464]\n",
      "['BaggingClassifier', 0.6068181818181818, 0.13303437967115098, 0.2182263996730691, 0.8294856939121134]\n",
      "['ExtraTreesClassifier', 0.5014005602240896, 0.08918784255107125, 0.15143824027072758, 0.8211961850432302]\n",
      "['DecisionTreeClassifier', 0.29725182277061135, 0.2640757349277529, 0.2796833773087071, 0.7566628041714948]\n",
      "['CalibratedClassifierCV', 0.5032258064516129, 0.03886397608370702, 0.07215541165587418, 0.8211961850432302]\n",
      "['SGDClassifier', 0.23306233062330622, 0.04285002491280518, 0.0723905723905724, 0.8035475532578661]\n",
      "['KNeighborsClassifier', 0.3710099424385139, 0.35326357747882414, 0.361919346605411, 0.7771637400837864]\n",
      "['MLPClassifier', 0.4036926147704591, 0.40308918784255104, 0.40339067564198455, 0.7867011320082004]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'csv_3_ReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping\n",
    "When we breaked the sentences to contain only the middle context it was observed that comma seperated BR's had a disadvantage as in some sentences that was taken as middle context, and no other words appeared. So to get more out the context of the surronding words around BR, BR's appearing together were grouped into a single BR entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatSen(sentence):\n",
    "    sentence = re.sub(\"\\s(the|The)\\s\",\" \",sentence)\n",
    "    sentence = re.sub(\"^(the|The)\",\"\",sentence)\n",
    "    sentence = re.sub(\"(nucleus)\",\" \",sentence)\n",
    "    sentence = re.sub(r\",\",\" \", sentence)\n",
    "    sentence = re.sub(\"\\([0-9]\\)\",\".\",sentence)\n",
    "    sentence = re.sub(\"[, ]*(BR[^12])([, ]*(BR[^12]))+[, ]*\",\" BR \",sentence)\n",
    "    sentence = re.sub(\"[, ]*(BR[, ]+)*BR1([, ]+BR[^12])*[, ]*\",\" BR1 \",sentence)\n",
    "    sentence = re.sub(\"[, ]*(BR[, ]+)*BR2([, ]+BR[^12])*[, ]*\",\" BR2 \",sentence)\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFSen = []\n",
    "for i in range(0,trainLen):\n",
    "    trainFSen.append(formatSen(trainSen[i]))\n",
    "    \n",
    "testFSen = []\n",
    "for i in range(0,testLen):\n",
    "    testFSen.append(formatSen(testSen[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['examined optic ( br1 so ) intermediate gray ( br2 sgi )',\n",
       " 'examined optic ( br1 so ) intermediate gray ( br sgi ) intermediate white ( br sai ) and deep gray ( br2 sgp )',\n",
       " 'intermediate gray ( br1 sgi ) intermediate white ( br sai ) and deep gray ( br2 sgp )',\n",
       " 'examined optic ( br1 so ) intermediate gray ( br sgi ) intermediate white ( br2 sai )',\n",
       " 'intermediate gray ( br1 sgi ) intermediate white ( br2 sai )']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMid = breakSen(trainFSen)\n",
    "testMid = breakSen(testFSen)\n",
    "trainMid[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  BernoulliNB\n",
      "[[9021  191]\n",
      " [1845  162]]\n",
      "0.458923512748 0.0807174887892 0.137288135593\n",
      "Training  RandomForestClassifier\n",
      "[[9060  152]\n",
      " [1559  448]]\n",
      "0.746666666667 0.223218734429 0.343690065209\n",
      "Training  BaggingClassifier\n",
      "[[8650  562]\n",
      " [1276  731]]\n",
      "0.565351894818 0.364225211759 0.44303030303\n",
      "Training  ExtraTreesClassifier\n",
      "[[9006  206]\n",
      " [1613  394]]\n",
      "0.656666666667 0.196312904833 0.302263137706\n",
      "Training  DecisionTreeClassifier\n",
      "[[8396  816]\n",
      " [1169  838]]\n",
      "0.506650544135 0.417538614848 0.457798415733\n",
      "Training  CalibratedClassifierCV\n",
      "[[8720  492]\n",
      " [1267  740]]\n",
      "0.600649350649 0.368709516692 0.45693115159\n",
      "Training  SGDClassifier\n",
      "[[8919  293]\n",
      " [1514  493]]\n",
      "0.627226463104 0.245640259093 0.353025420695\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35870140\n",
      "Iteration 2, loss = 0.11035559\n",
      "Iteration 3, loss = 0.04746342\n",
      "Iteration 4, loss = 0.02892450\n",
      "Iteration 5, loss = 0.02160953\n",
      "Iteration 6, loss = 0.01584850\n",
      "Iteration 7, loss = 0.01319013\n",
      "Iteration 8, loss = 0.01166109\n",
      "Iteration 9, loss = 0.01011525\n",
      "Iteration 10, loss = 0.00983266\n",
      "Iteration 11, loss = 0.00929591\n",
      "Iteration 12, loss = 0.00905977\n",
      "Iteration 13, loss = 0.00819208\n",
      "Iteration 14, loss = 0.00811347\n",
      "Iteration 15, loss = 0.00796355\n",
      "Iteration 16, loss = 0.00782711\n",
      "Iteration 17, loss = 0.00769519\n",
      "Iteration 18, loss = 0.00681670\n",
      "Iteration 19, loss = 0.00662773\n",
      "Iteration 20, loss = 0.00614013\n",
      "Iteration 21, loss = 0.00676261\n",
      "Iteration 22, loss = 0.00588664\n",
      "Iteration 23, loss = 0.00599674\n",
      "Iteration 24, loss = 0.00630831\n",
      "Iteration 25, loss = 0.00593900\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8626  586]\n",
      " [1260  747]]\n",
      "0.560390097524 0.372197309417 0.447305389222\n",
      "['BernoulliNB', 0.45892351274787535, 0.08071748878923767, 0.13728813559322034, 0.8185221499242357]\n",
      "['RandomForestClassifier', 0.7466666666666667, 0.22321873442949677, 0.3436900652090526, 0.8474908637133435]\n",
      "['BaggingClassifier', 0.5653518948182521, 0.36422521175884404, 0.443030303030303, 0.8361707817095998]\n",
      "['ExtraTreesClassifier', 0.6566666666666666, 0.1963129048330842, 0.3022631377061757, 0.837864337284963]\n",
      "['DecisionTreeClassifier', 0.5066505441354293, 0.4175386148480319, 0.4577984157334062, 0.8230680096265264]\n",
      "['CalibratedClassifierCV', 0.6006493506493507, 0.3687095166915795, 0.4569311515899969, 0.8432124075229521]\n",
      "['SGDClassifier', 0.6272264631043257, 0.24564025909317388, 0.35302542069459364, 0.8389339513325609]\n",
      "['MLPClassifier', 0.5603900975243811, 0.3721973094170404, 0.4473053892215569, 0.8354577056778679]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_vect, test_vect = convertTfidf(trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "0 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7058 2154]\n",
      " [ 942 1065]]\n",
      "0.330848089469 0.530642750374 0.40757749713\n",
      "Training  RandomForestClassifier\n",
      "[[9133   79]\n",
      " [1873  134]]\n",
      "0.629107981221 0.0667663178874 0.120720720721\n",
      "Training  BaggingClassifier\n",
      "[[9014  198]\n",
      " [1807  200]]\n",
      "0.502512562814 0.0996512207275 0.16632016632\n",
      "Training  ExtraTreesClassifier\n",
      "[[9004  208]\n",
      " [1834  173]]\n",
      "0.45406824147 0.0861983059292 0.144891122278\n",
      "Training  DecisionTreeClassifier\n",
      "[[7715 1497]\n",
      " [1483  524]]\n",
      "0.259277585354 0.261086198306 0.260178748759\n",
      "Training  CalibratedClassifierCV\n",
      "[[9015  197]\n",
      " [1858  149]]\n",
      "0.43063583815 0.074240159442 0.126646833829\n",
      "Training  SGDClassifier\n",
      "[[8917  295]\n",
      " [1833  174]]\n",
      "0.371002132196 0.0866965620329 0.140549273021\n",
      "Training  KNeighborsClassifier\n",
      "[[8288  924]\n",
      " [1425  582]]\n",
      "0.386454183267 0.289985052317 0.331340734415\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35100552\n",
      "Iteration 2, loss = 0.30122300\n",
      "Iteration 3, loss = 0.28292462\n",
      "Iteration 4, loss = 0.26833602\n",
      "Iteration 5, loss = 0.25759949\n",
      "Iteration 6, loss = 0.24842662\n",
      "Iteration 7, loss = 0.23655429\n",
      "Iteration 8, loss = 0.22735727\n",
      "Iteration 9, loss = 0.22048918\n",
      "Iteration 10, loss = 0.21009884\n",
      "Iteration 11, loss = 0.20130884\n",
      "Iteration 12, loss = 0.19208246\n",
      "Iteration 13, loss = 0.18452797\n",
      "Iteration 14, loss = 0.17660192\n",
      "Iteration 15, loss = 0.17295599\n",
      "Iteration 16, loss = 0.16510584\n",
      "Iteration 17, loss = 0.15674597\n",
      "Iteration 18, loss = 0.15218397\n",
      "Iteration 19, loss = 0.14762292\n",
      "Iteration 20, loss = 0.14373292\n",
      "Iteration 21, loss = 0.13977902\n",
      "Iteration 22, loss = 0.13244850\n",
      "Iteration 23, loss = 0.12994006\n",
      "Iteration 24, loss = 0.12662871\n",
      "Iteration 25, loss = 0.11914886\n",
      "Iteration 26, loss = 0.11850205\n",
      "Iteration 27, loss = 0.11825544\n",
      "Iteration 28, loss = 0.11236217\n",
      "Iteration 29, loss = 0.10888849\n",
      "Iteration 30, loss = 0.10519812\n",
      "Iteration 31, loss = 0.10301481\n",
      "Iteration 32, loss = 0.09925707\n",
      "Iteration 33, loss = 0.09535414\n",
      "Iteration 34, loss = 0.09542025\n",
      "Iteration 35, loss = 0.09063739\n",
      "Iteration 36, loss = 0.08890382\n",
      "Iteration 37, loss = 0.09512328\n",
      "Iteration 38, loss = 0.08477964\n",
      "Iteration 39, loss = 0.08353823\n",
      "Iteration 40, loss = 0.08065310\n",
      "Iteration 41, loss = 0.08032764\n",
      "Iteration 42, loss = 0.07891540\n",
      "Iteration 43, loss = 0.07794583\n",
      "Iteration 44, loss = 0.07544630\n",
      "Iteration 45, loss = 0.07253404\n",
      "Iteration 46, loss = 0.07356202\n",
      "Iteration 47, loss = 0.07178915\n",
      "Iteration 48, loss = 0.06948476\n",
      "Iteration 49, loss = 0.07023736\n",
      "Iteration 50, loss = 0.06856669\n",
      "Iteration 51, loss = 0.06883576\n",
      "Iteration 52, loss = 0.06595366\n",
      "Iteration 53, loss = 0.06158447\n",
      "Iteration 54, loss = 0.06320505\n",
      "Iteration 55, loss = 0.06197734\n",
      "Iteration 56, loss = 0.06266221\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8377  835]\n",
      " [1335  672]]\n",
      "0.445919044459 0.334828101644 0.382470119522\n",
      "['BernoulliNB', 0.33084808946877914, 0.5306427503736921, 0.40757749712973596, 0.7240395757197611]\n",
      "['RandomForestClassifier', 0.6291079812206573, 0.06676631788739412, 0.12072072072072072, 0.8260094482574204]\n",
      "['BaggingClassifier', 0.5025125628140703, 0.09965122072745392, 0.16632016632016633, 0.8212853195471967]\n",
      "['ExtraTreesClassifier', 0.4540682414698163, 0.08619830592924764, 0.14489112227805698, 0.8179873429004367]\n",
      "['DecisionTreeClassifier', 0.25927758535378526, 0.2610861983059293, 0.2601787487586891, 0.7343791781798734]\n",
      "['CalibratedClassifierCV', 0.430635838150289, 0.07424015944195317, 0.12664683382915431, 0.8168285943488724]\n",
      "['SGDClassifier', 0.37100213219616207, 0.08669656203288491, 0.14054927302100165, 0.8103217755593191]\n",
      "['KNeighborsClassifier', 0.38645418326693226, 0.28998505231689087, 0.33134073441502987, 0.7906230501827257]\n",
      "['MLPClassifier', 0.44591904445919045, 0.33482810164424515, 0.3824701195219123, 0.8065781263927266]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "10 22572\n",
      "Embedding... 11219\n",
      "1 11219\n",
      "Training  BernoulliNB\n",
      "[[7234 1978]\n",
      " [1024  983]]\n",
      "0.331982438365 0.489785749875 0.395732689211\n",
      "Training  RandomForestClassifier\n",
      "[[9116   96]\n",
      " [1847  160]]\n",
      "0.625 0.079720976582 0.141405214317\n",
      "Training  BaggingClassifier\n",
      "[[9031  181]\n",
      " [1751  256]]\n",
      "0.585812356979 0.127553562531 0.209492635025\n",
      "Training  ExtraTreesClassifier\n",
      "[[8943  269]\n",
      " [1820  187]]\n",
      "0.410087719298 0.0931738913802 0.151847340641\n",
      "Training  DecisionTreeClassifier\n",
      "[[7778 1434]\n",
      " [1468  539]]\n",
      "0.27318803852 0.26856003986 0.270854271357\n",
      "Training  CalibratedClassifierCV\n",
      "[[9037  175]\n",
      " [1822  185]]\n",
      "0.513888888889 0.0921773791729 0.156316011829\n",
      "Training  SGDClassifier\n",
      "[[6034 3178]\n",
      " [ 932 1075]]\n",
      "0.252762755702 0.53562531141 0.343450479233\n",
      "Training  KNeighborsClassifier\n",
      "[[8026 1186]\n",
      " [1358  649]]\n",
      "0.353678474114 0.323368211261 0.337844872462\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35300460\n",
      "Iteration 2, loss = 0.30599332\n",
      "Iteration 3, loss = 0.28854990\n",
      "Iteration 4, loss = 0.27350239\n",
      "Iteration 5, loss = 0.25768818\n",
      "Iteration 6, loss = 0.24557990\n",
      "Iteration 7, loss = 0.23467516\n",
      "Iteration 8, loss = 0.22347773\n",
      "Iteration 9, loss = 0.21363205\n",
      "Iteration 10, loss = 0.20515441\n",
      "Iteration 11, loss = 0.19805881\n",
      "Iteration 12, loss = 0.19138067\n",
      "Iteration 13, loss = 0.18254185\n",
      "Iteration 14, loss = 0.17642911\n",
      "Iteration 15, loss = 0.17081111\n",
      "Iteration 16, loss = 0.16556820\n",
      "Iteration 17, loss = 0.15831641\n",
      "Iteration 18, loss = 0.15443291\n",
      "Iteration 19, loss = 0.14960640\n",
      "Iteration 20, loss = 0.14618694\n",
      "Iteration 21, loss = 0.13906419\n",
      "Iteration 22, loss = 0.13769439\n",
      "Iteration 23, loss = 0.13362758\n",
      "Iteration 24, loss = 0.12984444\n",
      "Iteration 25, loss = 0.12511365\n",
      "Iteration 26, loss = 0.12094985\n",
      "Iteration 27, loss = 0.11669001\n",
      "Iteration 28, loss = 0.11656883\n",
      "Iteration 29, loss = 0.11361207\n",
      "Iteration 30, loss = 0.11150652\n",
      "Iteration 31, loss = 0.10653505\n",
      "Iteration 32, loss = 0.10415618\n",
      "Iteration 33, loss = 0.10317600\n",
      "Iteration 34, loss = 0.10059445\n",
      "Iteration 35, loss = 0.09788484\n",
      "Iteration 36, loss = 0.09565915\n",
      "Iteration 37, loss = 0.09390597\n",
      "Iteration 38, loss = 0.09324823\n",
      "Iteration 39, loss = 0.09233865\n",
      "Iteration 40, loss = 0.09024443\n",
      "Iteration 41, loss = 0.08609239\n",
      "Iteration 42, loss = 0.08479429\n",
      "Iteration 43, loss = 0.08367522\n",
      "Iteration 44, loss = 0.08155381\n",
      "Iteration 45, loss = 0.08056610\n",
      "Iteration 46, loss = 0.07973897\n",
      "Iteration 47, loss = 0.07938197\n",
      "Iteration 48, loss = 0.08168981\n",
      "Iteration 49, loss = 0.07754303\n",
      "Iteration 50, loss = 0.07629481\n",
      "Iteration 51, loss = 0.07245487\n",
      "Iteration 52, loss = 0.07410960\n",
      "Iteration 53, loss = 0.07359157\n",
      "Iteration 54, loss = 0.07034632\n",
      "Iteration 55, loss = 0.06946764\n",
      "Iteration 56, loss = 0.07111748\n",
      "Iteration 57, loss = 0.06777185\n",
      "Iteration 58, loss = 0.07012462\n",
      "Iteration 59, loss = 0.06563603\n",
      "Iteration 60, loss = 0.06619309\n",
      "Iteration 61, loss = 0.06557039\n",
      "Iteration 62, loss = 0.06498776\n",
      "Iteration 63, loss = 0.06397307\n",
      "Iteration 64, loss = 0.06371093\n",
      "Iteration 65, loss = 0.06542590\n",
      "Iteration 66, loss = 0.06158825\n",
      "Iteration 67, loss = 0.06338371\n",
      "Iteration 68, loss = 0.06219044\n",
      "Iteration 69, loss = 0.06315981\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[7835 1377]\n",
      " [1177  830]]\n",
      "0.376076121432 0.413552566019 0.393925011865\n",
      "['BernoulliNB', 0.3319824383654171, 0.489785749875436, 0.39573268921095006, 0.7324182190926107]\n",
      "['RandomForestClassifier', 0.625, 0.07972097658196313, 0.14140521431727796, 0.8268116587931188]\n",
      "['BaggingClassifier', 0.585812356979405, 0.127553562531141, 0.2094926350245499, 0.8277921383367501]\n",
      "['ExtraTreesClassifier', 0.4100877192982456, 0.0931738913801694, 0.1518473406414941, 0.8137980212140119]\n",
      "['DecisionTreeClassifier', 0.27318803852002027, 0.26856003986048826, 0.2708542713567839, 0.7413316694892593]\n",
      "['CalibratedClassifierCV', 0.5138888888888888, 0.09217737917289487, 0.15631601182931984, 0.8219983955789286]\n",
      "['SGDClassifier', 0.2527627557018575, 0.5356253114100648, 0.3434504792332268, 0.6336571886977449]\n",
      "['KNeighborsClassifier', 0.3536784741144414, 0.32336821126058796, 0.33784487246225925, 0.773241821909261]\n",
      "['MLPClassifier', 0.3760761214318079, 0.41355256601893375, 0.3939250118652112, 0.7723504768695962]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "0 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[7136 2076]\n",
      " [ 977 1030]]\n",
      "0.331616226658 0.513203786746 0.402894582437\n",
      "Training  RandomForestClassifier\n",
      "[[9150   62]\n",
      " [1879  128]]\n",
      "0.673684210526 0.0637767812656 0.116522530724\n",
      "Training  BaggingClassifier\n",
      "[[9038  174]\n",
      " [1808  199]]\n",
      "0.533512064343 0.0991529646238 0.167226890756\n",
      "Training  ExtraTreesClassifier\n",
      "[[9023  189]\n",
      " [1835  172]]\n",
      "0.476454293629 0.0857000498256 0.14527027027\n",
      "Training  DecisionTreeClassifier\n",
      "[[7690 1522]\n",
      " [1435  572]]\n",
      "0.273161413563 0.285002491281 0.278956352109\n",
      "Training  CalibratedClassifierCV\n",
      "[[9018  194]\n",
      " [1865  142]]\n",
      "0.422619047619 0.0707523667165 0.121212121212\n",
      "Training  SGDClassifier\n",
      "[[7703 1509]\n",
      " [1184  823]]\n",
      "0.352915951973 0.410064773293 0.379350080664\n",
      "Training  KNeighborsClassifier\n",
      "[[8291  921]\n",
      " [1384  623]]\n",
      "0.403497409326 0.310413552566 0.350887074064\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.36131800\n",
      "Iteration 2, loss = 0.30452838\n",
      "Iteration 3, loss = 0.28842600\n",
      "Iteration 4, loss = 0.27603270\n",
      "Iteration 5, loss = 0.26366487\n",
      "Iteration 6, loss = 0.25049905\n",
      "Iteration 7, loss = 0.23976693\n",
      "Iteration 8, loss = 0.22816408\n",
      "Iteration 9, loss = 0.21644980\n",
      "Iteration 10, loss = 0.20585128\n",
      "Iteration 11, loss = 0.20001283\n",
      "Iteration 12, loss = 0.18730848\n",
      "Iteration 13, loss = 0.18136282\n",
      "Iteration 14, loss = 0.17069199\n",
      "Iteration 15, loss = 0.16321783\n",
      "Iteration 16, loss = 0.15492033\n",
      "Iteration 17, loss = 0.14890036\n",
      "Iteration 18, loss = 0.14307946\n",
      "Iteration 19, loss = 0.13641024\n",
      "Iteration 20, loss = 0.13039871\n",
      "Iteration 21, loss = 0.12488057\n",
      "Iteration 22, loss = 0.11922658\n",
      "Iteration 23, loss = 0.11536968\n",
      "Iteration 24, loss = 0.10776194\n",
      "Iteration 25, loss = 0.10477833\n",
      "Iteration 26, loss = 0.10117498\n",
      "Iteration 27, loss = 0.09728463\n",
      "Iteration 28, loss = 0.09246671\n",
      "Iteration 29, loss = 0.09145689\n",
      "Iteration 30, loss = 0.08636841\n",
      "Iteration 31, loss = 0.08443756\n",
      "Iteration 32, loss = 0.08406730\n",
      "Iteration 33, loss = 0.08254806\n",
      "Iteration 34, loss = 0.07665324\n",
      "Iteration 35, loss = 0.07549140\n",
      "Iteration 36, loss = 0.07498426\n",
      "Iteration 37, loss = 0.07230527\n",
      "Iteration 38, loss = 0.07144207\n",
      "Iteration 39, loss = 0.06632934\n",
      "Iteration 40, loss = 0.06671521\n",
      "Iteration 41, loss = 0.06563986\n",
      "Iteration 42, loss = 0.06363019\n",
      "Iteration 43, loss = 0.05961976\n",
      "Iteration 44, loss = 0.06213843\n",
      "Iteration 45, loss = 0.05932626\n",
      "Iteration 46, loss = 0.05895621\n",
      "Iteration 47, loss = 0.06016363\n",
      "Iteration 48, loss = 0.05890732\n",
      "Iteration 49, loss = 0.05384471\n",
      "Iteration 50, loss = 0.05358557\n",
      "Iteration 51, loss = 0.05274737\n",
      "Iteration 52, loss = 0.05180088\n",
      "Iteration 53, loss = 0.05568367\n",
      "Iteration 54, loss = 0.04996240\n",
      "Iteration 55, loss = 0.04957516\n",
      "Iteration 56, loss = 0.04960343\n",
      "Iteration 57, loss = 0.04936307\n",
      "Iteration 58, loss = 0.04907092\n",
      "Iteration 59, loss = 0.04746339\n",
      "Iteration 60, loss = 0.04901642\n",
      "Iteration 61, loss = 0.04746087\n",
      "Iteration 62, loss = 0.04545367\n",
      "Iteration 63, loss = 0.04743177\n",
      "Iteration 64, loss = 0.04465463\n",
      "Iteration 65, loss = 0.04319738\n",
      "Iteration 66, loss = 0.04521674\n",
      "Iteration 67, loss = 0.04642284\n",
      "Iteration 68, loss = 0.04429342\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8077 1135]\n",
      " [1239  768]]\n",
      "0.403573305307 0.382660687593 0.39283887468\n",
      "['BernoulliNB', 0.33161622665808116, 0.5132037867463877, 0.40289458243692555, 0.72787235939032]\n",
      "['RandomForestClassifier', 0.6736842105263158, 0.0637767812655705, 0.11652253072371414, 0.8269899278010517]\n",
      "['BaggingClassifier', 0.5335120643431636, 0.09915296462381665, 0.16722689075630254, 0.8233354131384258]\n",
      "['ExtraTreesClassifier', 0.47645429362880887, 0.08570004982561036, 0.14527027027027029, 0.8195917639718335]\n",
      "['DecisionTreeClassifier', 0.2731614135625597, 0.2850024912805182, 0.27895635210924163, 0.7364292717711026]\n",
      "['CalibratedClassifierCV', 0.4226190476190476, 0.07075236671649228, 0.12121212121212123, 0.8164720563330065]\n",
      "['SGDClassifier', 0.35291595197255576, 0.41006477329347285, 0.3793500806637474, 0.7599607808182548]\n",
      "['KNeighborsClassifier', 0.40349740932642486, 0.31041355256601894, 0.35088707406364406, 0.7945449683572511]\n",
      "['MLPClassifier', 0.40357330530740937, 0.38266068759342303, 0.3928388746803069, 0.7883946875835636]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'csv_2_groupBR12'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding... 22572\n",
      "0 22572\n",
      "Embedding... 11219\n",
      "0 11219\n",
      "Training  BernoulliNB\n",
      "[[6787 2425]\n",
      " [ 874 1133]]\n",
      "0.31843732434 0.564524165421 0.407187780773\n",
      "Training  RandomForestClassifier\n",
      "[[9138   74]\n",
      " [1885  122]]\n",
      "0.622448979592 0.0607872446437 0.110758057195\n",
      "Training  BaggingClassifier\n",
      "[[9068  144]\n",
      " [1819  188]]\n",
      "0.566265060241 0.0936721474838 0.160752458316\n",
      "Training  ExtraTreesClassifier\n",
      "[[9032  180]\n",
      " [1851  156]]\n",
      "0.464285714286 0.0777279521674 0.133162612036\n",
      "Training  DecisionTreeClassifier\n",
      "[[7818 1394]\n",
      " [1411  596]]\n",
      "0.299497487437 0.296960637768 0.298223667751\n",
      "Training  CalibratedClassifierCV\n",
      "[[8972  240]\n",
      " [1846  161]]\n",
      "0.401496259352 0.0802192326856 0.133720930233\n",
      "Training  SGDClassifier\n",
      "[[9006  206]\n",
      " [1862  145]]\n",
      "0.413105413105 0.0722471350274 0.122985581001\n",
      "Training  KNeighborsClassifier\n",
      "[[8054 1158]\n",
      " [1372  635]]\n",
      "0.354155047407 0.31639262581 0.334210526316\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.34518678\n",
      "Iteration 2, loss = 0.30647807\n",
      "Iteration 3, loss = 0.29051010\n",
      "Iteration 4, loss = 0.27648151\n",
      "Iteration 5, loss = 0.26196028\n",
      "Iteration 6, loss = 0.24947082\n",
      "Iteration 7, loss = 0.23785409\n",
      "Iteration 8, loss = 0.22534376\n",
      "Iteration 9, loss = 0.21748672\n",
      "Iteration 10, loss = 0.20764623\n",
      "Iteration 11, loss = 0.19762488\n",
      "Iteration 12, loss = 0.19059609\n",
      "Iteration 13, loss = 0.18219621\n",
      "Iteration 14, loss = 0.17393533\n",
      "Iteration 15, loss = 0.16745352\n",
      "Iteration 16, loss = 0.16177199\n",
      "Iteration 17, loss = 0.15364792\n",
      "Iteration 18, loss = 0.15132348\n",
      "Iteration 19, loss = 0.14428133\n",
      "Iteration 20, loss = 0.13681596\n",
      "Iteration 21, loss = 0.13428249\n",
      "Iteration 22, loss = 0.13013019\n",
      "Iteration 23, loss = 0.12368202\n",
      "Iteration 24, loss = 0.12061741\n",
      "Iteration 25, loss = 0.11544699\n",
      "Iteration 26, loss = 0.11327047\n",
      "Iteration 27, loss = 0.10989061\n",
      "Iteration 28, loss = 0.10838618\n",
      "Iteration 29, loss = 0.10273920\n",
      "Iteration 30, loss = 0.09930285\n",
      "Iteration 31, loss = 0.09585039\n",
      "Iteration 32, loss = 0.09433326\n",
      "Iteration 33, loss = 0.09180620\n",
      "Iteration 34, loss = 0.09166513\n",
      "Iteration 35, loss = 0.08713370\n",
      "Iteration 36, loss = 0.08297118\n",
      "Iteration 37, loss = 0.08321080\n",
      "Iteration 38, loss = 0.08232176\n",
      "Iteration 39, loss = 0.08196288\n",
      "Iteration 40, loss = 0.07640205\n",
      "Iteration 41, loss = 0.07593650\n",
      "Iteration 42, loss = 0.07086225\n",
      "Iteration 43, loss = 0.07426977\n",
      "Iteration 44, loss = 0.07155639\n",
      "Iteration 45, loss = 0.06820896\n",
      "Iteration 46, loss = 0.06663261\n",
      "Iteration 47, loss = 0.06823818\n",
      "Iteration 48, loss = 0.06831302\n",
      "Iteration 49, loss = 0.06308909\n",
      "Iteration 50, loss = 0.06117953\n",
      "Iteration 51, loss = 0.06720062\n",
      "Iteration 52, loss = 0.05856874\n",
      "Iteration 53, loss = 0.05716287\n",
      "Iteration 54, loss = 0.05903037\n",
      "Iteration 55, loss = 0.06216006\n",
      "Iteration 56, loss = 0.05847029\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8107 1105]\n",
      " [1264  743]]\n",
      "0.402056277056 0.370204285002 0.385473411154\n",
      "['BernoulliNB', 0.3184373243395166, 0.5645241654210265, 0.4071877807726864, 0.7059452714145645]\n",
      "['RandomForestClassifier', 0.6224489795918368, 0.060787244643746886, 0.11075805719473446, 0.825385506729655]\n",
      "['BaggingClassifier', 0.5662650602409639, 0.09367214748380667, 0.16075245831551946, 0.8250289687137891]\n",
      "['ExtraTreesClassifier', 0.4642857142857143, 0.07772795216741404, 0.13316261203585147, 0.8189678224440681]\n",
      "['DecisionTreeClassifier', 0.2994974874371859, 0.2969606377678127, 0.29822366775081316, 0.7499777163740083]\n",
      "['CalibratedClassifierCV', 0.4014962593516209, 0.0802192326856004, 0.13372093023255816, 0.8140654247259114]\n",
      "['SGDClassifier', 0.4131054131054131, 0.07224713502740408, 0.12298558100084817, 0.8156698457973082]\n",
      "['KNeighborsClassifier', 0.35415504740658116, 0.31639262580966615, 0.33421052631578946, 0.7744897049647919]\n",
      "['MLPClassifier', 0.40205627705627706, 0.3702042850024913, 0.385473411154345, 0.788840360103396]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_trans = Phrases(trainFSen)\n",
    "\n",
    "modelName = 'csv_2_groupBR12_bigram'\n",
    "train_vect, test_vect = buildW2V(modelName,bigram_trans[trainMid], bigram_trans[testMid])\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Till now we were spliting the sentences using white space seperator. So the problem was the brackets were considered together with the words and not seperately. So before getting the vector we tokenize the sentence using nltk package function. This is only for word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def convertW2V(data,w2v):\n",
    "    wholeM = []\n",
    "    print 'Embedding...'\n",
    "    for sentence in data:\n",
    "        arr = []    \n",
    "        for word in word_tokenize(sentence):\n",
    "            if word in w2v:\n",
    "                arr.append(np.array(w2v[word],copy=True))  \n",
    "                                #Each word is checked if it is there in the word2vec vocabulary. If there then\n",
    "                                #the vector space for the word is taken and then the mean is calculated.\n",
    "\n",
    "        mean = np.zeros(100)\n",
    "        for mat in arr:\n",
    "            for j in range(len(mat)):\n",
    "                mean[j] += mat[j]\n",
    "        if len(arr) != 0:\n",
    "            mean = np.array(mean/len(arr))\n",
    "        wholeM.append(mean)\n",
    "    return wholeM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[7058 2154]\n",
      " [ 942 1065]]\n",
      "0.330848089469 0.530642750374 0.40757749713\n",
      "Training  RandomForestClassifier\n",
      "[[9120   92]\n",
      " [1875  132]]\n",
      "0.589285714286 0.0657698056801 0.118332586284\n",
      "Training  BaggingClassifier\n",
      "[[9010  202]\n",
      " [1824  183]]\n",
      "0.475324675325 0.0911808669656 0.153010033445\n",
      "Training  ExtraTreesClassifier\n",
      "[[9016  196]\n",
      " [1789  218]]\n",
      "0.526570048309 0.108619830593 0.180090871541\n",
      "Training  DecisionTreeClassifier\n",
      "[[7808 1404]\n",
      " [1422  585]]\n",
      "0.294117647059 0.291479820628 0.292792792793\n",
      "Training  CalibratedClassifierCV\n",
      "[[9015  197]\n",
      " [1858  149]]\n",
      "0.43063583815 0.074240159442 0.126646833829\n",
      "Training  SGDClassifier\n",
      "[[8969  243]\n",
      " [1943   64]]\n",
      "0.208469055375 0.0318883906328 0.0553154710458\n",
      "Training  KNeighborsClassifier\n",
      "[[8288  924]\n",
      " [1425  582]]\n",
      "0.386454183267 0.289985052317 0.331340734415\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.34221632\n",
      "Iteration 2, loss = 0.29992809\n",
      "Iteration 3, loss = 0.28144065\n",
      "Iteration 4, loss = 0.26681330\n",
      "Iteration 5, loss = 0.25453012\n",
      "Iteration 6, loss = 0.24321478\n",
      "Iteration 7, loss = 0.23360942\n",
      "Iteration 8, loss = 0.22425551\n",
      "Iteration 9, loss = 0.21599458\n",
      "Iteration 10, loss = 0.20682509\n",
      "Iteration 11, loss = 0.19825357\n",
      "Iteration 12, loss = 0.18995857\n",
      "Iteration 13, loss = 0.18357333\n",
      "Iteration 14, loss = 0.17609703\n",
      "Iteration 15, loss = 0.16898704\n",
      "Iteration 16, loss = 0.16436189\n",
      "Iteration 17, loss = 0.15853370\n",
      "Iteration 18, loss = 0.15191833\n",
      "Iteration 19, loss = 0.14905127\n",
      "Iteration 20, loss = 0.14076894\n",
      "Iteration 21, loss = 0.13764244\n",
      "Iteration 22, loss = 0.13467875\n",
      "Iteration 23, loss = 0.12851767\n",
      "Iteration 24, loss = 0.12771884\n",
      "Iteration 25, loss = 0.12572064\n",
      "Iteration 26, loss = 0.11635934\n",
      "Iteration 27, loss = 0.11564697\n",
      "Iteration 28, loss = 0.11407552\n",
      "Iteration 29, loss = 0.10827624\n",
      "Iteration 30, loss = 0.10497715\n",
      "Iteration 31, loss = 0.10345695\n",
      "Iteration 32, loss = 0.09992425\n",
      "Iteration 33, loss = 0.09742747\n",
      "Iteration 34, loss = 0.09704507\n",
      "Iteration 35, loss = 0.09501084\n",
      "Iteration 36, loss = 0.09215971\n",
      "Iteration 37, loss = 0.08761171\n",
      "Iteration 38, loss = 0.08854985\n",
      "Iteration 39, loss = 0.08457889\n",
      "Iteration 40, loss = 0.08326897\n",
      "Iteration 41, loss = 0.08353022\n",
      "Iteration 42, loss = 0.07891729\n",
      "Iteration 43, loss = 0.07784237\n",
      "Iteration 44, loss = 0.07564285\n",
      "Iteration 45, loss = 0.07532069\n",
      "Iteration 46, loss = 0.07472593\n",
      "Iteration 47, loss = 0.07312426\n",
      "Iteration 48, loss = 0.07245743\n",
      "Iteration 49, loss = 0.06757082\n",
      "Iteration 50, loss = 0.06835055\n",
      "Iteration 51, loss = 0.07108595\n",
      "Iteration 52, loss = 0.06654860\n",
      "Iteration 53, loss = 0.06430086\n",
      "Iteration 54, loss = 0.06594960\n",
      "Iteration 55, loss = 0.06735630\n",
      "Iteration 56, loss = 0.06176329\n",
      "Iteration 57, loss = 0.06060997\n",
      "Iteration 58, loss = 0.06155916\n",
      "Iteration 59, loss = 0.06282752\n",
      "Iteration 60, loss = 0.05847321\n",
      "Iteration 61, loss = 0.05500306\n",
      "Iteration 62, loss = 0.05665523\n",
      "Iteration 63, loss = 0.05682858\n",
      "Iteration 64, loss = 0.05969670\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8273  939]\n",
      " [1331  676]]\n",
      "0.418575851393 0.336821126059 0.373274434014\n",
      "['BernoulliNB', 0.33084808946877914, 0.5306427503736921, 0.40757749712973596, 0.7240395757197611]\n",
      "['RandomForestClassifier', 0.5892857142857143, 0.06576980568011959, 0.1183325862841775, 0.8246724306979232]\n",
      "['BaggingClassifier', 0.4753246753246753, 0.09118086696562033, 0.1530100334448161, 0.8194134949639005]\n",
      "['ExtraTreesClassifier', 0.5265700483091788, 0.10861983059292477, 0.18009087154068565, 0.8230680096265264]\n",
      "['DecisionTreeClassifier', 0.29411764705882354, 0.2914798206278027, 0.2927927927927928, 0.7481058917907122]\n",
      "['CalibratedClassifierCV', 0.430635838150289, 0.07424015944195317, 0.12664683382915431, 0.8168285943488724]\n",
      "['SGDClassifier', 0.20846905537459284, 0.03188839063278525, 0.05531547104580812, 0.8051519743292629]\n",
      "['KNeighborsClassifier', 0.38645418326693226, 0.28998505231689087, 0.33134073441502987, 0.7906230501827257]\n",
      "['MLPClassifier', 0.41857585139318887, 0.3368211260587942, 0.3732744340143567, 0.797664675996078]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[7234 1978]\n",
      " [1024  983]]\n",
      "0.331982438365 0.489785749875 0.395732689211\n",
      "Training  RandomForestClassifier\n",
      "[[9137   75]\n",
      " [1841  166]]\n",
      "0.688796680498 0.0827105132038 0.14768683274\n",
      "Training  BaggingClassifier\n",
      "[[8957  255]\n",
      " [1759  248]]\n",
      "0.493041749503 0.123567513702 0.197609561753\n",
      "Training  ExtraTreesClassifier\n",
      "[[9028  184]\n",
      " [1785  222]]\n",
      "0.546798029557 0.110612855007 0.184003315375\n",
      "Training  DecisionTreeClassifier\n",
      "[[7795 1417]\n",
      " [1511  496]]\n",
      "0.259278619969 0.247135027404 0.25306122449\n",
      "Training  CalibratedClassifierCV\n",
      "[[9037  175]\n",
      " [1822  185]]\n",
      "0.513888888889 0.0921773791729 0.156316011829\n",
      "Training  SGDClassifier\n",
      "[[8880  332]\n",
      " [1736  271]]\n",
      "0.449419568823 0.135027404086 0.207662835249\n",
      "Training  KNeighborsClassifier\n",
      "[[8026 1186]\n",
      " [1358  649]]\n",
      "0.353678474114 0.323368211261 0.337844872462\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.34707201\n",
      "Iteration 2, loss = 0.30414611\n",
      "Iteration 3, loss = 0.28779226\n",
      "Iteration 4, loss = 0.27312975\n",
      "Iteration 5, loss = 0.26086046\n",
      "Iteration 6, loss = 0.24864788\n",
      "Iteration 7, loss = 0.23731594\n",
      "Iteration 8, loss = 0.22934881\n",
      "Iteration 9, loss = 0.21980245\n",
      "Iteration 10, loss = 0.20963225\n",
      "Iteration 11, loss = 0.20307780\n",
      "Iteration 12, loss = 0.19799670\n",
      "Iteration 13, loss = 0.19228972\n",
      "Iteration 14, loss = 0.18493436\n",
      "Iteration 15, loss = 0.17796249\n",
      "Iteration 16, loss = 0.17319111\n",
      "Iteration 17, loss = 0.16871922\n",
      "Iteration 18, loss = 0.16299697\n",
      "Iteration 19, loss = 0.15861421\n",
      "Iteration 20, loss = 0.15258297\n",
      "Iteration 21, loss = 0.14874802\n",
      "Iteration 22, loss = 0.14561359\n",
      "Iteration 23, loss = 0.14060137\n",
      "Iteration 24, loss = 0.13896475\n",
      "Iteration 25, loss = 0.13560048\n",
      "Iteration 26, loss = 0.13256780\n",
      "Iteration 27, loss = 0.12885114\n",
      "Iteration 28, loss = 0.12566479\n",
      "Iteration 29, loss = 0.12431663\n",
      "Iteration 30, loss = 0.11860800\n",
      "Iteration 31, loss = 0.11453706\n",
      "Iteration 32, loss = 0.11346463\n",
      "Iteration 33, loss = 0.11086482\n",
      "Iteration 34, loss = 0.10734665\n",
      "Iteration 35, loss = 0.10536156\n",
      "Iteration 36, loss = 0.10448409\n",
      "Iteration 37, loss = 0.10359388\n",
      "Iteration 38, loss = 0.10122830\n",
      "Iteration 39, loss = 0.09796608\n",
      "Iteration 40, loss = 0.09470096\n",
      "Iteration 41, loss = 0.09435198\n",
      "Iteration 42, loss = 0.09280324\n",
      "Iteration 43, loss = 0.09119153\n",
      "Iteration 44, loss = 0.09025588\n",
      "Iteration 45, loss = 0.08679199\n",
      "Iteration 46, loss = 0.08611916\n",
      "Iteration 47, loss = 0.08594075\n",
      "Iteration 48, loss = 0.08654501\n",
      "Iteration 49, loss = 0.08472778\n",
      "Iteration 50, loss = 0.08069061\n",
      "Iteration 51, loss = 0.07944578\n",
      "Iteration 52, loss = 0.08185746\n",
      "Iteration 53, loss = 0.07726399\n",
      "Iteration 54, loss = 0.07595665\n",
      "Iteration 55, loss = 0.07522997\n",
      "Iteration 56, loss = 0.07563143\n",
      "Iteration 57, loss = 0.07407825\n",
      "Iteration 58, loss = 0.07288443\n",
      "Iteration 59, loss = 0.07345434\n",
      "Iteration 60, loss = 0.07048579\n",
      "Iteration 61, loss = 0.06994223\n",
      "Iteration 62, loss = 0.06888399\n",
      "Iteration 63, loss = 0.06810906\n",
      "Iteration 64, loss = 0.07029422\n",
      "Iteration 65, loss = 0.06570672\n",
      "Iteration 66, loss = 0.06655373\n",
      "Iteration 67, loss = 0.06585801\n",
      "Iteration 68, loss = 0.06629720\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8168 1044]\n",
      " [1346  661]]\n",
      "0.387683284457 0.329347284504 0.356142241379\n",
      "['BernoulliNB', 0.3319824383654171, 0.489785749875436, 0.39573268921095006, 0.7324182190926107]\n",
      "['RandomForestClassifier', 0.6887966804979253, 0.08271051320378675, 0.14768683274021352, 0.8292182904002139]\n",
      "['BaggingClassifier', 0.49304174950298213, 0.12356751370204284, 0.19760956175298805, 0.8204831090114983]\n",
      "['ExtraTreesClassifier', 0.5467980295566502, 0.11061285500747384, 0.1840033153750518, 0.8244941616899902]\n",
      "['DecisionTreeClassifier', 0.25927861996863566, 0.2471350274040857, 0.2530612244897959, 0.7390141723861307]\n",
      "['CalibratedClassifierCV', 0.5138888888888888, 0.09217737917289487, 0.15631601182931984, 0.8219983955789286]\n",
      "['SGDClassifier', 0.4494195688225539, 0.13502740408570005, 0.20766283524904214, 0.8156698457973082]\n",
      "['KNeighborsClassifier', 0.3536784741144414, 0.32336821126058796, 0.33784487246225925, 0.773241821909261]\n",
      "['MLPClassifier', 0.387683284457478, 0.32934728450423517, 0.35614224137931033, 0.7869685355200998]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[7514 1698]\n",
      " [1213  794]]\n",
      "0.318619582665 0.395615346288 0.352967326072\n",
      "Training  RandomForestClassifier\n",
      "[[9144   68]\n",
      " [1919   88]]\n",
      "0.564102564103 0.0438465371201 0.081368469718\n",
      "Training  BaggingClassifier\n",
      "[[9107  105]\n",
      " [1868  139]]\n",
      "0.569672131148 0.0692575984056 0.123500666371\n",
      "Training  ExtraTreesClassifier\n",
      "[[9032  180]\n",
      " [1865  142]]\n",
      "0.44099378882 0.0707523667165 0.121940747102\n",
      "Training  DecisionTreeClassifier\n",
      "[[7707 1505]\n",
      " [1455  552]]\n",
      "0.268351968887 0.275037369208 0.271653543307\n",
      "Training  CalibratedClassifierCV\n",
      "[[8969  243]\n",
      " [1875  132]]\n",
      "0.352 0.0657698056801 0.110831234257\n",
      "Training  SGDClassifier\n",
      "[[8955  257]\n",
      " [1889  118]]\n",
      "0.314666666667 0.0587942202292 0.0990764063812\n",
      "Training  KNeighborsClassifier\n",
      "[[8160 1052]\n",
      " [1370  637]]\n",
      "0.377146240379 0.317389138017 0.344696969697\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.33603187\n",
      "Iteration 2, loss = 0.29617287\n",
      "Iteration 3, loss = 0.27917034\n",
      "Iteration 4, loss = 0.26075980\n",
      "Iteration 5, loss = 0.24765263\n",
      "Iteration 6, loss = 0.23229375\n",
      "Iteration 7, loss = 0.22087892\n",
      "Iteration 8, loss = 0.20908541\n",
      "Iteration 9, loss = 0.19829846\n",
      "Iteration 10, loss = 0.18841083\n",
      "Iteration 11, loss = 0.17948681\n",
      "Iteration 12, loss = 0.17175647\n",
      "Iteration 13, loss = 0.16239726\n",
      "Iteration 14, loss = 0.15547540\n",
      "Iteration 15, loss = 0.14949119\n",
      "Iteration 16, loss = 0.14237745\n",
      "Iteration 17, loss = 0.13510767\n",
      "Iteration 18, loss = 0.13083946\n",
      "Iteration 19, loss = 0.12628927\n",
      "Iteration 20, loss = 0.11961731\n",
      "Iteration 21, loss = 0.12018873\n",
      "Iteration 22, loss = 0.11196170\n",
      "Iteration 23, loss = 0.10708873\n",
      "Iteration 24, loss = 0.10220510\n",
      "Iteration 25, loss = 0.09765469\n",
      "Iteration 26, loss = 0.09498492\n",
      "Iteration 27, loss = 0.09187694\n",
      "Iteration 28, loss = 0.09339028\n",
      "Iteration 29, loss = 0.08790153\n",
      "Iteration 30, loss = 0.08374388\n",
      "Iteration 31, loss = 0.08188102\n",
      "Iteration 32, loss = 0.08116515\n",
      "Iteration 33, loss = 0.07722602\n",
      "Iteration 34, loss = 0.07454299\n",
      "Iteration 35, loss = 0.07408459\n",
      "Iteration 36, loss = 0.07219870\n",
      "Iteration 37, loss = 0.06886900\n",
      "Iteration 38, loss = 0.06959688\n",
      "Iteration 39, loss = 0.06952890\n",
      "Iteration 40, loss = 0.06496312\n",
      "Iteration 41, loss = 0.06302407\n",
      "Iteration 42, loss = 0.06112252\n",
      "Iteration 43, loss = 0.06201518\n",
      "Iteration 44, loss = 0.06546172\n",
      "Iteration 45, loss = 0.05879017\n",
      "Iteration 46, loss = 0.05975739\n",
      "Iteration 47, loss = 0.06055579\n",
      "Iteration 48, loss = 0.06068464\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[7903 1309]\n",
      " [1238  769]]\n",
      "0.370067372474 0.383158943697 0.376499388005\n",
      "['BernoulliNB', 0.3186195826645265, 0.39561534628799205, 0.35296732607246056, 0.7405294589535609]\n",
      "['RandomForestClassifier', 0.5641025641025641, 0.04384653712007972, 0.08136846971798428, 0.8228897406185934]\n",
      "['BaggingClassifier', 0.569672131147541, 0.06925759840558046, 0.12350066637050199, 0.8241376236741242]\n",
      "['ExtraTreesClassifier', 0.4409937888198758, 0.07075236671649228, 0.12194074710176042, 0.8177199393885373]\n",
      "['DecisionTreeClassifier', 0.26835196888672824, 0.2750373692077728, 0.27165354330708663, 0.7361618682592032]\n",
      "['CalibratedClassifierCV', 0.352, 0.06576980568011959, 0.11083123425692695, 0.8112131205989839]\n",
      "['SGDClassifier', 0.31466666666666665, 0.05879422022919781, 0.09907640638119229, 0.8087173544879223]\n",
      "['KNeighborsClassifier', 0.3771462403789224, 0.3173891380169407, 0.34469696969696967, 0.7841162313931723]\n",
      "['MLPClassifier', 0.3700673724735322, 0.3831589436970603, 0.3764993880048959, 0.7729744183973616]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'csv_2_groupBR12_token'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. next word2vec model is the one where BR1 and BR2 are marked and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[6763 2449]\n",
      " [ 908 1099]]\n",
      "0.309751972943 0.547583457897 0.395679567957\n",
      "Training  RandomForestClassifier\n",
      "[[9155   57]\n",
      " [1872  135]]\n",
      "0.703125 0.067264573991 0.12278308322\n",
      "Training  BaggingClassifier\n",
      "[[9096  116]\n",
      " [1844  163]]\n",
      "0.584229390681 0.0812157448929 0.142607174103\n",
      "Training  ExtraTreesClassifier\n",
      "[[9030  182]\n",
      " [1862  145]]\n",
      "0.443425076453 0.0722471350274 0.124250214225\n",
      "Training  DecisionTreeClassifier\n",
      "[[7709 1503]\n",
      " [1421  586]]\n",
      "0.280516993777 0.291978076731 0.2861328125\n",
      "Training  CalibratedClassifierCV\n",
      "[[8979  233]\n",
      " [1852  155]]\n",
      "0.399484536082 0.0772296960638 0.129436325678\n",
      "Training  SGDClassifier\n",
      "[[8851  361]\n",
      " [1785  222]]\n",
      "0.380789022298 0.110612855007 0.171428571429\n",
      "Training  KNeighborsClassifier\n",
      "[[8185 1027]\n",
      " [1422  585]]\n",
      "0.362903225806 0.291479820628 0.323293727549\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.35546664\n",
      "Iteration 2, loss = 0.30747280\n",
      "Iteration 3, loss = 0.28913525\n",
      "Iteration 4, loss = 0.27500203\n",
      "Iteration 5, loss = 0.26074496\n",
      "Iteration 6, loss = 0.24719048\n",
      "Iteration 7, loss = 0.23471745\n",
      "Iteration 8, loss = 0.22518524\n",
      "Iteration 9, loss = 0.21448203\n",
      "Iteration 10, loss = 0.20331092\n",
      "Iteration 11, loss = 0.19966398\n",
      "Iteration 12, loss = 0.18779229\n",
      "Iteration 13, loss = 0.17769221\n",
      "Iteration 14, loss = 0.17368114\n",
      "Iteration 15, loss = 0.16525429\n",
      "Iteration 16, loss = 0.15875573\n",
      "Iteration 17, loss = 0.15149074\n",
      "Iteration 18, loss = 0.14805485\n",
      "Iteration 19, loss = 0.13991116\n",
      "Iteration 20, loss = 0.13664207\n",
      "Iteration 21, loss = 0.12852949\n",
      "Iteration 22, loss = 0.12739052\n",
      "Iteration 23, loss = 0.12147735\n",
      "Iteration 24, loss = 0.11659805\n",
      "Iteration 25, loss = 0.11565208\n",
      "Iteration 26, loss = 0.10924864\n",
      "Iteration 27, loss = 0.10481363\n",
      "Iteration 28, loss = 0.10148701\n",
      "Iteration 29, loss = 0.09840750\n",
      "Iteration 30, loss = 0.09495311\n",
      "Iteration 31, loss = 0.09442780\n",
      "Iteration 32, loss = 0.09051196\n",
      "Iteration 33, loss = 0.08669777\n",
      "Iteration 34, loss = 0.08366416\n",
      "Iteration 35, loss = 0.08423974\n",
      "Iteration 36, loss = 0.07807226\n",
      "Iteration 37, loss = 0.07920883\n",
      "Iteration 38, loss = 0.07637929\n",
      "Iteration 39, loss = 0.07458222\n",
      "Iteration 40, loss = 0.07065308\n",
      "Iteration 41, loss = 0.06720029\n",
      "Iteration 42, loss = 0.07056763\n",
      "Iteration 43, loss = 0.06586751\n",
      "Iteration 44, loss = 0.06450377\n",
      "Iteration 45, loss = 0.06331818\n",
      "Iteration 46, loss = 0.06158858\n",
      "Iteration 47, loss = 0.06458366\n",
      "Iteration 48, loss = 0.06570491\n",
      "Iteration 49, loss = 0.05918482\n",
      "Iteration 50, loss = 0.05838418\n",
      "Iteration 51, loss = 0.06132233\n",
      "Iteration 52, loss = 0.05597224\n",
      "Iteration 53, loss = 0.05660085\n",
      "Iteration 54, loss = 0.05368478\n",
      "Iteration 55, loss = 0.05644859\n",
      "Iteration 56, loss = 0.05414100\n",
      "Iteration 57, loss = 0.05164828\n",
      "Iteration 58, loss = 0.05129620\n",
      "Iteration 59, loss = 0.04988237\n",
      "Iteration 60, loss = 0.04949914\n",
      "Iteration 61, loss = 0.05258854\n",
      "Iteration 62, loss = 0.04898972\n",
      "Iteration 63, loss = 0.05102492\n",
      "Iteration 64, loss = 0.04814482\n",
      "Iteration 65, loss = 0.04749136\n",
      "Iteration 66, loss = 0.04898808\n",
      "Iteration 67, loss = 0.04894372\n",
      "Iteration 68, loss = 0.05057456\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[8092 1120]\n",
      " [1278  729]]\n",
      "0.394267171444 0.363228699552 0.378112033195\n",
      "['BernoulliNB', 0.30975197294250284, 0.5475834578973593, 0.3956795679567957, 0.7007754701845085]\n",
      "['RandomForestClassifier', 0.703125, 0.06726457399103139, 0.1227830832196453, 0.8280595418486496]\n",
      "['BaggingClassifier', 0.5842293906810035, 0.08121574489287493, 0.1426071741032371, 0.8252963722256885]\n",
      "['ExtraTreesClassifier', 0.4434250764525994, 0.07224713502740408, 0.1242502142245073, 0.8178090738925038]\n",
      "['DecisionTreeClassifier', 0.28051699377692674, 0.29197807673143994, 0.28613281249999994, 0.7393707104019966]\n",
      "['CalibratedClassifierCV', 0.39948453608247425, 0.07722969606377678, 0.12943632567849686, 0.8141545592298779]\n",
      "['SGDClassifier', 0.38078902229845624, 0.11061285500747384, 0.17142857142857143, 0.8087173544879223]\n",
      "['KNeighborsClassifier', 0.3629032258064516, 0.2914798206278027, 0.32329372754904667, 0.7817095997860772]\n",
      "['MLPClassifier', 0.3942671714440238, 0.3632286995515695, 0.3781120331950208, 0.786255459488368]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_trans = Phrases(trainFSen)\n",
    "\n",
    "modelName = 'csv_2_groupBR12_bigramToken'\n",
    "train_vect, test_vect = buildW2V(modelName,bigram_trans[trainMid], bigram_trans[testMid])\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------\n",
    "# Fin\n",
    "For the final time we take another csv(4) file this time. We had duplicates after grouping of BR's. This file is without any duplicate. Senteces were preprocessed as per need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fileName = \"WhiteText(Fin).csv\"\n",
    "trainSet = pd.read_csv('data/'+fileName,delimiter=\"|\")\n",
    "\n",
    "fileEval = \"WhiteTextUnseenEval(Fin).csv\"\n",
    "testSet = pd.read_csv('data/'+fileEval,delimiter=\"|\")\n",
    "\n",
    "trainSet.to_csv('Train(Fin)',sep=\"|\")\n",
    "testSet.to_csv('Test(Fin)',sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16547\n",
      "8475\n"
     ]
    }
   ],
   "source": [
    "trainData = pd.read_csv('Train(Fin)',delimiter='|')\n",
    "trainSen = trainData['sentence']\n",
    "trainLab = trainData['connection']\n",
    "trainLen = len(trainSen)\n",
    "print trainLen\n",
    "\n",
    "testData = pd.read_csv('Test(Fin)',delimiter='|')\n",
    "testSen = testData['sentence']\n",
    "testLab = testData['connection']\n",
    "testLen = len(testSen)\n",
    "print testLen\n",
    "\n",
    "\n",
    "def formatSen(sentence):\n",
    "    sentence = re.sub(\"\\s(the|The)\\s\",\" \",sentence)\n",
    "    sentence = re.sub(\"^(the|The)\",\"\",sentence)\n",
    "    sentence = re.sub(\"(nucleus)\",\" \",sentence)\n",
    "    sentence = re.sub(r\",\",\" \", sentence)\n",
    "    sentence = re.sub(\"\\([0-9]\\)\",\".\",sentence)\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFSen = []\n",
    "for i in range(0,trainLen):\n",
    "    trainFSen.append(formatSen(trainSen[i]))\n",
    "    \n",
    "testFSen = []\n",
    "for i in range(0,testLen):\n",
    "    testFSen.append(formatSen(testSen[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  BernoulliNB\n",
      "[[6647   80]\n",
      " [1632  116]]\n",
      "0.591836734694 0.0663615560641 0.119341563786\n",
      "Training  RandomForestClassifier\n",
      "[[6623  104]\n",
      " [1446  302]]\n",
      "0.743842364532 0.172768878719 0.280408542247\n",
      "Training  BaggingClassifier\n",
      "[[5959  768]\n",
      " [1042  706]]\n",
      "0.478968792402 0.403890160183 0.438237119801\n",
      "Training  ExtraTreesClassifier\n",
      "[[6586  141]\n",
      " [1473  275]]\n",
      "0.661057692308 0.157322654462 0.25415896488\n",
      "Training  DecisionTreeClassifier\n",
      "[[5471 1256]\n",
      " [ 977  771]]\n",
      "0.380365071534 0.441075514874 0.408476821192\n",
      "Training  CalibratedClassifierCV\n",
      "[[6513  214]\n",
      " [1293  455]]\n",
      "0.680119581465 0.260297482838 0.376499793132\n",
      "Training  SGDClassifier\n",
      "[[6634   93]\n",
      " [1433  315]]\n",
      "0.772058823529 0.180205949657 0.292207792208\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.42868549\n",
      "Iteration 2, loss = 0.26328427\n",
      "Iteration 3, loss = 0.16206960\n",
      "Iteration 4, loss = 0.11424762\n",
      "Iteration 5, loss = 0.08879925\n",
      "Iteration 6, loss = 0.07187474\n",
      "Iteration 7, loss = 0.05938229\n",
      "Iteration 8, loss = 0.05084608\n",
      "Iteration 9, loss = 0.04416215\n",
      "Iteration 10, loss = 0.03942339\n",
      "Iteration 11, loss = 0.03652985\n",
      "Iteration 12, loss = 0.03347254\n",
      "Iteration 13, loss = 0.03106477\n",
      "Iteration 14, loss = 0.02772484\n",
      "Iteration 15, loss = 0.02623833\n",
      "Iteration 16, loss = 0.02374664\n",
      "Iteration 17, loss = 0.02575761\n",
      "Iteration 18, loss = 0.02218715\n",
      "Iteration 19, loss = 0.01999372\n",
      "Iteration 20, loss = 0.01899144\n",
      "Iteration 21, loss = 0.01905953\n",
      "Iteration 22, loss = 0.01754672\n",
      "Iteration 23, loss = 0.01611727\n",
      "Iteration 24, loss = 0.01653318\n",
      "Iteration 25, loss = 0.01888874\n",
      "Iteration 26, loss = 0.01963721\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[6242  485]\n",
      " [1104  644]]\n",
      "0.570416297609 0.368421052632 0.447688564477\n",
      "['BernoulliNB', 0.5918367346938775, 0.06636155606407322, 0.11934156378600821, 0.7979941002949853]\n",
      "['RandomForestClassifier', 0.7438423645320197, 0.17276887871853547, 0.2804085422469824, 0.8171091445427728]\n",
      "['BaggingClassifier', 0.4789687924016282, 0.40389016018306634, 0.4382371198013656, 0.7864306784660767]\n",
      "['ExtraTreesClassifier', 0.6610576923076923, 0.15732265446224256, 0.2541589648798521, 0.8095575221238938]\n",
      "['DecisionTreeClassifier', 0.3803650715342871, 0.44107551487414187, 0.408476821192053, 0.736519174041298]\n",
      "['CalibratedClassifierCV', 0.680119581464873, 0.2602974828375286, 0.37649979313198184, 0.8221828908554573]\n",
      "['SGDClassifier', 0.7720588235294118, 0.18020594965675057, 0.2922077922077922, 0.8199410029498525]\n",
      "['MLPClassifier', 0.5704162976085031, 0.3684210526315789, 0.44768856447688565, 0.8125073746312684]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_vect, test_vect = convertTfidf(trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16547"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainFSen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[4925 1802]\n",
      " [ 958  790]]\n",
      "0.304783950617 0.451945080092 0.364055299539\n",
      "Training  RandomForestClassifier\n",
      "[[6634   93]\n",
      " [1633  115]]\n",
      "0.552884615385 0.0657894736842 0.117586912065\n",
      "Training  BaggingClassifier\n",
      "[[6620  107]\n",
      " [1604  144]]\n",
      "0.573705179283 0.0823798627002 0.144072036018\n",
      "Training  ExtraTreesClassifier\n",
      "[[6490  237]\n",
      " [1573  175]]\n",
      "0.424757281553 0.100114416476 0.162037037037\n",
      "Training  DecisionTreeClassifier\n",
      "[[5284 1443]\n",
      " [1198  550]]\n",
      "0.275965880582 0.314645308924 0.294039026998\n",
      "Training  CalibratedClassifierCV\n",
      "[[6670   57]\n",
      " [1673   75]]\n",
      "0.568181818182 0.0429061784897 0.0797872340426\n",
      "Training  SGDClassifier\n",
      "[[6692   35]\n",
      " [1717   31]]\n",
      "0.469696969697 0.0177345537757 0.0341786108049\n",
      "Training  KNeighborsClassifier\n",
      "[[5627 1100]\n",
      " [1261  487]]\n",
      "0.306868304978 0.278604118993 0.292053973013\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.42658315\n",
      "Iteration 2, loss = 0.38934386\n",
      "Iteration 3, loss = 0.37827551\n",
      "Iteration 4, loss = 0.37150356\n",
      "Iteration 5, loss = 0.36461264\n",
      "Iteration 6, loss = 0.35673608\n",
      "Iteration 7, loss = 0.35275616\n",
      "Iteration 8, loss = 0.34854012\n",
      "Iteration 9, loss = 0.34542536\n",
      "Iteration 10, loss = 0.34088178\n",
      "Iteration 11, loss = 0.33620928\n",
      "Iteration 12, loss = 0.33266224\n",
      "Iteration 13, loss = 0.32883980\n",
      "Iteration 14, loss = 0.32530539\n",
      "Iteration 15, loss = 0.32280803\n",
      "Iteration 16, loss = 0.32177265\n",
      "Iteration 17, loss = 0.31773300\n",
      "Iteration 18, loss = 0.31551854\n",
      "Iteration 19, loss = 0.31471167\n",
      "Iteration 20, loss = 0.31228258\n",
      "Iteration 21, loss = 0.31211214\n",
      "Iteration 22, loss = 0.30752813\n",
      "Iteration 23, loss = 0.30580295\n",
      "Iteration 24, loss = 0.30508992\n",
      "Iteration 25, loss = 0.30146177\n",
      "Iteration 26, loss = 0.30048490\n",
      "Iteration 27, loss = 0.29812765\n",
      "Iteration 28, loss = 0.29812692\n",
      "Iteration 29, loss = 0.29623845\n",
      "Iteration 30, loss = 0.29613107\n",
      "Iteration 31, loss = 0.29338088\n",
      "Iteration 32, loss = 0.29175852\n",
      "Iteration 33, loss = 0.29070752\n",
      "Iteration 34, loss = 0.29179139\n",
      "Iteration 35, loss = 0.29053821\n",
      "Iteration 36, loss = 0.28770329\n",
      "Iteration 37, loss = 0.28659558\n",
      "Iteration 38, loss = 0.28787261\n",
      "Iteration 39, loss = 0.28580412\n",
      "Iteration 40, loss = 0.28511335\n",
      "Iteration 41, loss = 0.28457314\n",
      "Iteration 42, loss = 0.28427926\n",
      "Iteration 43, loss = 0.28366487\n",
      "Iteration 44, loss = 0.28062982\n",
      "Iteration 45, loss = 0.28206711\n",
      "Iteration 46, loss = 0.28131134\n",
      "Iteration 47, loss = 0.27993557\n",
      "Iteration 48, loss = 0.27980217\n",
      "Iteration 49, loss = 0.27969547\n",
      "Iteration 50, loss = 0.27752043\n",
      "Iteration 51, loss = 0.27808437\n",
      "Iteration 52, loss = 0.27964285\n",
      "Iteration 53, loss = 0.27746613\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[6199  528]\n",
      " [1369  379]]\n",
      "0.417861080485 0.216819221968 0.28549905838\n",
      "['BernoulliNB', 0.30478395061728397, 0.45194508009153317, 0.3640552995391705, 0.6743362831858407]\n",
      "['RandomForestClassifier', 0.5528846153846154, 0.06578947368421052, 0.11758691206543967, 0.7963421828908555]\n",
      "['BaggingClassifier', 0.5737051792828686, 0.08237986270022883, 0.14407203601800903, 0.7981120943952802]\n",
      "['ExtraTreesClassifier', 0.42475728155339804, 0.10011441647597254, 0.16203703703703703, 0.7864306784660767]\n",
      "['DecisionTreeClassifier', 0.27596588058203714, 0.3146453089244851, 0.29403902699812884, 0.6883775811209439]\n",
      "['CalibratedClassifierCV', 0.5681818181818182, 0.04290617848970252, 0.0797872340425532, 0.7958702064896755]\n",
      "['SGDClassifier', 0.4696969696969697, 0.017734553775743706, 0.034178610804851156, 0.7932743362831859]\n",
      "['KNeighborsClassifier', 0.3068683049779458, 0.27860411899313503, 0.29205397301349323, 0.7214159292035398]\n",
      "['MLPClassifier', 0.4178610804851158, 0.21681922196796338, 0.2854990583804143, 0.776165191740413]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[5311 1416]\n",
      " [1066  682]]\n",
      "0.325071496663 0.390160183066 0.354654186167\n",
      "Training  RandomForestClassifier\n",
      "[[6635   92]\n",
      " [1629  119]]\n",
      "0.563981042654 0.0680778032037 0.121490556406\n",
      "Training  BaggingClassifier\n",
      "[[6602  125]\n",
      " [1602  146]]\n",
      "0.538745387454 0.08352402746 0.144626052501\n",
      "Training  ExtraTreesClassifier\n",
      "[[6516  211]\n",
      " [1606  142]]\n",
      "0.402266288952 0.0812356979405 0.135173726797\n",
      "Training  DecisionTreeClassifier\n",
      "[[5642 1085]\n",
      " [1355  393]]\n",
      "0.265899864682 0.224828375286 0.243645381277\n",
      "Training  CalibratedClassifierCV\n",
      "[[6660   67]\n",
      " [1675   73]]\n",
      "0.521428571429 0.04176201373 0.0773305084746\n",
      "Training  SGDClassifier\n",
      "[[3093 3634]\n",
      " [ 509 1239]]\n",
      "0.254258157193 0.70881006865 0.374263706389\n",
      "Training  KNeighborsClassifier\n",
      "[[5284 1443]\n",
      " [1178  570]]\n",
      "0.283159463487 0.326086956522 0.303110874767\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.43922314\n",
      "Iteration 2, loss = 0.39524567\n",
      "Iteration 3, loss = 0.38395140\n",
      "Iteration 4, loss = 0.37637043\n",
      "Iteration 5, loss = 0.36931243\n",
      "Iteration 6, loss = 0.36423503\n",
      "Iteration 7, loss = 0.35955722\n",
      "Iteration 8, loss = 0.35430149\n",
      "Iteration 9, loss = 0.34898447\n",
      "Iteration 10, loss = 0.34335668\n",
      "Iteration 11, loss = 0.34196066\n",
      "Iteration 12, loss = 0.33648022\n",
      "Iteration 13, loss = 0.33443515\n",
      "Iteration 14, loss = 0.32994476\n",
      "Iteration 15, loss = 0.32789914\n",
      "Iteration 16, loss = 0.32373708\n",
      "Iteration 17, loss = 0.32247257\n",
      "Iteration 18, loss = 0.31722083\n",
      "Iteration 19, loss = 0.31401003\n",
      "Iteration 20, loss = 0.31309243\n",
      "Iteration 21, loss = 0.31170960\n",
      "Iteration 22, loss = 0.30808388\n",
      "Iteration 23, loss = 0.30828305\n",
      "Iteration 24, loss = 0.30736595\n",
      "Iteration 25, loss = 0.30371464\n",
      "Iteration 26, loss = 0.30258024\n",
      "Iteration 27, loss = 0.30159933\n",
      "Iteration 28, loss = 0.29966144\n",
      "Iteration 29, loss = 0.29835600\n",
      "Iteration 30, loss = 0.29593318\n",
      "Iteration 31, loss = 0.29538455\n",
      "Iteration 32, loss = 0.29340921\n",
      "Iteration 33, loss = 0.29678959\n",
      "Iteration 34, loss = 0.29287112\n",
      "Iteration 35, loss = 0.29157147\n",
      "Iteration 36, loss = 0.29189819\n",
      "Iteration 37, loss = 0.28962907\n",
      "Iteration 38, loss = 0.29078361\n",
      "Iteration 39, loss = 0.28866003\n",
      "Iteration 40, loss = 0.28675080\n",
      "Iteration 41, loss = 0.28690757\n",
      "Iteration 42, loss = 0.28713351\n",
      "Iteration 43, loss = 0.28579024\n",
      "Iteration 44, loss = 0.28518949\n",
      "Iteration 45, loss = 0.28533124\n",
      "Iteration 46, loss = 0.28295932\n",
      "Iteration 47, loss = 0.28314043\n",
      "Iteration 48, loss = 0.28188816\n",
      "Iteration 49, loss = 0.28265324\n",
      "Iteration 50, loss = 0.27980539\n",
      "Iteration 51, loss = 0.28038537\n",
      "Iteration 52, loss = 0.28295224\n",
      "Iteration 53, loss = 0.27958450\n",
      "Iteration 54, loss = 0.27944004\n",
      "Iteration 55, loss = 0.27965425\n",
      "Iteration 56, loss = 0.27797055\n",
      "Iteration 57, loss = 0.27983258\n",
      "Iteration 58, loss = 0.27706852\n",
      "Iteration 59, loss = 0.27678708\n",
      "Iteration 60, loss = 0.27746308\n",
      "Iteration 61, loss = 0.27738979\n",
      "Iteration 62, loss = 0.27608042\n",
      "Iteration 63, loss = 0.27544602\n",
      "Iteration 64, loss = 0.27644028\n",
      "Iteration 65, loss = 0.27582742\n",
      "Iteration 66, loss = 0.27594236\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[6379  348]\n",
      " [1458  290]]\n",
      "0.454545454545 0.16590389016 0.24308466052\n",
      "['BernoulliNB', 0.32507149666348906, 0.3901601830663616, 0.35465418616744676, 0.7071386430678466]\n",
      "['RandomForestClassifier', 0.5639810426540285, 0.06807780320366133, 0.12149055640632977, 0.7969321533923304]\n",
      "['BaggingClassifier', 0.5387453874538746, 0.08352402745995423, 0.14462605250123822, 0.7962241887905604]\n",
      "['ExtraTreesClassifier', 0.40226628895184136, 0.08123569794050343, 0.13517372679676345, 0.7856047197640118]\n",
      "['DecisionTreeClassifier', 0.2658998646820027, 0.2248283752860412, 0.24364538127712335, 0.712094395280236]\n",
      "['CalibratedClassifierCV', 0.5214285714285715, 0.041762013729977114, 0.07733050847457626, 0.7944542772861357]\n",
      "['SGDClassifier', 0.25425815719269446, 0.7088100686498856, 0.3742637063887631, 0.5111504424778761]\n",
      "['KNeighborsClassifier', 0.28315946348733234, 0.32608695652173914, 0.3031108747673491, 0.6907374631268437]\n",
      "['MLPClassifier', 0.45454545454545453, 0.16590389016018306, 0.24308466051969824, 0.7869026548672566]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainFSen, testFSen)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[5923  804]\n",
      " [1387  361]]\n",
      "0.309871244635 0.20652173913 0.247854445589\n",
      "Training  RandomForestClassifier\n",
      "[[6702   25]\n",
      " [1685   63]]\n",
      "0.715909090909 0.0360411899314 0.0686274509804\n",
      "Training  BaggingClassifier\n",
      "[[6689   38]\n",
      " [1661   87]]\n",
      "0.696 0.0497711670481 0.0928990923652\n",
      "Training  ExtraTreesClassifier\n",
      "[[6579  148]\n",
      " [1651   97]]\n",
      "0.395918367347 0.0554919908467 0.0973406924235\n",
      "Training  DecisionTreeClassifier\n",
      "[[5774  953]\n",
      " [1367  381]]\n",
      "0.285607196402 0.217963386728 0.247242050616\n",
      "Training  CalibratedClassifierCV\n",
      "[[6660   67]\n",
      " [1659   89]]\n",
      "0.570512820513 0.0509153318078 0.093487394958\n",
      "Training  SGDClassifier\n",
      "[[6381  346]\n",
      " [1484  264]]\n",
      "0.432786885246 0.151029748284 0.223918575064\n",
      "Training  KNeighborsClassifier\n",
      "[[5886  841]\n",
      " [1364  384]]\n",
      "0.313469387755 0.219679633867 0.258324924319\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.43982131\n",
      "Iteration 2, loss = 0.40195343\n",
      "Iteration 3, loss = 0.39163010\n",
      "Iteration 4, loss = 0.38470989\n",
      "Iteration 5, loss = 0.37682092\n",
      "Iteration 6, loss = 0.37377945\n",
      "Iteration 7, loss = 0.36874699\n",
      "Iteration 8, loss = 0.36447677\n",
      "Iteration 9, loss = 0.36131582\n",
      "Iteration 10, loss = 0.35360602\n",
      "Iteration 11, loss = 0.35065333\n",
      "Iteration 12, loss = 0.34603989\n",
      "Iteration 13, loss = 0.34466853\n",
      "Iteration 14, loss = 0.33867441\n",
      "Iteration 15, loss = 0.33595228\n",
      "Iteration 16, loss = 0.33195131\n",
      "Iteration 17, loss = 0.32971440\n",
      "Iteration 18, loss = 0.32636599\n",
      "Iteration 19, loss = 0.32200773\n",
      "Iteration 20, loss = 0.31903282\n",
      "Iteration 21, loss = 0.31550088\n",
      "Iteration 22, loss = 0.31365290\n",
      "Iteration 23, loss = 0.31051383\n",
      "Iteration 24, loss = 0.30794151\n",
      "Iteration 25, loss = 0.30795017\n",
      "Iteration 26, loss = 0.30861472\n",
      "Iteration 27, loss = 0.30382244\n",
      "Iteration 28, loss = 0.30048369\n",
      "Iteration 29, loss = 0.29808792\n",
      "Iteration 30, loss = 0.29680143\n",
      "Iteration 31, loss = 0.29448484\n",
      "Iteration 32, loss = 0.29519545\n",
      "Iteration 33, loss = 0.29489279\n",
      "Iteration 34, loss = 0.29001638\n",
      "Iteration 35, loss = 0.28968130\n",
      "Iteration 36, loss = 0.28671393\n",
      "Iteration 37, loss = 0.28854737\n",
      "Iteration 38, loss = 0.28652845\n",
      "Iteration 39, loss = 0.28764700\n",
      "Iteration 40, loss = 0.28359133\n",
      "Iteration 41, loss = 0.28141783\n",
      "Iteration 42, loss = 0.28043790\n",
      "Iteration 43, loss = 0.28134597\n",
      "Iteration 44, loss = 0.27772510\n",
      "Iteration 45, loss = 0.27772217\n",
      "Iteration 46, loss = 0.27935451\n",
      "Iteration 47, loss = 0.27627705\n",
      "Iteration 48, loss = 0.27913892\n",
      "Iteration 49, loss = 0.27605532\n",
      "Iteration 50, loss = 0.27573655\n",
      "Iteration 51, loss = 0.27594176\n",
      "Iteration 52, loss = 0.27637595\n",
      "Iteration 53, loss = 0.27352006\n",
      "Iteration 54, loss = 0.27735163\n",
      "Iteration 55, loss = 0.27236384\n",
      "Iteration 56, loss = 0.27253314\n",
      "Iteration 57, loss = 0.27211156\n",
      "Iteration 58, loss = 0.26958709\n",
      "Iteration 59, loss = 0.27256533\n",
      "Iteration 60, loss = 0.27021133\n",
      "Iteration 61, loss = 0.26955838\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[6221  506]\n",
      " [1345  403]]\n",
      "0.443344334433 0.230549199085 0.303349642454\n",
      "['BernoulliNB', 0.3098712446351931, 0.20652173913043478, 0.24785444558874012, 0.7414749262536873]\n",
      "['RandomForestClassifier', 0.7159090909090909, 0.036041189931350116, 0.06862745098039216, 0.7982300884955752]\n",
      "['BaggingClassifier', 0.696, 0.04977116704805492, 0.09289909236518953, 0.79952802359882]\n",
      "['ExtraTreesClassifier', 0.39591836734693875, 0.055491990846681924, 0.09734069242348217, 0.7877286135693216]\n",
      "['DecisionTreeClassifier', 0.2856071964017991, 0.21796338672768878, 0.2472420506164828, 0.7262536873156342]\n",
      "['CalibratedClassifierCV', 0.5705128205128205, 0.05091533180778032, 0.0934873949579832, 0.7963421828908555]\n",
      "['SGDClassifier', 0.43278688524590164, 0.15102974828375287, 0.22391857506361323, 0.784070796460177]\n",
      "['KNeighborsClassifier', 0.313469387755102, 0.21967963386727687, 0.25832492431886983, 0.7398230088495575]\n",
      "['MLPClassifier', 0.44334433443344334, 0.2305491990846682, 0.3033496424538954, 0.7815929203539823]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_trans = Phrases(trainFSen)\n",
    "\n",
    "modelName = 'csv_Fin_bigramToken'\n",
    "train_vect, test_vect = buildW2V(modelName,bigram_trans[trainFSen], bigram_trans[testFSen])\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Middle Sentences taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['examined optic ( br1 so ) intermediate gray ( br2 sgi )',\n",
       " 'examined optic ( br1 so ) intermediate gray ( br sgi ) intermediate white ( br sai ) and deep gray ( br2 sgp )',\n",
       " 'intermediate gray ( br1 sgi ) intermediate white ( br sai ) and deep gray ( br2 sgp )',\n",
       " 'examined optic ( br1 so ) intermediate gray ( br sgi ) intermediate white ( br2 sai )',\n",
       " 'intermediate gray ( br1 sgi ) intermediate white ( br2 sai )']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMid = breakSen(trainFSen)\n",
    "testMid = breakSen(testFSen)\n",
    "trainMid[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tf-idf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  BernoulliNB\n",
      "[[6638   89]\n",
      " [1634  114]]\n",
      "0.56157635468 0.0652173913043 0.116863147104\n",
      "Training  RandomForestClassifier\n",
      "[[6578  149]\n",
      " [1336  412]]\n",
      "0.73440285205 0.235697940503 0.356864443482\n",
      "Training  BaggingClassifier\n",
      "[[6303  424]\n",
      " [1103  645]]\n",
      "0.603367633302 0.368993135011 0.457933972311\n",
      "Training  ExtraTreesClassifier\n",
      "[[6580  147]\n",
      " [1376  372]]\n",
      "0.71676300578 0.212814645309 0.328187031319\n",
      "Training  DecisionTreeClassifier\n",
      "[[5835  892]\n",
      " [1032  716]]\n",
      "0.445273631841 0.409610983982 0.426698450536\n",
      "Training  CalibratedClassifierCV\n",
      "[[6363  364]\n",
      " [1141  607]]\n",
      "0.625128733265 0.347254004577 0.446487679294\n",
      "Training  SGDClassifier\n",
      "[[6447  280]\n",
      " [1236  512]]\n",
      "0.646464646465 0.29290617849 0.403149606299\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.41572883\n",
      "Iteration 2, loss = 0.15999117\n",
      "Iteration 3, loss = 0.06183704\n",
      "Iteration 4, loss = 0.03497055\n",
      "Iteration 5, loss = 0.02390761\n",
      "Iteration 6, loss = 0.01634154\n",
      "Iteration 7, loss = 0.01267299\n",
      "Iteration 8, loss = 0.00971220\n",
      "Iteration 9, loss = 0.01007198\n",
      "Iteration 10, loss = 0.00742034\n",
      "Iteration 11, loss = 0.00658061\n",
      "Iteration 12, loss = 0.00515213\n",
      "Iteration 13, loss = 0.00492012\n",
      "Iteration 14, loss = 0.00526376\n",
      "Iteration 15, loss = 0.00377123\n",
      "Iteration 16, loss = 0.00376144\n",
      "Iteration 17, loss = 0.00391049\n",
      "Iteration 18, loss = 0.00272378\n",
      "Iteration 19, loss = 0.00287691\n",
      "Iteration 20, loss = 0.00325885\n",
      "Iteration 21, loss = 0.00291358\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[6126  601]\n",
      " [1036  712]]\n",
      "0.542269611577 0.407322654462 0.465207448546\n",
      "['BernoulliNB', 0.5615763546798029, 0.06521739130434782, 0.11686314710404921, 0.7966961651917404]\n",
      "['RandomForestClassifier', 0.7344028520499108, 0.23569794050343248, 0.35686444348202684, 0.8247787610619469]\n",
      "['BaggingClassifier', 0.6033676333021516, 0.36899313501144165, 0.4579339723109691, 0.8198230088495575]\n",
      "['ExtraTreesClassifier', 0.7167630057803468, 0.2128146453089245, 0.32818703131892374, 0.8202949852507375]\n",
      "['DecisionTreeClassifier', 0.44527363184079605, 0.4096109839816934, 0.4266984505363528, 0.7729793510324484]\n",
      "['CalibratedClassifierCV', 0.6251287332646756, 0.34725400457665906, 0.4464876792938581, 0.8224188790560472]\n",
      "['SGDClassifier', 0.6464646464646465, 0.2929061784897025, 0.4031496062992126, 0.8211209439528023]\n",
      "['MLPClassifier', 0.5422696115765423, 0.4073226544622426, 0.46520744854622675, 0.8068436578171091]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "train_vect, test_vect = convertTfidf(trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "#### a. For the first task we will be taking the initial model that was built from the xml file without any tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[5111 1616]\n",
      " [ 901  847]]\n",
      "0.34388956557 0.484553775744 0.402279743529\n",
      "Training  RandomForestClassifier\n",
      "[[6662   65]\n",
      " [1602  146]]\n",
      "0.691943127962 0.08352402746 0.149055640633\n",
      "Training  BaggingClassifier\n",
      "[[6606  121]\n",
      " [1549  199]]\n",
      "0.621875 0.113844393593 0.192456479691\n",
      "Training  ExtraTreesClassifier\n",
      "[[6574  153]\n",
      " [1573  175]]\n",
      "0.533536585366 0.100114416476 0.16859344894\n",
      "Training  DecisionTreeClassifier\n",
      "[[5554 1173]\n",
      " [1237  511]]\n",
      "0.303444180523 0.29233409611 0.297785547786\n",
      "Training  CalibratedClassifierCV\n",
      "[[6572  155]\n",
      " [1583  165]]\n",
      "0.515625 0.0943935926773 0.159574468085\n",
      "Training  SGDClassifier\n",
      "[[5010 1717]\n",
      " [ 849  899]]\n",
      "0.343654434251 0.514302059497 0.412007332722\n",
      "Training  KNeighborsClassifier\n",
      "[[5969  758]\n",
      " [1265  483]]\n",
      "0.389202256245 0.276315789474 0.32318501171\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.40381963\n",
      "Iteration 2, loss = 0.35897360\n",
      "Iteration 3, loss = 0.34312163\n",
      "Iteration 4, loss = 0.33232742\n",
      "Iteration 5, loss = 0.32169671\n",
      "Iteration 6, loss = 0.31165904\n",
      "Iteration 7, loss = 0.29896243\n",
      "Iteration 8, loss = 0.29098681\n",
      "Iteration 9, loss = 0.28280297\n",
      "Iteration 10, loss = 0.27379856\n",
      "Iteration 11, loss = 0.26436953\n",
      "Iteration 12, loss = 0.25503596\n",
      "Iteration 13, loss = 0.24859851\n",
      "Iteration 14, loss = 0.23828437\n",
      "Iteration 15, loss = 0.23089966\n",
      "Iteration 16, loss = 0.22474441\n",
      "Iteration 17, loss = 0.21436388\n",
      "Iteration 18, loss = 0.21104537\n",
      "Iteration 19, loss = 0.20474015\n",
      "Iteration 20, loss = 0.19968618\n",
      "Iteration 21, loss = 0.19187218\n",
      "Iteration 22, loss = 0.18928717\n",
      "Iteration 23, loss = 0.17984582\n",
      "Iteration 24, loss = 0.17580945\n",
      "Iteration 25, loss = 0.16952185\n",
      "Iteration 26, loss = 0.16335684\n",
      "Iteration 27, loss = 0.15912281\n",
      "Iteration 28, loss = 0.15831897\n",
      "Iteration 29, loss = 0.14978803\n",
      "Iteration 30, loss = 0.14597340\n",
      "Iteration 31, loss = 0.14291024\n",
      "Iteration 32, loss = 0.13954490\n",
      "Iteration 33, loss = 0.14252382\n",
      "Iteration 34, loss = 0.13075758\n",
      "Iteration 35, loss = 0.12808561\n",
      "Iteration 36, loss = 0.12529223\n",
      "Iteration 37, loss = 0.12002290\n",
      "Iteration 38, loss = 0.11768537\n",
      "Iteration 39, loss = 0.11916981\n",
      "Iteration 40, loss = 0.11496002\n",
      "Iteration 41, loss = 0.11190791\n",
      "Iteration 42, loss = 0.11421090\n",
      "Iteration 43, loss = 0.10647146\n",
      "Iteration 44, loss = 0.10569477\n",
      "Iteration 45, loss = 0.09915456\n",
      "Iteration 46, loss = 0.10263913\n",
      "Iteration 47, loss = 0.09825769\n",
      "Iteration 48, loss = 0.09199937\n",
      "Iteration 49, loss = 0.09156221\n",
      "Iteration 50, loss = 0.08726094\n",
      "Iteration 51, loss = 0.08786379\n",
      "Iteration 52, loss = 0.09104695\n",
      "Iteration 53, loss = 0.08673438\n",
      "Iteration 54, loss = 0.08337794\n",
      "Iteration 55, loss = 0.08536899\n",
      "Iteration 56, loss = 0.08259202\n",
      "Iteration 57, loss = 0.08225044\n",
      "Iteration 58, loss = 0.07937774\n",
      "Iteration 59, loss = 0.07609082\n",
      "Iteration 60, loss = 0.07613390\n",
      "Iteration 61, loss = 0.07836844\n",
      "Iteration 62, loss = 0.07562765\n",
      "Iteration 63, loss = 0.07283116\n",
      "Iteration 64, loss = 0.07068911\n",
      "Iteration 65, loss = 0.07040333\n",
      "Iteration 66, loss = 0.07176190\n",
      "Iteration 67, loss = 0.06977291\n",
      "Iteration 68, loss = 0.06690418\n",
      "Iteration 69, loss = 0.07488676\n",
      "Iteration 70, loss = 0.06591934\n",
      "Iteration 71, loss = 0.06614050\n",
      "Iteration 72, loss = 0.06611552\n",
      "Iteration 73, loss = 0.06525305\n",
      "Iteration 74, loss = 0.06666069\n",
      "Iteration 75, loss = 0.06491863\n",
      "Iteration 76, loss = 0.06116322\n",
      "Iteration 77, loss = 0.05987760\n",
      "Iteration 78, loss = 0.06383576\n",
      "Iteration 79, loss = 0.06252969\n",
      "Iteration 80, loss = 0.06214067\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[5855  872]\n",
      " [1072  676]]\n",
      "0.43669250646 0.386727688787 0.410194174757\n",
      "['BernoulliNB', 0.34388956557044253, 0.4845537757437071, 0.402279743528853, 0.7030088495575221]\n",
      "['RandomForestClassifier', 0.6919431279620853, 0.08352402745995423, 0.149055640632976, 0.8033038348082596]\n",
      "['BaggingClassifier', 0.621875, 0.11384439359267734, 0.1924564796905222, 0.8029498525073746]\n",
      "['ExtraTreesClassifier', 0.5335365853658537, 0.10011441647597254, 0.16859344894026976, 0.7963421828908555]\n",
      "['DecisionTreeClassifier', 0.30344418052256533, 0.2923340961098398, 0.2977855477855478, 0.7156342182890856]\n",
      "['CalibratedClassifierCV', 0.515625, 0.09439359267734554, 0.15957446808510636, 0.7949262536873156]\n",
      "['SGDClassifier', 0.34365443425076453, 0.5143020594965675, 0.41200733272227313, 0.6972271386430678]\n",
      "['KNeighborsClassifier', 0.38920225624496374, 0.27631578947368424, 0.32318501170960184, 0.7612979351032448]\n",
      "['MLPClassifier', 0.43669250645994834, 0.38672768878718533, 0.41019417475728154, 0.7706194690265487]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlOriginalSen'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Second we take the word2vec model that built after BR's are marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[5451 1276]\n",
      " [ 988  760]]\n",
      "0.373280943026 0.434782608696 0.401691331924\n",
      "Training  RandomForestClassifier\n",
      "[[6655   72]\n",
      " [1580  168]]\n",
      "0.7 0.0961098398169 0.169014084507\n",
      "Training  BaggingClassifier\n",
      "[[6575  152]\n",
      " [1519  229]]\n",
      "0.601049868766 0.131006864989 0.215124471583\n",
      "Training  ExtraTreesClassifier\n",
      "[[6602  125]\n",
      " [1575  173]]\n",
      "0.580536912752 0.0989702517162 0.169110459433\n",
      "Training  DecisionTreeClassifier\n",
      "[[5699 1028]\n",
      " [1296  452]]\n",
      "0.305405405405 0.258581235698 0.280049566295\n",
      "Training  CalibratedClassifierCV\n",
      "[[6572  155]\n",
      " [1568  180]]\n",
      "0.537313432836 0.102974828375 0.172827652424\n",
      "Training  SGDClassifier\n",
      "[[4923 1804]\n",
      " [ 864  884]]\n",
      "0.328869047619 0.505720823799 0.398557258792\n",
      "Training  KNeighborsClassifier\n",
      "[[5818  909]\n",
      " [1193  555]]\n",
      "0.379098360656 0.317505720824 0.345579078456\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.42374897\n",
      "Iteration 2, loss = 0.36568700\n",
      "Iteration 3, loss = 0.35143264\n",
      "Iteration 4, loss = 0.34003247\n",
      "Iteration 5, loss = 0.32883042\n",
      "Iteration 6, loss = 0.31880696\n",
      "Iteration 7, loss = 0.30775127\n",
      "Iteration 8, loss = 0.29800329\n",
      "Iteration 9, loss = 0.28556772\n",
      "Iteration 10, loss = 0.27700762\n",
      "Iteration 11, loss = 0.26727744\n",
      "Iteration 12, loss = 0.25777494\n",
      "Iteration 13, loss = 0.25268278\n",
      "Iteration 14, loss = 0.24330401\n",
      "Iteration 15, loss = 0.23597023\n",
      "Iteration 16, loss = 0.22614032\n",
      "Iteration 17, loss = 0.22251762\n",
      "Iteration 18, loss = 0.21314176\n",
      "Iteration 19, loss = 0.20898193\n",
      "Iteration 20, loss = 0.20309926\n",
      "Iteration 21, loss = 0.19669789\n",
      "Iteration 22, loss = 0.19251449\n",
      "Iteration 23, loss = 0.18629396\n",
      "Iteration 24, loss = 0.17872323\n",
      "Iteration 25, loss = 0.17524780\n",
      "Iteration 26, loss = 0.16947406\n",
      "Iteration 27, loss = 0.16965648\n",
      "Iteration 28, loss = 0.15997003\n",
      "Iteration 29, loss = 0.15727769\n",
      "Iteration 30, loss = 0.15570245\n",
      "Iteration 31, loss = 0.14810433\n",
      "Iteration 32, loss = 0.14605238\n",
      "Iteration 33, loss = 0.13970585\n",
      "Iteration 34, loss = 0.13876466\n",
      "Iteration 35, loss = 0.13438123\n",
      "Iteration 36, loss = 0.12922105\n",
      "Iteration 37, loss = 0.12615188\n",
      "Iteration 38, loss = 0.12644256\n",
      "Iteration 39, loss = 0.12235486\n",
      "Iteration 40, loss = 0.11950670\n",
      "Iteration 41, loss = 0.11703091\n",
      "Iteration 42, loss = 0.11633565\n",
      "Iteration 43, loss = 0.11489575\n",
      "Iteration 44, loss = 0.10721306\n",
      "Iteration 45, loss = 0.10641068\n",
      "Iteration 46, loss = 0.10879249\n",
      "Iteration 47, loss = 0.10467477\n",
      "Iteration 48, loss = 0.10221199\n",
      "Iteration 49, loss = 0.10290456\n",
      "Iteration 50, loss = 0.09521846\n",
      "Iteration 51, loss = 0.09889937\n",
      "Iteration 52, loss = 0.09452978\n",
      "Iteration 53, loss = 0.09212093\n",
      "Iteration 54, loss = 0.09202803\n",
      "Iteration 55, loss = 0.08844004\n",
      "Iteration 56, loss = 0.09134437\n",
      "Iteration 57, loss = 0.08875706\n",
      "Iteration 58, loss = 0.08585256\n",
      "Iteration 59, loss = 0.08531911\n",
      "Iteration 60, loss = 0.08949082\n",
      "Iteration 61, loss = 0.08424784\n",
      "Iteration 62, loss = 0.08272722\n",
      "Iteration 63, loss = 0.07746207\n",
      "Iteration 64, loss = 0.07912153\n",
      "Iteration 65, loss = 0.08019926\n",
      "Iteration 66, loss = 0.07902460\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[5897  830]\n",
      " [1097  651]]\n",
      "0.439567859554 0.372425629291 0.403220811397\n",
      "['BernoulliNB', 0.37328094302554027, 0.43478260869565216, 0.40169133192389, 0.7328613569321534]\n",
      "['RandomForestClassifier', 0.7, 0.09610983981693363, 0.16901408450704225, 0.8050737463126844]\n",
      "['BaggingClassifier', 0.6010498687664042, 0.13100686498855835, 0.21512447158290277, 0.8028318584070796]\n",
      "['ExtraTreesClassifier', 0.5805369127516778, 0.09897025171624714, 0.16911045943304007, 0.799410029498525]\n",
      "['DecisionTreeClassifier', 0.3054054054054054, 0.2585812356979405, 0.28004956629491945, 0.7257817109144543]\n",
      "['CalibratedClassifierCV', 0.5373134328358209, 0.10297482837528604, 0.1728276524243879, 0.7966961651917404]\n",
      "['SGDClassifier', 0.3288690476190476, 0.505720823798627, 0.39855725879170417, 0.6851917404129794]\n",
      "['KNeighborsClassifier', 0.3790983606557377, 0.31750572082379863, 0.3455790784557908, 0.751976401179941]\n",
      "['MLPClassifier', 0.4395678595543552, 0.37242562929061784, 0.4032208113967172, 0.7726253687315634]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "modelName = 'xmlReplaceBR'\n",
    "train_vect, test_vect = buildW2V(modelName,trainMid, testMid)\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding...\n",
      "Embedding...\n",
      "Training  BernoulliNB\n",
      "[[6073  654]\n",
      " [1342  406]]\n",
      "0.383018867925 0.232265446224 0.289173789174\n",
      "Training  RandomForestClassifier\n",
      "[[6688   39]\n",
      " [1668   80]]\n",
      "0.672268907563 0.045766590389 0.0856989823246\n",
      "Training  BaggingClassifier\n",
      "[[6655   72]\n",
      " [1621  127]]\n",
      "0.638190954774 0.0726544622426 0.130457113508\n",
      "Training  ExtraTreesClassifier\n",
      "[[6578  149]\n",
      " [1636  112]]\n",
      "0.429118773946 0.0640732265446 0.11149825784\n",
      "Training  DecisionTreeClassifier\n",
      "[[5574 1153]\n",
      " [1262  486]]\n",
      "0.296522269677 0.278032036613 0.286979627989\n",
      "Training  CalibratedClassifierCV\n",
      "[[6589  138]\n",
      " [1612  136]]\n",
      "0.496350364964 0.0778032036613 0.134520276954\n",
      "Training  SGDClassifier\n",
      "[[5693 1034]\n",
      " [1084  664]]\n",
      "0.391048292108 0.379862700229 0.385374347069\n",
      "Training  KNeighborsClassifier\n",
      "[[5874  853]\n",
      " [1186  562]]\n",
      "0.397173144876 0.321510297483 0.355358836548\n",
      "Training  MLPClassifier\n",
      "Iteration 1, loss = 0.41130927\n",
      "Iteration 2, loss = 0.37081959\n",
      "Iteration 3, loss = 0.35622398\n",
      "Iteration 4, loss = 0.34662213\n",
      "Iteration 5, loss = 0.33357675\n",
      "Iteration 6, loss = 0.32063849\n",
      "Iteration 7, loss = 0.31095806\n",
      "Iteration 8, loss = 0.29832358\n",
      "Iteration 9, loss = 0.28543745\n",
      "Iteration 10, loss = 0.27784715\n",
      "Iteration 11, loss = 0.26626035\n",
      "Iteration 12, loss = 0.25808033\n",
      "Iteration 13, loss = 0.24676000\n",
      "Iteration 14, loss = 0.23857596\n",
      "Iteration 15, loss = 0.22987844\n",
      "Iteration 16, loss = 0.22508386\n",
      "Iteration 17, loss = 0.21484997\n",
      "Iteration 18, loss = 0.20876241\n",
      "Iteration 19, loss = 0.20608903\n",
      "Iteration 20, loss = 0.19621975\n",
      "Iteration 21, loss = 0.18542971\n",
      "Iteration 22, loss = 0.18630313\n",
      "Iteration 23, loss = 0.17954458\n",
      "Iteration 24, loss = 0.17378100\n",
      "Iteration 25, loss = 0.17055042\n",
      "Iteration 26, loss = 0.16704243\n",
      "Iteration 27, loss = 0.15743514\n",
      "Iteration 28, loss = 0.15291862\n",
      "Iteration 29, loss = 0.15116779\n",
      "Iteration 30, loss = 0.14443839\n",
      "Iteration 31, loss = 0.14015985\n",
      "Iteration 32, loss = 0.13382097\n",
      "Iteration 33, loss = 0.13365464\n",
      "Iteration 34, loss = 0.12833149\n",
      "Iteration 35, loss = 0.12734787\n",
      "Iteration 36, loss = 0.12429537\n",
      "Iteration 37, loss = 0.12481081\n",
      "Iteration 38, loss = 0.11726880\n",
      "Iteration 39, loss = 0.11307827\n",
      "Iteration 40, loss = 0.11384365\n",
      "Iteration 41, loss = 0.10467709\n",
      "Iteration 42, loss = 0.10081395\n",
      "Iteration 43, loss = 0.09829301\n",
      "Iteration 44, loss = 0.09744727\n",
      "Iteration 45, loss = 0.09440746\n",
      "Iteration 46, loss = 0.09783289\n",
      "Iteration 47, loss = 0.09131676\n",
      "Iteration 48, loss = 0.09074380\n",
      "Iteration 49, loss = 0.09237820\n",
      "Iteration 50, loss = 0.08361048\n",
      "Iteration 51, loss = 0.08045228\n",
      "Iteration 52, loss = 0.07889556\n",
      "Iteration 53, loss = 0.07636241\n",
      "Iteration 54, loss = 0.07329607\n",
      "Iteration 55, loss = 0.07087902\n",
      "Iteration 56, loss = 0.07985322\n",
      "Iteration 57, loss = 0.06980281\n",
      "Iteration 58, loss = 0.07365093\n",
      "Iteration 59, loss = 0.06620597\n",
      "Iteration 60, loss = 0.06502039\n",
      "Iteration 61, loss = 0.06345656\n",
      "Iteration 62, loss = 0.06471314\n",
      "Iteration 63, loss = 0.06701069\n",
      "Iteration 64, loss = 0.06213139\n",
      "Iteration 65, loss = 0.05847378\n",
      "Iteration 66, loss = 0.05628158\n",
      "Iteration 67, loss = 0.05685626\n",
      "Iteration 68, loss = 0.05527985\n",
      "Iteration 69, loss = 0.06290352\n",
      "Iteration 70, loss = 0.06653314\n",
      "Iteration 71, loss = 0.05620202\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "[[6064  663]\n",
      " [1166  582]]\n",
      "0.467469879518 0.33295194508 0.388907450718\n",
      "['BernoulliNB', 0.38301886792452833, 0.2322654462242563, 0.2891737891737892, 0.7644837758112094]\n",
      "['RandomForestClassifier', 0.6722689075630253, 0.04576659038901602, 0.0856989823245849, 0.7985840707964602]\n",
      "['BaggingClassifier', 0.6381909547738693, 0.07265446224256293, 0.130457113507961, 0.8002359882005899]\n",
      "['ExtraTreesClassifier', 0.42911877394636017, 0.06407322654462243, 0.11149825783972125, 0.7893805309734513]\n",
      "['DecisionTreeClassifier', 0.2965222696766321, 0.2780320366132723, 0.28697962798937116, 0.7150442477876107]\n",
      "['CalibratedClassifierCV', 0.49635036496350365, 0.07780320366132723, 0.13452027695351138, 0.7935103244837758]\n",
      "['SGDClassifier', 0.3910482921083628, 0.37986270022883295, 0.38537434706906554, 0.7500884955752213]\n",
      "['KNeighborsClassifier', 0.3971731448763251, 0.32151029748283755, 0.3553588365475815, 0.7594100294985251]\n",
      "['MLPClassifier', 0.4674698795180723, 0.3329519450800915, 0.3889074507183428, 0.784188790560472]\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram_trans = Phrases(trainFSen)\n",
    "\n",
    "modelName = 'csv_Fin_bigramToken'\n",
    "train_vect, test_vect = buildW2V(modelName,bigram_trans[trainMid], bigram_trans[testMid])\n",
    "print(classify(train_vect,trainLab,test_vect,testLab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
